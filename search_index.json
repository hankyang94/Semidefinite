[["index.html", "Optimal Control and Estimation Preface", " Optimal Control and Estimation Heng Yang 2023-08-02 Preface This is the textbook for Harvard ES/AM 158: Introduction to Optimal Control and Estimation. Information about the offerings of the class is listed below. 2023 Fall Time: Mon/Wed 2:15 - 3:30pm Location: Science and Engineering Complex, Room TBD Instructor: Heng Yang Teaching Fellow: Weiyu Li Syllabus Acknowledgment "],["formulation.html", "Chapter 1 The Optimal Control Formulation 1.1 The Basic Problem 1.2 Dynamic Programming and Principle of Optimality", " Chapter 1 The Optimal Control Formulation 1.1 The Basic Problem Consider a discrete-time dynamical system \\[\\begin{equation} x_{k+1} = f_k (x_k, u_k, w_k), \\quad k =0,1,\\dots,N-1 \\tag{1.1} \\end{equation}\\] where \\(x_k \\in \\mathbb{X} \\subseteq \\mathbb{R}^n\\) is the state of the system, \\(u_k \\in \\mathbb{U} \\subseteq \\mathbb{R}^m\\) is the control we wish to design, \\(w_k \\in \\mathbb{W} \\subseteq \\mathbb{R}^p\\) a random disturbance or noise (e.g., due to unmodelled dynamics) which is described by a probability distribution \\(P_k(\\cdot \\mid x_k, u_k)\\) that may depend on \\(x_k\\) and \\(u_k\\) but not on prior disturbances \\(w_0,\\dots,w_{k-1}\\), \\(k\\) indexes the discrete time, \\(N\\) denotes the horizon, \\(f_k\\) models the transition function of the system (typically \\(f_k \\equiv f\\) is time-invariant, especially for robotics systems; we use \\(f_k\\) here to keep full generality). Remark (Deterministic v.s. Stochastic). When \\(w_k \\equiv 0\\) for all \\(k\\), we say the system (1.1) is deterministic; otherwise we say the system is stochastic. In the following we will deal with the stochastic case, but most of the methodology should carry over to the deterministic setup. We consider the class of controllers (also called policies) that consist of a sequence of functions \\[ \\pi = \\{ \\mu_0,\\dots,\\mu_{N-1} \\}, \\] where \\(\\mu_k (x_k) \\in \\mathbb{U}\\) for all \\(x_k\\), i.e., \\(\\mu_k\\) is a feedback controller that maps the state to an admissible control. Given an initial state \\(x_0\\) and an admissible policy \\(\\pi\\), the state trajectory of the system is a sequence of random variables that evolve according to \\[\\begin{equation} x_{k+1} = f_k(x_k,\\mu_k(x_k),w_k), \\quad k=0,\\dots,N-1 \\tag{1.2} \\end{equation}\\] where the randomness comes from the disturbance \\(w_k\\). We assume the state-control trajectory \\(\\{u_k\\}_{k=0}^{N-1}\\) and \\(\\{x_k \\}_{k=0}^{N}\\) induce an additive cost \\[\\begin{equation} g_N(x_N) + \\sum_{k=0}^{N-1} g_k(x_k,u_k) \\tag{1.3} \\end{equation}\\] where \\(g_k,k=0,\\dots,N\\) are some user-designed functions. With (1.2) and (1.3), for any admissible policy \\(\\pi\\), we denote its induced expected cost with initial state \\(x_0\\) as \\[\\begin{equation} J_\\pi (x_0) = \\mathbb{E} \\left\\{ g_N(x_N) + \\sum_{k=0}^{N-1} g_k (x_k, \\mu_k(x_k)) \\right\\}, \\tag{1.4} \\end{equation}\\] where the expectation is taken over the randomness of \\(w_k\\). Definition 1.1 (Discrete-time, Finite-horizon Optimal Control) Find the best admissible controller that minimizes the expected cost in (1.4) \\[\\begin{equation} \\pi^\\star \\in \\arg\\min_{\\pi \\in \\Pi} J_\\pi(x_0), \\end{equation}\\] where \\(\\Pi\\) is the set of all admissible controllers. The cost attained by the optimal controller, i.e., \\(J^\\star = J_{\\pi^\\star}(x_0)\\) is called the optimal cost-to-go, or the optimal value function. Remark (Open-loop v.s. Closed-loop). An important feature of the basic problem in Definition 1.1 is that the problem seeks feedback policies, instead of numerical values of the controls, i.e., \\(u_k = \\mu_k(x_k)\\) is in general a function of the state \\(x_k\\). In other words, the controls are executed sequentially, one at a time after observing the state at each time. This is called closed-loop control, and is in general better than open-loop control \\[ \\min_{u_0,\\dots,u_{N-1}} \\mathbb{E} \\left\\{ g_N(x_N) + \\sum_{k=0}^{N-1} g_k (x_k, u_k) \\right\\} \\] where all the controls are planned at \\(k=0\\). Intuitively, a closed-loop policy is able to utilize the extra information received at each timestep (i.e., it observes \\(x_{k+1}\\) and hence also observes the disturbance \\(w_k\\)) to obtain a lower cost than an open-loop controller. Example 1.2.1 in (Bertsekas 2012) gives a concrete application where a closed-loop policy attains a lower cost than an open-loop policy. In deterministic control (i.e., when \\(w_k \\equiv 0,\\forall k\\)), however, a closed-loop policy has no advantage over an open-loop controller. This is obvious because at \\(k=0\\), even the open-loop controller predicts perfectly the consequences of all its actions and there is no extra information to be observed at later time steps. In fact, even in stochastic problems, a closed-loop policy may not be advantageous, see Exercise 1.27 in (Bertsekas 2012). 1.2 Dynamic Programming and Principle of Optimality We now introduce a general and powerful algorithm, namely dynamic programming (DP), for solving the optimal control problem 1.1. The DP algorithm builds upon a quite simple intuition called the Bellman principle of optimality. Theorem 1.1 (Bellman Principle of Optimality) Let \\(\\pi^\\star = \\{ \\mu_0^\\star,\\mu_1^\\star,\\dots,\\mu_{N-1}^\\star \\}\\) be an optimal policy for the optimal control problem 1.1. Assume that when using \\(\\pi^\\star\\), a given state \\(x_i\\) occurs at timestep \\(i\\) with positive probability (i.e., \\(x_i\\) is reachable at time \\(i\\)). Now consider the following subproblem where we are at \\(x_i\\) at time \\(i\\) and wish to minimize the cost-to-go from time \\(i\\) to time \\(N\\) \\[ \\min_{\\mu_i,\\dots,\\mu_{N-1}} \\mathbb{E} \\left\\{ g_N(x_N) + \\sum_{k=i}^{N-1} g_k (x_k, \\mu_k(x_k)) \\right\\}. \\] Then the truncated policy \\(\\{\\mu^\\star_i,\\mu^\\star_{i+1},\\dots, \\mu^\\star_{N-1}\\}\\) must be optimal for the subproblem. Theorem 1.1 can be proved intuitively by contradiction: if the truncated policy \\(\\{\\mu^\\star_i,\\mu^\\star_{i+1},\\dots, \\mu^\\star_{N-1}\\}\\) is not optimal for the subproblem, say there exists a different policy \\(\\{\\mu_i&#39;,\\mu_{i+1}&#39;,\\dots, \\mu_{N-1}&#39;\\}\\) that attains a lower cost for the subproblem starting at \\(x_i\\) at time \\(i\\). Then the combined policy \\(\\{\\mu_0^\\star,\\dots,\\mu^\\star_{i-1},\\mu_i&#39;,\\dots,\\mu_{N-1}&#39;\\}\\) must attain a lower cost for the original optimal control problem 1.1 due to the additive cost structure, contradicting the optimality of \\(\\pi^\\star\\). The Bellman principle of optimality is more than just a principle, it is also an algorithm. It suggests that, to build an optimal policy, one can start by solving the last-stage subproblem to obtain \\(\\{\\mu^\\star_{N-1} \\}\\), and then proceed to solve the subproblem containing the last two stages to obtain \\(\\{ \\mu^\\star_{N-2},\\mu^\\star_{N-1} \\}\\). The recursion continues until optimal policies at all stages are computed. The following theorem formalizes this concept. Theorem 1.2 (Dynamic Programming) The optimal value function \\(J^\\star(x_0)\\) of the optimal control problem 1.1 (starting from any given initial condition \\(x_0\\)) is equal to \\(J_0(x_0)\\), which can be computed backwards and recursively as \\[\\begin{align} J_N(X_N) &amp;= g_N(x_N) \\\\ J_k(x_k) &amp;= \\min_{u_k \\in \\mathbb{U}} \\displaystyle \\mathbb{E}_{w_k \\sim P_k(\\cdot \\mid x_k, u_k)} \\displaystyle \\left\\{ g_k(x_k,u_k) + J_{k+1}(f_k(x_k,u_k,w_k) ) \\right\\}, \\ k=N-1,\\dots,1,0. \\tag{1.5} \\end{align}\\] Moreover, if \\(u_k^\\star = \\mu_k^\\star(x_k)\\) is a minimizer of (1.5) for every \\(x_k\\), then the policy \\(\\pi^\\star = \\{\\mu_0^\\star,\\dots,\\mu_{N-1}^\\star \\}\\) is optimal. Proof. For any admissible policy \\(\\pi = \\{ \\mu_0,\\dots,\\mu_{N-1} \\}\\), denote \\(\\pi^k = \\{ \\mu_k,\\dots,\\mu_{N-1} \\}\\) the last-\\((N-k)\\)-stage truncated policy. Consider the subproblem consisting of the last \\(N-k\\) stages starting from \\(x_k\\), and let \\(J^\\star_k(x_k)\\) be its optimal cost-to-go. Mathematically, this is \\[\\begin{equation} J^\\star_{k}(x_k) = \\min_{\\pi^k} \\mathbb{E}_{w_k,\\dots,w_{N-1}} \\left\\{ g_N(x_N) + \\sum_{i=k}^{N-1} g_i (x_i,\\mu_i(x_i)) \\right\\}, \\quad k=0,1,\\dots,N-1. \\tag{1.6} \\end{equation}\\] We define \\(J^\\star_N(x_N) = g(x_N)\\) for \\(k=N\\). Our goal is to prove the \\(J_k(x_k)\\) computed by dynamic programming from (1.5) is equal to \\(J^\\star_k (x_k)\\) for all \\(k=0,\\dots,N\\). We will prove this by induction. Firstly, we already have \\(J^\\star_N(x_N) = J_N(x_N) = g(x_N)\\), so \\(k=N\\) holds automatically. Now we assume \\(J^\\star_{k+1}(x_{k+1}) = J_{k+1}(x_{k+1})\\) for all \\(x_{k+1}\\), and we wish to induce \\(J^\\star_{k}(x_{k}) = J_{k}(x_{k})\\). To show this, we write \\[\\begin{align} \\hspace{-16mm} J^\\star_{k}(x_k) &amp;= \\min_{\\pi^k} \\mathbb{E}_{w_k,\\dots,w_{N-1}} \\left\\{ g_N(x_N) + \\sum_{i=k}^{N-1} g_i (x_i,\\mu_i(x_i)) \\right\\} \\tag{1.7}\\\\ &amp;= \\min_{\\mu_k,\\pi^{k+1}} \\mathbb{E}_{w_k,\\dots,w_{N-1}} \\left\\{ g_k(x_k,\\mu_k(x_k)) + g_N(x_N) + \\sum_{i=k+1}^{N-1} g_i(x_i,\\mu_i(x_i)) \\right\\} \\tag{1.8}\\\\ &amp;= \\min_{\\mu_k} \\left[ \\min_{\\pi^{k+1}} \\mathbb{E}_{w_k,\\dots,w_{N-1}} \\left\\{ g_k(x_k,\\mu_k(x_k)) + g_N(x_N) + \\sum_{i=k+1}^{N-1} g_i(x_i,\\mu_i(x_i)) \\right\\}\\right] \\tag{1.9}\\\\ &amp;= \\min_{\\mu_k} \\mathbb{E}_{w_k} \\left\\{ g_k(x_k,\\mu_k(x_k)) + \\min_{\\pi^{k+1}} \\left[ \\mathbb{E}_{w_{k+1},\\dots,w_{N-1}} \\left\\{ g_N(x_N) + \\sum_{i=k+1}^{N-1} g_i(x_i,\\mu_i(x_i)) \\right\\} \\right] \\right\\} \\tag{1.10}\\\\ &amp;= \\min_{\\mu_k} \\mathbb{E}_{w_k} \\left\\{ g_k(x_k,\\mu_k(x_k)) + J^\\star_{k+1}(f_k(x_k,\\mu_k(x_k),w_k)) \\right\\} \\tag{1.11}\\\\ &amp;= \\min_{\\mu_k} \\mathbb{E}_{w_k} \\left\\{ g_k(x_k,\\mu_k(x_k)) + J_{k+1}(f_k(x_k,\\mu_k(x_k),w_k)) \\right\\} \\tag{1.12}\\\\ &amp;= \\min_{u_k \\in \\mathbb{U}} \\mathbb{E}_{w_k} \\left\\{ g_k(x_k,\\mu_k(x_k)) + J_{k+1}(f_k(x_k,\\mu_k(x_k),w_k)) \\right\\} \\tag{1.13}\\\\ &amp;= J_k(x_k), \\tag{1.14} \\end{align}\\] where (1.7) follows from definition (1.6); (1.8) expands \\(\\pi^k = \\{ \\mu_k, \\pi^{k+1}\\}\\) and \\(\\sum_{i=k}^{N-1} g_i = g_k + \\sum_{i=k+1}^{N-1}\\); (1.9) writes the joint minimization over \\(\\mu_k\\) and \\(\\pi^{k+1}\\) as equivalently first minimizing over \\(\\pi^{k+1}\\) and then minimizing over \\(\\mu_k\\); (1.10) is the key step and holds because \\(g_k\\) and \\(w_k\\) depend only on \\(\\mu_k\\) but not on \\(\\pi^{k+1}\\); (1.11) follows again from definition (1.6) with \\(k\\) replaced by \\(k+1\\); (1.12) results from the induction assumption; (1.13) clearly holds because any \\(\\mu_k(x_k)\\) belongs to \\(\\mathbb{U}\\) and any element in \\(\\mathbb{U}\\) can be chosen by a feedback controller \\(\\mu_k\\); and lastly (1.14) follows from the dynamic programming algorithm (1.5). By induction, this shows that \\(J^\\star_k(x_k) = J_k(x_k)\\) for all \\(k=0,\\dots,N\\). The careful reader, especially from a robotics background, may soon become disappointed when seeing the DP algorithm (1.5) because it is rather conceptual than practical. To see this, we only need to run DP for \\(k=N-1\\): \\[\\begin{equation} J_{N-1}(x_{N-1}) = \\min_{u_{N-1} \\in \\mathbb{U}} \\mathbb{E}_{w_{N-1}} \\left\\{ g_{N-1}(x_{N-1},u_{N-1}) + J_N(f_{N-1}(x_{N-1},u_{N-1},w_{N-1})) \\right\\}. \\tag{1.15} \\end{equation}\\] Two challenges immediately show up: How to perform the minimization over \\(u_{N-1}\\) when \\(\\mathbb{U}\\) is a continuous constraint set? Even if we assume \\(g_{N-1}\\) is convex1 in \\(u_{N-1}\\), \\(J_N\\) is convex in \\(x_{N}\\), and the dynamics \\(f_{N-1}\\) is also convex in \\(u_{N-1}\\) (so that the optimization (1.15) is convex), we may be able to solve the minimization numerically for each \\(x_{N-1}\\) using a convex optimization solver, but rarely will we be able to find an analytical policy \\(\\mu_{N-1}^\\star\\) such that \\(u_{N-1}^\\star = \\mu_{N-1}^\\star (x_{N-1})\\) for every \\(x_{N-1}\\) (i.e., the optimal policy \\(\\mu_{N-1}^\\star\\) is implict but not explict). Suppose we can find an anlytical optimal policy \\(\\mu_{N-1}^\\star\\), say \\(\\mu_{N-1}^\\star = K x_{N-1}\\) a linear policy, how will plugging \\(\\mu_{N-1}^\\star\\) into (1.15) affect the complexity of \\(J_{N-1}(x_{N-1})\\)? One can see that even if \\(\\mu_{N-1}^\\star\\) is linear in \\(x_{N-1}\\), \\(J_{N-1}\\) may be highly nonlinear in \\(x_{N-1}\\) due to the composition with \\(g_{N-1}\\), \\(f_{N-1}\\) and \\(J_N\\). If \\(J_{N-1}(x_{N-1})\\) becomes too complex, then clearly it becomes more challenging to perform (1.15) for the next step \\(k=N-2\\). Due to these challenges, only in a very limited amount of cases will we be able to perform exact dynamic programming. For example, when the state space \\(\\mathbb{X}\\) and control space \\(\\mathbb{U}\\) are discrete, we can design efficient algorithms for exact DP. For another example, when the dynamics \\(f_k\\) is linear and the cost \\(g_k\\) is quadratic, we will also be able to compute \\(J_k(x_k)\\) in closed form (though this sounds a bit surprising!). We will study these problems in more details in Chapter 2. For general optimal control problems with continuous state space and control space (and most problems we care about in robotics), unfortunately, we will have to resort to approximate dynamic programming, basically variations of the DP algorithm (1.5) where approximate value functions \\(J_k(x_k)\\) and/or control policies \\(\\mu_k(x_k)\\) are used (e.g., with neural networks and machine learning).2 We will introduce several popular approximation schemes in Chapter 3. We will see that, although exact DP is not possible anymore, the Bellman principle of optimality still remains one of the most important guidelines for designing approximation algorithms. Efficient algorithms for approximate dynamic programming, preferrably with performance guarantees, still remain an active area of research. References "],["exactdp.html", "Chapter 2 Exact Dynamic Programming 2.1 Linear Quadratic Regulator", " Chapter 2 Exact Dynamic Programming In Chapter 1, we introduced the basic formulation of the finite-horizon and discrete-time optimal control problem, presented the Bellman principle of optimality, and derived the dynamic programming (DP) algorithm. We mentioned that, despite being a general-purpose algorithm, it can be difficult to implement DP exactly in practical applications. In this Chapter, we will introduce two problem setups where DP can in fact be implemented exactly. 2.1 Linear Quadratic Regulator Consider a linear discrete-time dynamical system \\[\\begin{equation} x_{k+1} = A_k x_k + B_k u_k + w_k, \\quad k=0,1,\\dots,N-1, \\tag{2.1} \\end{equation}\\] where \\(x_k \\in \\mathbb{R}^n\\) the state, \\(u_k \\in \\mathbb{R}^m\\) the control, \\(w_k \\in \\mathbb{R}^n\\) the independent, zero-mean disturbance with given probability distribution that does not depend on \\(x_k,u_k\\), and \\(A_k \\in \\mathbb{R}^{n \\times n}, B_k \\in \\mathbb{R}^{n \\times m}\\) are known matrices determining the transition dynamics. We want to solve the following optimal control problem \\[\\begin{equation} \\min_{\\mu_0,\\dots,\\mu_{N-1}} \\mathbb{E} \\left\\{ x_N^T Q_N x_N + \\sum_{k=0}^{N-1} \\left( x_k^T Q_k x_k + u_k^T R_k u_k \\right) \\right\\}, \\tag{2.2} \\end{equation}\\] where the expectation is taken over the randomness in \\(w_0,\\dots,w_{N-1}\\). In (2.2), \\(\\{Q_k \\}_{k=0}^N\\) are positive semidefinite matrices, and \\(\\{ R_k \\}_{k=0}^{N-1}\\) are positive definite matrices. The formulation (2.2) is typically known as the linear quadratic regulator (LQR) problem because the dynamics is linear, the cost is quadratic, and the formulation can be considered to “regulate” the system around the origin \\(x=0\\). We will now show that the DP algorithm in Theorem 1.2 can be exactly implemented for LQR. The DP algorithm computes the optimal cost-to-go backwards in time. The terminal cost is \\[ J_N(x_N) = x_N^T Q_N x_N \\] by definition. The optimal cost-to-go at time \\(N-1\\) is equal to \\[\\begin{equation} \\begin{split} J_{N-1}(x_{N-1}) = \\min_{u_{N-1}} \\mathbb{E}_{w_{N-1}} \\{ x_{N-1}^T Q_{N-1} x_{N-1} + u_{N-1}^T R_{N-1} u_{N-1} + \\\\ \\Vert \\underbrace{A_{N-1} x_{N-1} + B_{N-1} u_{N-1} + w_{N-1} }_{x_N} \\Vert^2_{Q_N} \\} \\end{split} \\tag{2.3} \\end{equation}\\] where \\(\\Vert v \\Vert_Q^2 = v^T Q v\\) for \\(Q \\succeq 0\\). Now observe that the objective in (2.3) is \\[\\begin{equation} \\begin{split} x_{N-1}^T Q_{N-1} x_{N-1} + u_{N-1}^T R_{N-1} u_{N-1} + \\Vert A_{N-1} x_{N-1} + B_{N-1} u_{N-1} \\Vert_{Q_N}^2 + \\\\ \\mathbb{E}_{w_{N-1}} \\left[ 2(A_{N-1} x_{N-1} + B_{N-1} u_{N-1} )^T Q_{N-1} w_{N-1} \\right] + \\\\ \\mathbb{E}_{w_{N-1}} \\left[ w_{N-1}^T Q_N w_{N-1} \\right] \\end{split} \\end{equation}\\] where the second line is zero due to \\(\\mathbb{E}(w_{N-1}) = 0\\) and the third line is a constant with respect to \\(u_{N-1}\\). Consequently, the optimal control \\(u_{N-1}^\\star\\) can be computed by setting the derivative of the objective with respect to \\(u_{N-1}\\) equal to zero \\[\\begin{equation} u_{N-1}^\\star = - \\left[ \\left( R_{N-1} + B_{N-1}^T Q_N B_{N-1} \\right)^{-1} B_{N-1}^T Q_N A_{N-1} \\right] x_{N-1}. \\tag{2.4} \\end{equation}\\] Plugging the optimal controller \\(u^\\star_{N-1}\\) back to the objective of (2.3) leads to \\[\\begin{equation} J_{N-1}(x_{N-1}) = x_{N-1}^T S_{N-1} x_{N-1} + \\mathbb{E} \\left[ w_{N-1}^T Q_N w_{N-1} \\right], \\tag{2.5} \\end{equation}\\] with \\[ S_{N-1} = Q_{N-1} + A_{N-1}^T \\left[ Q_N - Q_N B_{N-1} \\left( R_{N-1} + B_{N-1}^T Q_N B_{N-1} \\right)^{-1} B_{N-1}^T Q_N \\right] A_{N-1}. \\] We note that \\(S_{N-1}\\) is positive semidefinite (this is an exercise for you to convince yourself). Now we realize that something surprising and nice has happened. The optimal controller \\(u^{\\star}_{N-1}\\) in (2.4) is a linear feedback policy of the state \\(x_{N-1}\\), and The optimal cost-to-go \\(J_{N-1}(x_{N-1})\\) in (2.5) is quadratic in \\(x_{N-1}\\), just the same as \\(J_{N}(x_N)\\). This implies that, if we continue to compute the optimal cost-to-go at time \\(N-2\\), we will again compute a linear optimal controller and a quadratic optimal cost-to-go. This is the rare nice property for the LQR problem, that is, The (representation) complexity of the optimal controller and cost-to-go does not grow as we run the DP recursion backwards in time. We summarize the solution for the LQR problem (2.2) as follows. Proposition 2.1 (Solution of Discrete-Time Finite-Horizon LQR) The optimal controller for the LQR problem (2.2) is a linear state-feedback policy \\[\\begin{equation} \\mu_k^\\star(x_k) = - K_k x_k, \\quad k=0,\\dots,N-1. \\tag{2.6} \\end{equation}\\] The gain matrix \\(K_k\\) can be computed as \\[ K_k = \\left( R_k + B_k^T S_{k+1} B_k \\right)^{-1} B_k^T S_{k+1} A_k, \\] where the matrix \\(S_k\\) satisfies the following backwards recursion \\[\\begin{equation} \\hspace{-6mm} \\begin{split} S_N &amp;= Q_N \\\\ S_k &amp;= Q_k + A_k^T \\left[ S_{k+1} - S_{k+1}B_k \\left( R_k + B_k^T S_{k+1} B_k \\right)^{-1} B_k^T S_{k+1} \\right] A_k, k=N-1,\\dots,0. \\end{split} \\tag{2.7} \\end{equation}\\] The optimal cost-to-go is given by \\[ J_0(x_0) = x_0^T S_0 x_0 + \\sum_{k=0}^{N-1} \\mathbb{E} \\left[ w_k^T S_{k+1} w_k\\right]. \\] The recursion (2.7) is called the discrete-time Riccati equation. Proposition 2.1 states that, to evaluate the optimal policy (2.6), one can first run the backwards Riccati equation (2.7) to compute all the positive definite matrices \\(S_k\\), and then compute the gain matrices \\(K_k\\). For systems of reasonable dimensions, evalutating the matrix inversion in (2.7) should be fairly efficient. 2.1.1 Infinite-Horizon LQR In many robotics applications, it is often more useful to study the infinite-horizon LQR problem \\[\\begin{align} \\min_{u_k} &amp; \\quad \\sum_{k=0}^{\\infty} \\left( x_k^T Q x_k + u_k^T R u_k \\right) \\tag{2.8} \\\\ \\text{subject to} &amp; \\quad x_{k+1} = A x_k + B u_k, \\quad k=0,\\dots,\\infty, \\tag{2.9} \\end{align}\\] where \\(Q \\succeq 0\\), \\(R \\succ 0\\), and \\(A,B\\) are constant matrices. The reason for studying the formulation (2.8) is twofold. First, for nonlinear systems, we often linearize the nonlinear dynamics around an (equilibrium) point we care about, leading to constant \\(A\\) and \\(B\\) matrices. Second, we care more about the asymptotic effect of our controller than its behavior in a fixed number of steps. We will soon see an example of this formulation for balancing a simple pendulum. The infinite-horizon formulation is essentially the finite-horizon formulation (2.2) with \\(N \\rightarrow \\infty\\). Based on our intuition in deriving the finite-horizon LQR solution, we may want to hypothesize that the optimal cost-to-go is a quadratic function \\[\\begin{equation} J_{k}(x_{k}) = x_{k}^T S x_{k}, k=0,\\dots,\\infty \\tag{2.10} \\end{equation}\\] for some positive definite matrix \\(S\\), and proceed to invoke the DP algorithm. Notice that we hypothesize the matrix \\(S\\) is in fact stationary, i.e., it does not change with respect to time. This hypothesis makes sense because the \\(A,B,Q,R\\) matrices are stationary in the formulation (2.8). Invoking the DP algorithm we have \\[\\begin{equation} x_k^T S x_k = J_k(x_k) = \\min_{u_k} \\left\\{ x_k^T Q x_k + u_k^T R u_k + \\Vert \\underbrace{A x_k + B u_k}_{x_{k+1}} \\Vert_S^2 \\right\\}. \\tag{2.11} \\end{equation}\\] The minimization over \\(u_k\\) in (2.11) can again be solved in closed-form by setting the gradient of the objective with respect to \\(u_k\\) to be zero \\[\\begin{equation} u_k^\\star = - \\underbrace{\\left[ \\left( R + B^T S B \\right)^{-1} B^T S A \\right]}_{K} x_k. \\tag{2.12} \\end{equation}\\] Plugging the optimal \\(u_k^\\star\\) back into (2.11), we see that the matrix \\(S\\) has to satisfy the following equation \\[\\begin{equation} S = Q + A^T \\left[ S - SB \\left( R + B^T S B \\right)^{-1} B^T S \\right] A. \\tag{2.13} \\end{equation}\\] Equation (2.13) is the famous algebraic Riccati equation. Let’s zoom out to see what we have done. We started with a hypothetical optimal cost-to-go (2.10) that is stationary, and invoked the DP algorithm in (2.11), which led us to the algebraic Riccati equation (2.13). Therefore, if there actually exists a solution to the algebraic Riccati equation (2.13), then the linear controller (2.12) is indeed optimal (by the optimality of DP)! So the question boils down to if the algebraic Riccati equation has a solution \\(S\\) that is positive definite? The following proposition gives an answer. Proposition 2.2 (Solution of Discrete-Time Infinite-Horizon LQR) Consider a linear system \\[ x_{k+1} = A x_k + B u_k, \\] with \\((A,B)\\) controllable (see Appendix A.2). Let \\(Q \\succeq 0\\) in (2.8) be such that \\(Q\\) can be written as \\(Q = C^T C\\) with \\((A,C)\\) observable. Then the optimal controller for the infinite-horizon LQR problem (2.8) is a stationary linear policy \\[ \\mu^\\star (x) = - K x, \\] with \\[ K = \\left( R + B^T S B \\right)^{-1} B^T S A. \\] The matrix \\(S\\) is the unique positive definite matrix that satisfies the algebraic Riccati equation \\[ S = Q + A^T \\left[ S - SB \\left( R + B^T S B \\right)^{-1} B^T S \\right] A. \\] Moreover, the closed-loop system \\[ x_{k+1} = A x_k + B (-K x_k) = (A - BK) x_k \\] is stable, i.e., the eigenvalues of the matrix \\(A - BK\\) are strictly within the unit circle (see Appendix A.1.2). A rigorous proof of Proposition 2.2 is available in Proposition 3.1.1 of (Bertsekas 2012). The proof basically studies the limit of the discrete-time Riccati equation (2.7) when \\(N \\rightarrow \\infty\\). Indeed, the algebraic Riccati equation (2.13) is the limit of the discrete-time Riccati equation (2.7) when \\(N \\rightarrow \\infty\\). The assumptions of \\((A,B)\\) being controllable and \\((A,C)\\) being observable can be relaxted to \\((A,B)\\) being stabilizable and \\((A,C)\\) being detectable (for definitions of stabilizability and detectability, see Appendix A). We have not discussed how to solve the algebraic Riccati equation (2.7). It is clear that (2.7) is not a linear system of equations in \\(S\\). In fact, the numerical algorithms for solving the algebraic Riccati equation can be highly nontrivial, for example see (Arnold and Laub 1984). Fortunately, such algorithms are often readily available, and as practitioners we do not need to worry about solving the algebraic Riccati equation by ourselves. For example, the Matlab dlqr function computes the \\(K\\) and \\(S\\) matrices from \\(A,B,Q,R\\). Let us now apply the infinite-horizon LQR solution to stabilizing a simple pendulum. Example 2.1 (Pendulum Stabilization by LQR) Consider the simple pendulum dynamics References "],["approximatedp.html", "Chapter 3 Approximate Dynamic Programming 3.1 Introduction 3.2 Approximation in value space 3.3 Approximation in policy space 3.4 Extension", " Chapter 3 Approximate Dynamic Programming Thanks to Jiarui Li for writing this Chapter. 3.1 Introduction The limitations of classical deterministic dynamic programming (DP) were mentioned in Chapter 1, particularly its inefficiency in fields such as robotics where both the state and control spaces are typically large and continuous. The process of discretization in such contexts is not only challenging but also costly. Even when discretization is achievable, the resultant state and control spaces tend to be extraordinarily vast and often high-dimensional, leading to prohibitive computational demands. This issue, commonly called the curse of dimensionality, renders the use of classical DP unfeasible. Add time complexity analysis here. To circumvent the constraints of traditional DP algorithms in such contexts, a pragmatic approach involves the adoption of a suboptimal control scheme. This compromises between the ease of implementation and adequate performance. The principal objective of this chapter is to find such suboptimal control. In this chapter, we will spend most of the time discussing finite horizon problems with discrete state and control space, which is the classical scenario. We will also mention the infinite horizon problem and continuous state and control spaces scenario later. Broadly, two categories of approximation are used in the context of DP-based suboptimal control. The first is approximation in value space, where we aim to approximate the optimal cost function or the cost function of a given policy. The second is approximation in policy space, where we select the policy by using optimization over a suitable class of policies. 3.2 Approximation in value space Let us first recap the iteration process of the generic form of DP as mentioned in theorem 1.2. We can obtain the cost-to-go function \\(J_k\\), which means the cost-to-go value at time \\(k\\), thereby defining corresponding control \\(u_k\\) or policy \\(\\mu_k\\). \\[\\begin{equation} J_k(x_k) = \\min_{u_k \\in \\mathbb{U}} \\displaystyle \\mathbb{E} \\displaystyle \\left\\{ g_k(x_k,u_k) + J_{k+1}(f_k(x_k,u_k,w_k) ) \\right\\}, \\ k=N-1,\\dots,1,0. \\end{equation}\\] By using the approximation in value space methods, we could replace the optimal cost-to-go function \\(J_k\\) with some other functions \\(\\tilde J_k\\). In other words, the suboptimal policy \\(\\tilde{\\mu}_k(x_k)\\) (and the corresponding control) is obtained from the one-step lookahead minimization \\[\\begin{align} \\tilde{J}_k(x_k) &amp;= \\min_{u_k \\in \\mathbb{U}_k (x_k)} \\displaystyle \\mathbb{E} \\displaystyle \\left\\{g_k(x_k,u_k,w_k) + \\tilde J_{k+1} (f_k(x_k,u_k,w_k) ) \\right\\} \\\\ \\tilde{\\mu}_k(x_k) &amp;= \\arg \\min_{u_k \\in \\mathbb{U}_k (x_k)} \\displaystyle \\mathbb{E} \\displaystyle \\left\\{g_k(x_k,u_k,w_k) + \\tilde J_{k+1} (f_k(x_k,u_k,w_k) ) \\right\\} \\tag{3.1} \\end{align}\\] The major issue in value space approximation is how to compute the approximate cost-to-go functions \\(\\tilde J_{k+1}\\) in (3.1). We will consider three types of methods: Problem approximation Parametric cost approximation Online approximate optimization In approximation in value space, we may also distinguish between online and offline methods. Offline methods, where the entire function \\(\\tilde J_{k+1}\\) in (3.1) is computed for every \\(k\\) before the control process begins. The advantage of this is that most of the computation is done offline. Once the control process starts, the only thing we have to do is one-step lookahead minimization. These methods are well-suited for settings where there are strict time constraints for the online computation of the control, and where there is no need for online replanning. Online methods, where most of the computation is performed just after the current state \\(x_k\\) becomes known, the values \\(\\tilde J_{k+1}(x_{k+1})\\) are computed only at the relevant next states \\(x_{k+1}\\) and are used to compute \\(u_k\\) via (3.1). These methods require the computation of control only for the \\(N\\) states actually encountered in the control process. These methods are well-suited for online replanning. 3.2.1 Problem Approximation The functions \\(\\tilde J_{k+1}\\) are obtained (by exact DP, or other methods) as the optimal or nearly optimal cost functions of a simplified version of the original problem. The problem is how to simplify the problem, which is more convenient for computation. There are three widely-used approaches to simplify the initial problem: Simplifying the structure of the problem through enforced decomposition. Simplifying the probabilistic structure of the problem, such as replacing the stochastic problem with a deterministic one by certainty equivalence. To be more specific, the original stochastic system contains the disturbance term \\(w_k(x_k,u_k)\\). To simplify the probabilistic structure of the problem, we could fix the disturbances at some “typical” values and transform the stochastic problem into a deterministic one. Aggregation, where the original problem is approximated with a new problem with fewer states, makes it easier to obtain the cost-to-go function. The state in this new problem is the “combination” of the states in the initial problem. It is worth noting that the discretization of continuous state space and action space could be viewed as a kind of aggregation. 3.2.2 Parametric cost approximation For discrete problems, it is natural to consider using the tabular method to represent the \\(\\tilde J_k\\) functions. However, if the number of the state space is large this method’s memory cost will be overwhelming. On the other hand, for tabular representation, it is not convenient to optimize the function, while we can only update the value of one state at a time, but in many circumstances, there is a cluster of states that have similar attributes, which means their corresponding \\(\\tilde J_k\\) are also similar. It is inconvenient to update them one by one. In this part, we will discuss an alternative approach to represent \\(\\tilde J_k\\) function, whereby \\(\\tilde J_k\\) are chosen to be members of a parametric class of functions, with the parameters “optimized” or “trained” by using some algorithms. To be more specific, the \\(\\tilde J_k\\) functions could be described as \\(\\tilde J_k (x_k,r_k)\\) that for each \\(k\\), depend on the current state \\(x_k\\) and a vector \\(r_k=(r_{1,k}, ..., r_{m,k})\\) of \\(m\\) “tunable” scalar parameters, also called weights. By adjusting the weights, one can change the “shape” of \\(\\tilde J_k\\) so that it is a reasonably close approximation to the true cost-to-go function \\(J_k\\). In order to train those weights, we can use some cost functions to measure the accuracy of the approximation. The most common cost function is least squares. Training the parameters \\(r_k\\) using least squares as the cost function is sometimes referred to as fitted value iteration. Value iteration could be viewed as a special form of dynamic programming, where the parameter vectors \\(r_k\\) are determined sequentially, starting from the end of the horizon and proceeding backward. The algorithm samples the state space for each stage \\(k\\) and generates a large number of states \\(x_k^s\\), \\(s=1,...,q\\). It then determines sequentially the parameter vectors \\(r_k\\) to obtain a good “least square fit” to the DP algorithm. \\[\\begin{equation} \\beta_k^s=\\min_{u \\in \\mathbb U_k(x_k^s)} E \\displaystyle \\left\\{g(x_k^s,u,w_k) + \\tilde J_{k+1} (f_k(x_k^s,u,w_k),r_{k+1}) \\right\\} \\tag{3.2} \\end{equation}\\] \\[\\begin{equation} r_k = \\arg \\min_r \\sum_{s=1}^q (\\tilde J_k(x_k^s,r) - \\beta_k^s)^2 \\tag{3.3} \\end{equation}\\] The next question is how to choose the most suitable class of functions, which is called approximation architecture. It is obvious that approximation architecture can greatly affect the performance of the approximation and the difficulty of training. The most popular architecture is neural networks, which are widely used in reinforcement learning, but the optimization process is difficult and the optimality is not guaranteed. We will start with a simpler linear feature-based approximation architecture. 3.2.2.1 Linear feature-based architecture In this architecture, the \\(\\tilde J_k\\) function could be parameterized as follows: \\[\\begin{equation} \\tilde J_k(x_k, r_k)=r_k^T \\phi_k(x_k) \\end{equation}\\] Here \\(T\\) means the transpose of the matrix, \\(\\phi_k(x_k)\\) is pre-selected and called the (non-linear) feature vector associated with \\(x_k\\) at time \\(k\\). The scalar components of the feature vector are called features. Common examples of features include polynomials and radial basis functions. The notion of feature is commonly used in the theory of computer vision, where \\(x_k\\) could be interpreted as an image, and the \\(\\phi_k\\) function extracts the critical features such as angles and points that could be used for object recognition or image alignment. By using this architecture, the fitted value iteration (3.2), (3.3) greatly simplifies and admits a closed-form solution. Example 3.1 (Swinging up a pendulum using feature-based method) EXPERIMENT HERE Figure 3.1: example of swinging up a pendulum using feature-based value function approximation It is worth noting that the cost-to-go function in Linear Quadratic Regulator is also using the linear architecture mentioned above. \\[\\begin{equation} J(x,S)=x^T S x \\end{equation}\\] It is quadratic in \\(x\\) but linear in \\(S\\). Therefore, except for using the Recatti equation to solve for S, we could also try to use fitted value iteration to handle it. 3.2.2.2 Neural networks The selection of features is frequently manually crafted, relying on human intellect, intuition, or experience, and can pose considerable challenges. The utilization of a neural network as the approximation architecture has emerged as a popular approach in recent years. In this context, the parameter \\(r_k\\) may correspond to the weights of the neural networks. A diverse range of machine learning techniques can then be implemented to manage the training problem, steering the approximation toward the optimal value. However, the optimization process of the weights is more difficult due to the non-convexity. According to the equation (3.1), the \\(J_{k+1}\\) is non-convex which makes the entire equation hard to optimize. Example 3.2 (Swinging up a pendulum using NN-based method) EXPERIMENT HERE Figure 3.2: example of swinging up a pendulum using NN-based value function approximation 3.2.3 Online approximate optimization Different from previous sections, in this section, we will discuss online approaches for computing the one-step lookahead control \\(u_k\\) just after the current state \\(x_k\\) becomes known. Here, to compute \\(u_k\\), the values \\(\\tilde J_{k+1} (x_{k+1})\\) need only be computed at the relevant next states \\(x_{k+1}\\) (the ones that can occur following application of \\(u_k\\)). A particularly effective online approach is rollout. In rollout algorithm, \\(\\tilde J_{k+1}(x_{k+1})\\) is calculated by a suboptimal policy, or base policy. \\(\\tilde J_{k+1}\\) could be calculated either analytically or by Monte Carlo simulation. This part is interconnected with model predictive control (MPC), which we will also discuss at the end of this section. 3.2.3.1 Rollout algorithm The essence of the rollout is policy improvement, which generates a better policy on top of the base policy. In the rollout algorithm, \\(\\tilde J_{k+1}\\) is the cost-to-go of some known suboptimal policy \\(\\pi = \\{\\mu_0,...,\\mu_{N-1}\\}\\), referred to as base policy. The policy \\(\\bar \\pi=\\{\\bar\\mu_0,...,\\bar\\mu_{N-1}\\}\\) thus obtained is called the rollout policy based on \\(\\pi\\). In short, the rollout policy is the one-step lookahead policy, with the optimal cost-to-go approximated by the cost-to-go of the base policy_. Definition 3.1 (One-step Rollout Algorithm) We can get an improved policy from the base policy \\(\\pi\\) \\[\\begin{equation} \\bar\\mu_k(x_k) = \\arg\\min_{u_k\\in\\mathbb U_k(x_k)} E\\left\\{g_k(x_k,u_k,w_k)+\\tilde J_{k+1}(f_k(x_k,u_k,w_k))\\right\\} \\tag{3.4} \\end{equation}\\] where \\(\\tilde J_{k+1}\\) is the corresponding cost-to-go function of the base policy \\(\\pi\\). If we use \\(H_{k+1}\\) to represent the cost-to-go function of the base policy \\(\\pi\\), the rollout algorithm will be: \\[\\begin{equation} \\bar\\mu_k(x_k) = \\arg\\min_{u_k\\in\\mathbb U_k(x_k)} E\\left\\{g_k(x_k,u_k,w_k)+H_{k+1}(f_k(x_k,u_k,w_k))\\right\\} \\tag{3.5} \\end{equation}\\] In the control system, after the current state \\(x_k\\) is revealed, we calculate the cost-to-go function \\(H_{k+1}\\) of the known base policy \\(\\pi\\) and conduct one-step lookahead minimization to find \\(\\bar\\mu_k(x_k)\\) and feed it into the system immediately. Note that it is also possible to define the rollout policy that makes use of multistep lookahead. While such multistep lookahead involves much more online computation, it will likely yield better performance than its one-step counterpart. In what follows, we concentrate on rollout policy with one-step lookahead. Theorem 3.1 (Cost improvement property of rollout algorithm) It is possible to show that the rollout policy’s performance is no worse than the one of the base policy, while some special conditions must hold to guarantee this cost improvement property. Here we introduce the sequential improvement condition. We say that the base policy has sequential improvement property if, for all \\(x_k\\) and \\(k\\), we have \\[\\begin{equation} \\min_{u_k \\in \\mathbb U_k(x_k)} \\left\\{g_k(x_k,u_k)+H_{k+1}(f_k(x_k,u_k))\\right\\} \\leq H_k(x_k) \\end{equation}\\] where \\(H_k(x_k)\\) denotes the cost of the base policy starting from \\(x_k\\). Here we use deterministic problems to make our proof concise. Sometimes people also use the Q factor mentioned below: \\[\\begin{equation} \\tilde Q_k(x_k,u_k) = g_k(x_k,u_k)+H_{k+1}(f_k(x_k,u_k)) \\end{equation}\\] so now the sequential improvement property could also be written as: \\[\\begin{equation} \\min_{u_k \\in \\mathbb U_k(x_k)} \\tilde Q_k(x_k,u_k) \\leq H_k(x_k) \\end{equation}\\] We will now show that the rollout algorithm obtained with a base policy with sequential improvement property yields no worse cost than the base policy. In particular, consider the rollout policy \\(\\tilde \\pi = \\{\\tilde \\mu_0, ..., \\tilde \\mu_{N-1}\\}\\), and let \\(J_{k, \\tilde \\pi} (x_k)\\) denote the cost obtained with \\(\\tilde \\pi\\) starting from \\(x_k\\). We claim that \\[\\begin{equation} J_{k,\\tilde \\pi} (x_k) \\leq H_k(x_k), for \\ all \\ x_k \\ and \\ k \\tag{3.6} \\end{equation}\\] Proof. We prove this inequality by induction. Clearly it holds for \\(k=N\\), since \\(J_{N,\\tilde \\pi} = H_N = g_N\\). Assume it holds for index \\(k+1\\). We have: \\[\\begin{align} \\tilde J_{k, \\tilde \\pi}(x_k) &amp;= g_k(x_k, \\tilde \\mu_k(x_k)) + J_{k+1,\\tilde \\pi}(f_k(x_k, \\tilde \\mu_k(x_k))) \\\\ &amp;\\leq g_k(x_k, \\tilde \\mu_k(x_k)) + H_{k+1}(f_k(x_k, \\tilde \\mu_k(x_k))) \\\\ &amp;= \\min_{u_k \\in \\mathbb U_k(x_k)} \\left[g_k(x_k,u_k) + H_{k+1} (f_k(x_k,u_k))\\right] \\\\ &amp;= \\min_{u_k \\in \\mathbb U_k(x_k)} \\tilde Q_k(x_k,u_k) \\\\ &amp;\\leq H_k(x_k) \\end{align}\\] where: The first equality is the DP equation for the rollout policy \\(\\tilde \\pi\\). The first inequality holds by the induction hypothesis. The second equality holds by the definition of the rollout algorithm. The second inequality holds by the sequential improvement property. This completes the induction proof of the cost improvement property (3.6). Computational issues in rollout algorithms. In the rollout algorithm, the cost-to-go function \\(H_{k+1}\\) of the base policy is required to be computed online at all possible next states \\(f_k(x_k,u_k,w_k)\\). However, the real-time constraint will be a critical problem, for the corresponding cost-to-go function of a given base policy is not easy to calculate in real-time. In most cases, we will use the approximate version of the cost-to-go \\(\\tilde H_{k+1}\\) to simplify the calculation. So the rollout algorithm will be: \\[\\begin{equation} \\bar\\mu_k(x_k) = \\arg\\min_{u_k\\in\\mathbb U_k(x_k)} E\\left\\{g_k(x_k,u_k,w_k)+\\tilde H_{k+1}(f_k(x_k,u_k,w_k))\\right\\} \\tag{3.7} \\end{equation}\\] There are two variants to handle the computational difficulties, deterministic case, and stochastic case. Deterministic case. If the problem is deterministic, the calculation is greatly simplified. Stochastic case. In these cases, the \\(\\tilde H_{k+1}\\) are evaluated online by Monte Carlo simulation for all \\(u_k \\in \\mathbb U_k(x_k)\\). Truncated rollout algorithm with multistep lookahead and terminal cost approximation. We may incorporate multistep lookahead into the rollout framework. Let us start with a two-step lookahead for deterministic problems. Suppose that after \\(k\\) steps we have reached state \\(x_k\\). We then consider the set of all two-step-ahead states \\(x_{k+2}\\), run the base policy starting from each of them, and compute the two-stage cost to get from \\(x_k\\) to \\(x_{k+2}\\), plus the cost of the base policy from \\(x_{k+2}\\). We select the state, say \\(\\tilde x_{k+2}\\), that is associated with minimum cost, compute the controls \\(\\tilde u_k\\) and \\(\\tilde u_{k+1}\\) that lead from \\(x_k\\) to \\(\\tilde x_{k+2}\\), and choose \\(\\tilde u_k\\) as the next rollout control and \\(x_{k+1}=f_k(x_k,\\tilde u_k)\\) as the next state. Figure 3.3: Illustration of truncated rollout with two-step lookahead The extension of the algorithm to lookahead of more than two steps is straightforward: instead of the two-step-ahead states \\(x_{k+2}\\) we run the base policy starting from all the possible \\(l\\)-step ahead states \\(x_{k+l}\\), etc. An important variation for problems with a large number of stages is truncated rollout with terminal cost approximation. Here the rollout trajectories are obtained by running the base policy from the leaf nodes of the lookahead tree, and they are truncated after a given number of steps, while a terminal cost approximation is added to the policy cost to compensate for the resulting error. One possibility that works well for many problems is to simply set the terminal cost approximation to zero. Alternatively, the terminal cost function approximation may be obtained by problem approximation or by using some sophisticated offline training process that may involve an approximation architecture such as a neural network. 3.2.3.2 Model predictive control (MPC) In this section, we will discuss a popular control algorithm called model predictive control (MPC). We will start by considering the case where the objective is to keep the state close to the origin (or more generally some point of interest, called the set point, or fixed point); this is called the regulation problem. Similar approaches have been developed for the problem of maintaining the state of a non-stationary system along a given state trajectory, and also, with appropriate modifications, to control problems involving disturbances. In particular, in some cases, the trajectory is treated like a sequence of set points, and the subsequently described algorithm is applied repeatedly. We will consider a deterministic system \\[\\begin{equation} x_{k+1} = f_k(x_k,u_k) \\end{equation}\\] whose state \\(x_k\\) and control \\(u_k\\) are vectors that consist of a finite number of scalar components. The cost per stage is assumed nonnegative \\[\\begin{equation} g_k(x_k,u_k) \\geq 0, for \\ all \\ (x_k,u_k) \\end{equation}\\] (e.g., a quadratic cost). We impose state and control constraints \\[\\begin{equation} x_k \\in \\mathbb X_k, u_k \\in \\mathbb U_k(x_k), k = 0,1,... \\end{equation}\\] We also assume that the system can be kept at the origin at zero cost, i.e., \\[\\begin{equation} f_k(0,\\bar u_k)=0,g_k(0,\\bar u_k)=0 \\end{equation}\\] for some control \\(\\bar u_k \\in \\mathbb U_k(0)\\). This is a characteristic that all fixed points possess. For a given initial state \\(x_0 \\in \\mathbb X_0\\), we want to obtain a sequence \\(\\{u_0,u_1,...\\}\\) such that the states and controls of the system satisfy the state and control constraints with a small total cost. The MPC algorithm. Let us describe the MPC algorithm for the deterministic problem just described. At the current state \\(x_k\\): MPC solves an \\(l\\)-step lookahead version of the problem, which requires that \\(x_{k+l}=0\\). If \\(\\{\\tilde u_k, ..., \\tilde u_{k+l-1}\\}\\) is the optimal control sequence of this problem, MPC applies \\(\\tilde u_k\\) and discards the other controls \\(\\tilde u_{k+1}, ..., \\tilde u_{k+l-1}\\). At the next stage, MPC repeats this process, once the next state \\(x_{k+1}\\) is revealed. In some literature, this MPC algorithm is also called Receding Horizon Control algorithm, or RHC for short. One obvious drawback of this method is the online computation time limit. The MPC algorithm needs to solve an optimization problem online, which is time-consuming and does not guarantee a solution. To make the connection between MPC and rollout, we first recap the case of the truncated rollout algorithm. In a truncated rollout algorithm with multistep lookahead and terminal cost approximation, \\[\\begin{equation} \\min_{u_k \\in \\mathbb U_k(x_k), ..., u_{k+l} \\in \\mathbb U_{k+l}(x_{k+l})} \\left\\{\\sum_{i=k}^{k+l} g_i(x_i, u_i) + \\sum_{i=k+l+1}^{k+l+m} g_i(x_i,\\mu_i(x_i)) + \\tilde J (x_{k+l+m+1}) \\right\\} \\end{equation}\\] such that \\[\\begin{equation} x_{i+1} = f_i(x_i,u_i) \\end{equation}\\] The control \\(u_k\\) will be used as the control at step \\(k\\) (online current step). All the \\(x_i\\) are admissible states. Here \\(\\tilde J\\) means the terminal cost approximation, which can be obtained through offline computation or sometimes be set to zero. \\(l\\) means the number of lookahead steps, and \\(m\\) means the number of steps that the base policy runs to evaluate the cost-to-go function \\(H_{k+l+1}\\). Let us discuss a special case, where the \\(\\tilde J\\) is set to zero while \\(m\\) is also zero. So now the rollout algorithm becomes: \\[\\begin{equation} \\min_{u_k \\in \\mathbb U_k(x_k), ..., u_{k+l} \\in \\mathbb U_{k+l}(x_{k+l})} \\sum_{i=k}^{k+l} g_i(x_i, u_i) \\end{equation}\\] while \\(u_k\\) still be used as the current online control, and all other optimized controls are discarded. We can see that now it is almost the case of model predictive control, without the terminal state constraint (in this case the terminal state constraint is \\(x_{k+l+1}=0\\)). This constraint is also called recursive feasibility, for it guarantees the optimization will not suddenly encounter a situation where the solver returns “infeasible”. 3.3 Approximation in policy space A major alternative to approximation in value space is approximation in policy space, whereby we select the policy from a suitably restricted class of policies, usually a parametric class of some form. In particular, we can introduce a parametric family of policies \\[\\begin{equation} \\mu_k(x_k,r_k), k=0,...,N-1 \\end{equation}\\] where \\(r_k\\) is a parameter, such as a family represented by a neural network, and then estimate the parameters \\(r_k\\) using some type of optimization. An important advantage of approximation in policy space is that the computation of controls during the online operation of the system is often much easier compared with the lookahead minimization (3.1). In this section, we will present two distinct approaches for computing \\(r\\): training by cost optimization and training by using an expert. 3.3.1 Training by using an expert This approach is pretty similar to supervised learning in machine learning. We \\(r_k\\) by “training” on a large number of sample state-control pairs \\((x_k^s, u_k^s), s=1, ... ,q\\), such that for each \\(s\\), \\(u_k^s\\) is a “good” control at state \\(x_k^s\\). This can be done for example by solving for each \\(k\\) the least squares problem \\[\\begin{equation} \\min_{r_k} \\sum_{s=1}^q \\left\\Vert{u_k^s - \\tilde \\mu_k(x_k^s,r_k)}\\right\\Vert^2 \\end{equation}\\] (possibly with added regularization). In particular, we may determine \\(u_k^s\\) by a human or a software “expert” that can choose “near-optimal” controls at the given states \\(x_k^s\\), so \\(\\tilde{\\mu}_k\\) is trained to match the behavior of the expert. Of course, in the expert training approach, we cannot expect to obtain a controller that performs better than the expert with which it is trained. The “near-optimal” controls of sampled states \\(x_k^s, s = 1, ...,q\\) could also be calculated from one-step lookahead minimization with a suitable approximation \\(\\tilde{J}_{k+1}\\). \\[\\begin{equation} u_k^s = \\arg \\min_{u_k \\in \\mathbb{U}_k (x_k)} \\displaystyle \\mathbb{E} \\displaystyle \\left\\{g_k(x_k^s,u_k,w_k) + \\tilde J_{k+1} (f_k(x_k^s,u_k,w_k) ) \\right\\} \\end{equation}\\] 3.3.2 Training by cost optimization POLICY GRADIENT 3.4 Extension It is possible for a suboptimal control scheme to employ both types of approximation: in policy space and in value space, with a distinct architecture for each case. This is known as the simultaneous use of a “policy network” (or “actor network”) and a “value network” (or “critic network”), each with its own set of parameters. Simultaneous approximation in policy space and value space through the use of deep neural networks are central in AlphaGo and AlphaZero, DeepMind’s Go and chess playing programs. "],["stability.html", "Chapter 4 Stability Analysis 4.1 Autonomous Systems 4.2 Controlled Systems 4.3 Non-autonomous Systems", " Chapter 4 Stability Analysis Optimal control formulates a control problem via the language of mathematical optimization. However, there are control problems, and sometimes even the very basic control problems, that cannot be easily stated in the optimal control formulation. For example, suppose our goal is to swing up a pendulum to the upright position and stabilize it there. You may want to formalize the problem as \\[\\begin{equation} \\min_{u(t) \\in \\mathbb{U}} \\int_{0}^{\\infty} \\Vert x(t) - x_d \\Vert^2 dt, \\quad \\text{subject to} \\quad \\dot{x} = f(x,u), x(0) = x_0, \\tag{4.1} \\end{equation}\\] where \\(x_d\\) is the desired upright position for the pendulum. However, does the solution of problem (4.1), if exists, guarantee the stabilization of the pendulum at the upright position? The answer is unclear without a rigorous proof. However, after a slight change of perspective, the optimal control problem may be formulated to better match the goal. Suppose there exists a region, \\(\\Omega\\), in the state space such that as long as the pendulum enters \\(\\Omega\\), there always exists a sequence of control to bring the pendulum to the goal state \\(x_d\\), then we can simply formulate a different optimal control problem \\[\\begin{equation} \\min_{u(t) \\in \\mathbb{U}} \\int_{0}^{T} \\Vert u(t) \\Vert^2 dt, \\quad \\text{subject to} \\quad x(0)=x_0, x(T) \\in \\Omega, \\dot{x} = f(x,u), \\tag{4.2} \\end{equation}\\] where now it is very clear, if a solution exists to problem (4.2), then we will definitely achieve our goal. This is because the constraint \\(x(T) \\in \\Omega\\) guarantees that we will be able to stabilize the pendulum, and the cost function of (4.2) simply encourages minimum control effort along the way. This highlights that, sometimes the formulation of a problem may deserve more thoughts than the actual solution. Of course the formulation (4.2) may be much more difficult to solve. In fact, does the set \\(\\Omega\\) exist, and if so, how to describe it? This is the main focus of this chapter: to introduce tools that can help us analyze the stability of uncontrolled and controlled nonlinear systems. Specifically, we will introduce the notion of stability certificates, which are conditions that, if hold, certify the stability of the system (e.g., in the set \\(\\Omega\\)). Interestingly, you will see that the notion of stability certificates is intuitive and easy, but what is really challenging is to find and compute the stability certificates. We will highlight the power and also limitation of computational tools, especially those that are based on convex optimization (see Appendix B for a review of convex optimization). 4.1 Autonomous Systems Let us first focus on autonomous systems, i.e., systems whose dynamics do not depent on time (and control). We introduce different concepts of stability and ways to certify them. 4.1.1 Concepts of Stability Consider the autonomous system \\[\\begin{equation} \\dot{x} = f(x) \\tag{4.3} \\end{equation}\\] where \\(x \\in \\mathbb{X} \\subseteq \\mathbb{R}^n\\) is the state and \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) is the (potentially nonlinear) dynamics. Before talking about concepts of stability, we need to define an equilibrium point. Definition 4.1 (Equilibrium Point) A state \\(x^\\star\\) is called an equilibrium point of system (4.3) if \\(f(x^\\star) = 0\\), i.e., once the system reaches \\(x^\\star\\), it stays at \\(x^\\star\\). For example, a linear system \\[ \\dot{x} = A x \\] has a single equilibrium point \\(x^\\star = 0\\) when \\(A\\) is nonsingular, and an infinite number of equilibrium points when \\(A\\) is singular (those equilibrium points lie in the kernel of matrix \\(A\\)). When analyzing the behavior of a dynamical system around the equilibrium point, it is often helpful to “shift” the dynamics equation so that \\(0\\) is the equilibrium point. For example, if we are interested in the behavior of system (4.3) near the equilibrium point \\(x^\\star\\), we can create a new variable \\[ z = x - x^\\star, \\] so that \\[\\begin{equation} \\dot{z} = \\dot{x} = f(x) = f(z + x^\\star). \\tag{4.4} \\end{equation}\\] Clearly, \\(z^\\star = 0\\) is an equilibrium point for the shifted system (4.4). Let us find the equilibrium points of a simple pendulum. Example 4.1 (Equilibrium Points of A Simple Pendulum) Consider the dynamics of an uncontrolled pendulum \\[\\begin{equation} \\begin{cases} \\dot{\\theta} = \\dot{\\theta} \\\\ \\ddot{\\theta} = - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) \\end{cases} \\tag{4.5} \\end{equation}\\] where \\(\\theta\\) is the angle between the pendulum and the vertical line, and \\(x = [\\theta,\\dot{\\theta}]^T\\) is the state of the pendulum (\\(m,g,l,b\\) denote the mass, gravity constant, length, and damping constant, respectively). To find the equilibrium points of the pendulum, we need the right hand sides of (4.5) to be equal to zero: \\[ \\dot{\\theta} = 0, \\quad - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) = 0. \\] The solutions are easy to find \\[ x^\\star = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\quad \\text{or} \\quad \\begin{bmatrix} \\pi \\\\ 0 \\end{bmatrix}, \\] corresponding to the bottomright and upright positions of the pendulum, respectively. The pendulum dynamics has two equilibrium points, but our physics intuition tells us these two equilibrium points are dramatically different. Specifically, the bottomright equilibrium \\(x^\\star = [0,0]^T\\) is such that if you perturb the pendulum around the equilibrium, the pendulum will go back to that equilibrium; the upright equilibrium \\(x^\\star = [\\pi,0]^T\\) is such that if you perturb the pendulum (even just a little bit) around the equilibrium, it will diverge from that equilibrium. This physical intuition is exactly what we want to formalize as the concepts of stability. In the following, we focus on the nonlinear autonomous system (4.3) with \\(f(0) = 0\\), i.e., \\(x^\\star = 0\\) is an equilibrium point. We now formally define the different concepts of stability. Definition 4.2 (Lyapunov Stability) The equilibrium point \\(x=0\\) is said to be stable in the sense of Lyapunov if, for any \\(R &gt; 0\\), there exists \\(r &gt;0\\) such that if \\(\\Vert x(0) \\Vert &lt; r\\), then \\(\\Vert x(t) \\Vert &lt; R\\) for all \\(t \\geq 0\\). Otherwise, the equilibrium point is unstable. For a system that is Lyapunov stable around \\(x=0\\), the definition says that, if we want to constrain the trajectory of the system to be within the ball \\(B_R = \\{ x \\mid \\Vert x \\Vert &lt; R \\}\\), then we can always find a smaller ball \\(B_r = \\{ x \\mid \\Vert x \\Vert &lt; r \\}\\) such that if the system starts within \\(B_r\\), it will remain in the larger ball \\(B_R\\). On the other hand, if the system is not Lyapunov stable at \\(x=0\\), then there exists at least one ball \\(B_R\\), such that no matter how close the system’s initial condition is to the origin, it will eventually exit the ball \\(B_R\\). The following exercise is left for you to verify the instability of the Van der Pol oscillator. Exercise 4.1 (Instability of the Van der Pol oscillator) Show that the Van der Pol oscillator \\[ \\begin{cases} \\dot{x}_1 = x_2 \\\\ \\dot{x}_2 = - x_1 + (1-x_1^2) x_2 \\end{cases} \\] is unstable at the equilibrium point \\(x = 0\\). Lyapunov stability does not guarantee the system trajectory will actually converge to \\(x =0\\). Instead, asymptotic stability will ask the system trajectory to converge to \\(x=0\\). Definition 4.3 (Asymptotic Stability and Domain of Attraction) The equilibrium point \\(x = 0\\) is said to be asymptotically stable if (i) it is Lyapunov stable, and (ii) there exists some \\(r &gt; 0\\) such that \\(x(0) \\in B_r\\) implies \\(x(t) \\rightarrow 0\\) as \\(t \\rightarrow 0\\). The domain of attraction (for the equilibrium \\(x=0\\)) is the largest set of points in the state space such that trajectories initiated at those points will converge to the equilibrium point. That is, \\[ \\Omega(x^\\star) = \\{ x \\in \\mathbb{X} \\mid x(0) = x \\Longrightarrow \\lim_{t \\rightarrow \\infty} x(t) = x^\\star \\}. \\] The ball \\(B_r\\) is a domain of attraction for the equilibrium point \\(x=0\\), but not necessarily the largest domain of attraction. You may immediately realize that in the definition of asymptotic stability, we require Lyapunov stability to hold first. Is this necessary? i.e., does there exist a system where trajectories eventually converge to zero, but is not stable in the sense of Lyapunov? You should work out the following exercise. Exercise 4.2 (Vinograd System) Show that for the Vinograd dynamical system (Vinograd 1957) \\[ \\begin{cases} \\dot{x} = \\frac{x^2(y-x) + y^5}{(x^2+y^2)(1 + (x^2+y^2)^2)} \\\\ \\dot{y} = \\frac{y^2 (y - 2x)}{(x^2+y^2)(1 + (x^2+y^2)^2)} \\end{cases}, \\] all system trajectories converge to the equilibrium point \\((x,y) = 0\\), but the equilibrium point is not stable in the sense of Lyapunov. (Hint: the system trajectories will behave like the following plot.) Figure 4.1: Trajectories of the Vinograd system. Copied from the original article of Vinograd. In many cases, we want the convergence of the system trajectory towards \\(x=0\\) to be fast, thus bringing in the notion of exponential stability. Definition 4.4 (Exponential Stability) An equilibrium point \\(x=0\\) is said to be exponentially stable, if there exists a ball \\(B_r\\) such that as long as as \\(x(0) \\in B_r\\), then \\[ \\Vert x(t) \\Vert \\leq \\alpha \\Vert x(0) \\Vert e^{-\\lambda t}, \\quad \\forall t, \\] for some \\(\\alpha &gt; 0\\) and \\(\\lambda &gt; 0\\) (\\(\\lambda\\) is called the rate of exponential convergence). Exponential stability implies asymptotic stability (and certainly also Lyapunov stability). What is nice about exponential stability is that we can quantify the distance of the system trajectory to the equilibrium point as a function of time (as long as we know the constants \\(\\alpha, \\Vert x(0) \\Vert, \\lambda\\)). In many safety-critical applications, we need such performance guarantees. For example, in Chapter 5.1, we will see the application of exponential stability in observer-feedback control. All the concepts of stability we have mentioned so far only talk about the stability of the system locally around the equilibrium point \\(x=0\\) (via arguments like \\(B_r\\) and \\(B_R\\)). It would be much nicer if we can guarantee stability of the system globally, i.e., no matter where the system starts in the state space \\(\\mathbb{X}\\), its trajectoy will converge to \\(x=0\\). Definition 4.5 (Global Asymptotic and Exponential Stability) The equilibrium point \\(x = 0\\) is said to be globally asymptotically (exponentially) stable if asymptotic (exponential) stability holds for any initial states. That is, \\[ \\forall x \\in \\mathbb{X}, \\quad x(0) = x \\Longrightarrow \\begin{cases} \\lim_{t \\rightarrow \\infty} x(t) = 0 &amp; \\text{global asymptotic stability} \\\\ \\exists \\alpha, \\lambda &gt; 0, \\text{ s.t. } \\Vert x(t) \\Vert \\leq \\alpha \\Vert x(0) \\Vert e^{-\\lambda t} &amp; \\text{global exponential stability} \\end{cases} \\] This concludes our definitions of stability for nonlinear systems (Definition 4.2-4.5). It is worth mentioning that the concepts of stability are complicated (refined) here due to our focus on nonlinear systems. For linear systems, the concepts of stability are simpler. Specifically, all local stability properties of linear systems are also global and asymptotic stability is equal to exponential stability. In fact, for a linear time-invariant system \\(\\dot{x} = Ax\\), it is either asymptotically (exponentially) stable, or marginally stable, or unstable. Moreover, we can fully characterize the stability property by inspecting the eigenvalues of \\(A\\) (you can find a refreshment of this in Appendix A.1). How do we characterize the stability property of a nonlinear system? If someone gave me a nonlinear system (4.3), how can I provide a certificate to her that the system is stable or unstable (I cannot use eigenvalues anymore in this case)? Let us describe some of these certificates below. 4.1.2 Stability by Linearization A natural idea is to linearize, if possible, the nonlinear system (4.3) at a given equilibrium point \\(x^\\star\\) and inspect the stability of the linearized system (for which we can compute eigenvalues). Therefore, the key question here is how does the stability and instability of the linearized system relate to the stability and instability of the original nonlinear system. Theorem 4.1 (Stability by Linearization) Assume \\(x=0\\) is an equilibrium point of system (4.3) and \\(f\\) is continuously differentiable. Let \\[\\begin{equation} \\dot{x} = Ax, \\quad A = \\frac{\\partial f}{\\partial x} \\Big\\vert_{x=0} \\tag{4.6} \\end{equation}\\] be the linearized system at \\(x=0\\). The following statements are true about the stability relationship between (4.3) and (4.6). If the linearized system (4.6) is strictly stable (i.e., all eigenvalues of \\(A\\) have strictly negative real parts), then the original system (4.3) is asymptotically stable at \\(x=0\\). If the linearized system (4.6) is unstable (i.e., at least one eigenvalue of \\(A\\) has strictly positive real part), then the original system (4.3) is unstable at \\(x=0\\). If the linearized system (4.6) is marginally stable (i.e., all eigenvalues of \\(A\\) have nonpositive real parts, and at least one eigenvalue has zero real part), then the stability of the original system (4.3) at \\(x=0\\) is indeterminate. Theorem 4.1 is actually quite useful when we want to quickly examine the local stability of a nonlinear system around a given equilibrium point, as we will show in the next example. Example 4.2 (Stability of A Simple Pendulum by Linearization) Consider the simple pendulum dynamics (4.5) in Example 4.1. Without loss of generality, let \\(m=1,l=1,b=0.1\\). The Jacobian of the nonlinear dynamics reads \\[ A = \\frac{\\partial f}{\\partial x} = \\begin{bmatrix} 0 &amp; 1 \\\\ -\\frac{g}{l} \\cos \\theta &amp; -\\frac{b}{ml^2} \\end{bmatrix}. \\] At the bottomright equilibrium point \\(\\theta =0, \\dot{\\theta} = 0\\), the matrix \\(A\\) has two eigenvalues \\[ -0.0500 \\pm 3.13i, \\] and hence the pendulum is asymptotically stable at the bottomright equilibrium point. At the upright equilibrium point \\(\\theta =\\pi, \\dot{\\theta} = 0\\), the matrix \\(A\\) has two eigenvalues \\[ 3.08, \\quad -3.18, \\] and hence the pendulum is unstable at the upright equilibrium point. The linearization method is easy to carry out. However, it tells us nothing about global stability or exponential stability. Moreover, when the linearized system is marginally stable, the stability of the orignal system is inconclusive. In the next, we will introduce a more general, and perhaps the most popular framework for analyzing the stability of nonliear systems. 4.1.3 Lyapunov Analysis The basic idea of Lyapunov analysis is quite intuitive: if can find an “energy-like” scalar functin for a system such that the scalar function is zero at an equilibrium point and positive everywhere else, and the time-derivative of the scalar function is zero at the equilibrium point but negative otherwise, then we know that the energy of the system will eventually converge to zero, and hence the state trajectory will converge to the equilibrium point. Lyapunov analysis was originally inspired by the energy function of a mechanical system: the total energy of a mechanical system (potental energy plus kinetic energy) will settle down to its minimum value if it is constantly dissipated (e.g., due to damping). However, the concept of a Lyapunov function is much broader than the energy function, i.e., it can be an arbitrary abstract function without any physical meaning. Let us now introduce the concept of a Lyapunov function. Definition 4.6 (Positive Definite Function) A scalar function \\(V(x)\\) is said to be locally positive definite in a ball \\(B_R\\) if \\[ V(0) = 0 \\quad \\text{and} \\quad V(x) &gt; 0, \\forall x \\in B_R \\backslash \\{0\\}, \\] and globally positive definite if \\[ V(0) = 0 \\quad \\text{and} \\quad V(x) &gt; 0, \\forall x \\in \\mathbb{X} \\backslash \\{0\\}, \\] where \\(\\mathbb{X}\\) is the entire state space. A function \\(V(x)\\) is said to be negative definite if \\(-V(x)\\) is positive definite. A function \\(V(x)\\) is said to be positive semidefinite if the “\\(&gt;\\)” sign is replaced by the “\\(\\geq\\)” sign in the above equations. A function \\(V(x)\\) is said to be negative semidefinite if \\(-V(x)\\) is positive semidefinite. For example, when \\(\\mathbb{X} = \\mathbb{R}^2\\), the function \\(V(x) = x_1^2 + x_2^2\\) is positive definite, but the function \\(V(x) = x_1^2\\) is only positive semidefinite. Definition 4.7 (Lyapunov Function) In the ball \\(B_R\\), if a function \\(V(x)\\) is positive definite, and its time derivative along any system trajectory \\[ \\dot{V}(x) = \\frac{\\partial V}{\\partial x} f(x) \\] is negative semidefinite (we assume the partial derivative \\(\\frac{\\partial f}{\\partial x}\\) exists and is continuous), then \\(V(x)\\) is said to be a Lyapunov function for system (4.3). Note that \\(\\dot{V}(x^\\star) = 0\\) at any equilibrium point \\(x^\\star\\) by definition. With the introduction of positive definite and Lyapunov functions, we are now ready to use them to certify different concepts of stability. Theorem 4.2 (Lyapunov Local Stability) Consider the nonlinear system (4.3) in a ball \\(B_R\\) with equilibrium point \\(x=0\\), if there exists a scalar function \\(V(x)\\) (with continuous partial derivatives) such that \\(V(x)\\) is positive definite (in \\(B_R\\)) \\(\\dot{V}(x)\\) is negative semidefinite (in \\(B_R\\)) then the equilibrium point \\(x=0\\) is stable in the sense of Lyapunov (cf. Definition 4.2). Moreover, if \\(\\dot{V}(x)\\) is negative definite in \\(B_R\\), then the equilibrium point is asymptotically stable (cf. Definition 4.3). if \\(\\dot{V}(x) \\leq - \\alpha V(x)\\) for any \\(x \\in B_R\\), then the equilibrium point is exponentially stable (cf. Definition 4.4). Let us apply Theorem 4.2 to the simple pendulum. Example 4.3 (Lyapunov Local Stability for A Simple Pendulum) Consider the pendulum dynamics (4.5). The total energy of a pendulum is \\[\\begin{equation} V(x) = \\frac{1}{2} ml^2 \\dot{\\theta}^2 + mgl (1 - \\cos \\theta). \\tag{4.7} \\end{equation}\\] Clearly, \\(V(x)\\) is positive definite on the entire state space, and the only point where \\(V(x) = 0\\) is the equilibrium point \\(\\theta = 0, \\dot{\\theta} = 0\\). Let us compute the time derivative of \\(V(x)\\): \\[ \\dot{V}(x) = ml^2 \\dot{\\theta} \\ddot{\\theta} + mgl \\sin \\theta \\dot{\\theta} = ml^2 \\dot{\\theta} \\left( -\\frac{1}{ml^2}(b \\dot{\\theta} + mgl \\sin\\theta) \\right) + mgl \\sin \\theta \\dot{\\theta} = -b \\dot{\\theta}^2 , \\] which is clearly negative semidefinite. In fact, \\(\\dot{V}(x)\\) is precisely the energy dissipation rate due to damping. By Theorem 4.2 we conclude that the equilibrium point is stable in the sense of Lyapunov. Note that with this choice of \\(V(x)\\) as in (4.7), we actually cannot certify asymptotic local stability of the bottomright equilibrium point. So a natural question is, can we find a better Lyapunov function that indeed certifies asymptotic stability? The answer is yes. Consider a different Lyapunov function \\[\\begin{equation} \\tilde{V}(x) = \\frac{1}{2} ml^2 \\dot{\\theta}^2 + \\frac{1}{2} ml^2 \\left( \\frac{b}{ml^2}\\theta + \\dot{\\theta} \\right)^2 + 2mgl (1 - \\cos \\theta), \\tag{4.8} \\end{equation}\\] which is positive definite and admits a single zero-value point \\(\\theta = 0, \\dot{\\theta} = 0\\) that is also the bottomright equilibrium point. Simplifying \\(\\tilde{V}(x)\\) we can get \\[\\begin{align} \\tilde{V}(x) &amp;= ml^2 \\dot{\\theta}^2 + 2mgl(1-\\cos \\theta) + \\frac{1}{2} ml^2 \\left( \\frac{b^2}{m^2 l^4} \\theta^2 + \\frac{2b}{ml^2} \\theta \\dot{\\theta} \\right) \\\\ &amp;= 2V(x) + \\frac{1}{2} ml^2 \\left( \\frac{b^2}{m^2 l^4} \\theta^2 + \\frac{2b}{ml^2} \\theta \\dot{\\theta} \\right). \\end{align}\\] The time derivative of the new function \\(\\tilde{V}(x)\\) is \\[\\begin{align} \\dot{\\tilde{V}}(x) &amp;= 2 \\dot{V}(x) + \\frac{ml^2}{2} \\left( \\frac{2b^2}{m^2 l^4} \\theta \\dot{\\theta} + \\frac{2b}{ml^2} (\\dot{\\theta}^2 + \\theta \\ddot{\\theta}) \\right) \\\\ &amp; = 2\\dot{V}(x) + b\\dot{\\theta}^2 + \\left( \\frac{b^2}{ml^2} \\theta\\dot{\\theta} + b \\theta \\left( -\\frac{1}{ml^2} (b\\dot{\\theta} + mgl \\sin \\theta) \\right) \\right) \\\\ &amp; = -b \\left( \\dot{\\theta}^2 + \\frac{g}{l} \\theta \\sin \\theta \\right). \\end{align}\\] \\(\\dot{\\tilde{V}}(x)\\) is negative definite locally around the equilibrium point (locally \\(\\sin\\theta \\approx \\theta\\)). Therefore, with the new Lyapunov function \\(\\tilde{V}(x)\\) we can certify asymptotic stability. Interestingly, \\(V(x)\\) is intuitive (the total energy of the pendulum system), but it fails to certify asymptotic local stability (as least by just using Theorem 4.2). \\(\\tilde{V}(x)\\) does not have any physical intuition, but it successfully certifies local asymptotic stability. In Section 4.1.4, we will see that when using \\(V(x)\\) with the invariant set theorem, we can actually still certify the asymptotic stability of the pendulum around the bottomright equilibrium. In many applications, we desire to certify the global stability of an equilibrium point. The following theorem states that if in addition the scalar function \\(V(x)\\) is radially unbounded, then global stability can be certified. Theorem 4.3 (Lyapunov Global Stability) For the autonomous system (4.3), suppose there exists a scalar function \\(V(x)\\) with (continuous partial derivatives) such that \\(V(x)\\) is positive define; \\(\\dot{V}(x)\\) is negative define; \\(V(x) \\rightarrow \\infty\\) as \\(\\Vert x \\Vert \\rightarrow \\infty\\), then the equilibrium point \\(x = 0\\) is globally asymptotically stable (cf. Definition 4.5). Moreover, if in addition to the three conditions above \\(\\dot{V}(x) \\leq - \\alpha V(x)\\) for some \\(\\alpha &gt; 0\\), then the equilibrium point is globally exponentially stable. 4.1.4 Invariant Set Theorem Through Theorem 4.2, Theorem 4.3, and Example 4.3, we see that in order to certify asymptotic stability, the time derivative \\(\\dot{V}(x)\\) is required to be positive definite. However, in many cases, with Example 4.3 being a typical one, \\(\\dot{V}(x)\\) is only negative semidefinite, which makes it difficult to certify asymptotic stability. In this section, we will introduce the invariant set theorem that can help us reason about asymptotic stability even when \\(\\dot{V}(x)\\) is only negative semidefinite. Let us first introduce the notion of an invariant set. Definition 4.8 (Invariant Set) A set \\(G\\) is an invariant set for a dynamical system (4.3) if every system trajectory that starts within \\(G\\) remains in \\(G\\) for all future time. Formally, \\[ x(0) \\in G \\Longrightarrow x(t) \\in G,\\forall t. \\] A trivial invariant set if the entire state space \\(\\mathbb{X}\\). Another example of an invariant set is the singleton \\(\\{x^\\star \\}\\) with \\(x^\\star\\) being an equilibrium point. A nontrivial invariant set is the domain of attraction of an equilibrium point (cf. Definition 4.3). We now state the local invariant set theorem. Theorem 4.4 (Local Invariant Set) Consider the autonomous system (4.3), and let \\(V(x)\\) be a scalar function with continuous partial derivatives. Assume that the sublevel set \\(\\Omega_{\\rho} = \\{ x \\in \\mathbb{X} \\mid V(x) &lt; \\rho \\}\\) is bounded for some \\(\\rho &gt; 0\\), and \\(V(x) \\leq 0\\) for all \\(x \\in \\Omega_{\\rho}\\). Let \\(\\mathcal{R}\\) be the set of all points within \\(\\Omega_{\\rho}\\) such that \\(\\dot{V}(x) = 0\\), and \\(\\mathcal{M}\\) be the largest invariant set in \\(\\mathcal{R}\\). Then, every trajectory that starts in \\(\\Omega_{\\rho}\\) will converge to \\(\\mathcal{M}\\) as \\(t \\rightarrow \\infty\\). With this theorem, we can now revisit the pendulum example 4.3. Example 4.4 (Revisiting the Local Stability of A Simple Pendulum) In Example 4.3, using the Lyapunov function \\[ V(x) = \\frac{1}{2} ml^2 \\dot{\\theta}^2 + mgl(1 - \\cos\\theta), \\] with time derivative \\[ \\dot{V}(x) = -b\\dot{\\theta}^2, \\] we were only able to verify the stability of the bottomright equilibrium point in the sense of Lyapunov. Now let us use the invariant set theorem 4.4 to show the asymptotic stability of the bottomright equilibrium point. First it is easy to see that the sublevel set of \\(V(x)\\) is bounded. For example, with \\(\\rho = \\frac{1}{4} mgl\\), \\[\\begin{align} V(x) &lt; \\frac{1}{4} mgl \\Rightarrow \\frac{1}{2} ml^2 \\dot{\\theta}^2 &lt; \\frac{1}{4} mgl \\Rightarrow \\dot{\\theta}^2 &lt; \\frac{1}{2} \\frac{g}{l} \\\\ V(x) &lt; \\frac{1}{4} mgl \\Rightarrow mgl(1-\\cos\\theta) &lt; \\frac{1}{4} mgl \\Rightarrow \\cos\\theta &gt; \\frac{3}{4} \\Rightarrow \\theta \\in (-\\arccos \\frac{3}{4}, \\arccos \\frac{3}{4}). \\end{align}\\] The set \\(\\mathcal{R}\\), including all the points in \\(\\Omega_{\\rho}\\) such that \\(\\dot{V}(x) = 0\\) is \\[ \\mathcal{R} = \\{ x \\in \\Omega_{\\rho} \\mid \\dot{\\theta} = 0 \\}. \\] We now claim that the largest invariant set \\(\\mathcal{M}\\) in \\(\\mathcal{R}\\) is just the single equilibrium point \\(x = [0,0]^T\\). We can prove this by contradiction. Suppose there is a different point \\(x&#39; = [\\theta,0]^T\\) with \\(\\theta \\neq 0\\) also belonging to the invariant set \\(\\mathcal{M}\\), then \\[ \\ddot{\\theta} = -\\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) = - \\frac{g}{l} \\sin\\theta \\neq 0, \\] which means \\(\\dot{\\theta}\\) will immediately become nonzero, and hence the trajectory will exit \\(\\mathcal{R}\\) and also \\(\\mathcal{M}\\). So that point cannot belong to the invariant set. Now by Theorem 4.4, we conclude the bottomright equilibrium point is asymptotically stable. Note that through this analysis we also obtain \\(\\Omega_{\\rho}\\) as a domain of attraction for the bottomright equilibrium point. Similarly, with the addition of the radial unboundedness of \\(V(x)\\), we have a global version of the invariant set theorem. Theorem 4.5 (Global Invariant Set) For the autonomous system (4.3), let \\(V(x)\\) be a scalar function with continuous partial derivatives that satisfies \\(V(x) \\rightarrow \\infty\\) as \\(\\Vert x \\Vert \\rightarrow \\infty\\), and \\(\\dot{V}(x) \\leq 0\\) over the entire state space. Let \\(\\mathcal{R} = \\{ x\\in \\mathbb{X} \\mid \\dot{V}(x)= 0 \\}\\), and \\(\\mathcal{M}\\) be the largest invariant set in \\(\\mathcal{R}\\). Then all system trajectories asymptotically converge to \\(\\mathcal{M}\\) as \\(t \\rightarrow \\infty\\). 4.1.5 Computing Lyapunov Certificates All the Theorems we have stated so far (Theorems 4.2, 4.3, 4.4, and 4.5) are very general and powerful tools for certifying stability of nonlinear systems. However, the key requirement for applying the results is a Lyapunov function \\(V(x)\\) that verifies different types of nonnegativity constraints. How to find these functions? In Example 4.3, we have seen that physical intuition can help us find a good Lyapunov function (4.7). Nevertheless, it did not quite give us what we want in terms of asymptotic stability. Instead, a hand-crafted function (4.8) helped us certify local asymptotic stability. Wouldn’t it be cool that we can design an algorithm to find the Lyapunov certificates for us? A closer look at the Theorems 4.2, 4.3, 4.4, and 4.5 tells us the key property of a Lyapunov certificate is that it needs to satisfy the positivity (or negativity) constraint for all states inside a set. This is a nontrivial and difficult requirement, because even if we were given a function \\(V(x)\\), naively evaluting if \\(V(x)\\) is nonnegative inside a set requires enumeration over all the states in the set, which is impractical given that the set is continuous and has infinite number of states.3 When the dynamics (4.3) is linear, searching for Lyapunov functions is well understood and presented in Appendix A.1. However, when the dynamics is nonlinear, things can get very complicated. In the next, I want to introduce a general framework for searching Lyapunov certificates for nonlinear systems that is based on convex optimization. This framework, although having deep connections with many other disciplines such as algebraic geometry, theoretical computer science, and mathematical optimization, is based on a very simple intution that we all have since high school. Example 4.5 (A Simple Example for Certifying Nonnegativity) Suppose I give you a polynomial of a single variable \\(x \\in \\mathbb{R}\\) \\[ p(x) = x^2 + 2x + 1 \\] and ask you if \\(p(x) \\geq 0\\) for all \\(x\\). You would not hesitate to answer “yes”, because you know \\[ p(x) = (x+1)^2 \\] is the square of \\(x+1\\) and hence must be nonnegative. Let me make it more challenging. Suppose I give you a different polynomial \\[ p(x) = -x^4 + 2 x^2 + x + 1 \\] and ask you if \\(p(x)\\) is nonnegative for any \\(x \\in [-1,1]\\) (instead of any \\(x \\in \\mathbb{R}\\)). At first glance, it seems much harder to answer this question because (i) we have a constraint set \\(x \\in [-1,1]\\), and (ii) the polynomial \\(p(x)\\) has a higher degree and it is not a polynomial that we are very famliar with (compared to \\(p(x) = x^2 + 2x +1\\)). However, if I show you that \\(p(x)\\) can be written as \\[\\begin{equation} p(x) = -x^4 + 2 x^2 + 2x + 1 = (x+1)^2 + x^2 (1 - x^2), \\tag{4.9} \\end{equation}\\] it becomes easy again to certify that \\(p(x)\\) is nonnegative for any \\(x \\in [-1,1]\\). Why? First notice that \\((x+1)^2 \\geq 0\\) for any \\(x \\in \\mathbb{R}\\), Then notice that \\(1 - x^2 \\geq 0\\) for any \\(x \\in [-1,1]\\), and \\(x^2 \\geq 0\\) for any \\(x\\). Therefore, \\(x^2 (1-x^2) \\geq 0\\) for any \\(x \\in [-1,1]\\). Combining the above two reasonings, it becomes clear \\(p(x)\\) is nonnegative for any \\(x \\in [-1,1]\\). What we have learned from this simple example is that Given a polynomial \\(p(x)\\) and a constraint set \\(x \\in \\mathcal{X} \\subseteq \\mathbb{R}^n\\), if we can write \\(p(x)\\) as a sum of a finite number of products \\[ p(x) = \\sum_{i=1}^K \\sigma_i(x) g_i(x) \\] where \\(\\sigma_i(x)\\) is a polynomial that we know is always nonnegative for any \\(x \\in \\mathbb{R}^n\\) (just like \\((x+1)^2\\) and \\(x^2\\) in (4.9)), and \\(g_i(x)\\) is a polynomial that we know is always nonnegative for any \\(x\\) in the constraint set \\(\\mathcal{X}\\) (just like \\(1-x^2\\) for the set \\([-1,1]\\) in (4.9)), then we have a certificate that \\(p(x) \\geq 0\\) for any \\(x \\in \\mathcal{X}\\). With this simple intuition, let me now formalize the framework of sum of squares (SOS) certificates for proving nonnegativity (also known as positivstellensatz, or in short P-satz). Positivstellensatz, Sum of Squares, and Convex Optimization Basic Semialgebraic Set. Let \\(x = [x_1,\\dots,x_n] \\in \\mathbb{R}^n\\) be a list of variables, we define a basic semialgebraic set as \\[\\begin{equation} \\mathcal{X} = \\{ x \\in \\mathbb{R}^{n} \\mid p_i(x) = 0, i = 1,\\dots,l_{\\mathrm{eq}}; p_i(x) \\geq 0, i=l_{\\mathrm{eq}}+1,\\dots,l_{\\mathrm{eq}} + l_{\\mathrm{ineq}} \\} \\tag{4.10} \\end{equation}\\] where \\(p_i(x),i=1,\\dots,l_{\\mathrm{eq}}+l_{\\mathrm{ineq}}\\) are polynomial functions in \\(x\\). In other words, the set \\(\\mathcal{X}\\) is a subset of \\(\\mathbb{R}^n\\) that is defined by \\(l_{\\mathrm{eq}}\\) equality constraints and \\(l_{\\mathrm{ineq}}\\) inequality constraints. Observe that a basic semialgebraic set can capture a lot of the common constraint sets, such as a unit sphere, a unit ball, and a box (try this for yourself). Positivstellensatz. We are now given the same question as in Example 4.5. Suppose I give you another polynomial function \\(p_0(x)\\), how can you tell me if \\(p_0(x)\\) is nonnegative for any \\(x\\) in the basic semialgebraic set \\(\\mathcal{X}\\)? That is, to verify if \\[ p_0(x) \\geq 0, \\quad \\forall x \\in \\mathcal{X}. \\] Formalizing the intution obtained from Example 4.5, you will say if someone can produce a decomposition of \\(p_0(x)\\) as \\[\\begin{equation} p_0(x) = \\sigma_0(x) + \\sum_{i=1}^{l_{\\mathrm{ineq}}} \\sigma_i(x) p_{i + l_{\\mathrm{eq}}}(x) + \\sum_{i=1}^{l_{\\mathrm{eq}}} \\lambda_i(x) p_{i}(x), \\tag{4.11} \\end{equation}\\] where \\(\\sigma_0,\\sigma_1,\\dots,\\sigma_{l_{\\mathrm{ineq}}}\\) are “some type of” polynomials that we know are always nonnegative (for any \\(x \\in \\mathbb{R}^n\\)), and \\(\\lambda_1,\\dots,\\lambda_{l_{\\mathrm{eq}}}\\) are arbitrary polynomials. Then I have a “certificate” that \\(p_0(x) \\geq 0\\) for any \\(x \\in \\mathcal{X}\\). Why? The reasoning is exactly the same as before. \\(\\sigma_0(x) \\geq 0\\) for any \\(x\\), \\(\\sigma_i(x) p_{i+l_{\\mathrm{eq}}}(x) \\geq 0, i=1,\\dots,l_{\\mathrm{ineq}}\\) for any \\(x \\in \\mathcal{X}\\), because (a) \\(\\sigma_i(x) \\geq 0\\) for any \\(x\\), and (b) \\(p_{i+l_{\\mathrm{eq}}}(x) \\geq 0\\) for any \\(x \\in \\mathcal{X}\\) by definition of the basic semialgebraic set (4.10), \\(\\lambda_i(x) p_i (x) = 0, i=1,\\dots,l_{\\mathrm{eq}}\\) for any \\(x \\in \\mathcal{X}\\) by definition of the basic semialgebraic set (4.10). We call \\(\\sigma_i\\)’s “nonnegative polynomial multipliers”, and \\(\\lambda_i\\)’s “polynomial multipliers”. Sum-of-Squares. Now it comes the key question: what type of polynomials should we choose as the nonnegative polynomial multipliers? Ideally, this type of polynomials should be always (trivially) nonnegative, and have a nice representation for its unknown parameters (coefficients). Looking back at our choice of multipliers, i.e., \\((x+1)^2\\) and \\(x^2\\) in Example 4.5, it is natural to come up with the choice of a “sum-of-squares” (SOS) polynomial. Definition 4.9 (Sum-of-Squares Polynomial) A polynomial \\(\\sigma(x)\\) is called an SOS polynomial if \\[ \\sigma(x) = \\sum_{i=1}^k q_i^2(x), \\] i.e., \\(\\sigma(x)\\) can be written as a sum of \\(k\\) squared polynomials. OK, an SOS polynomial is trivially nonnegative (satisfying requirement (a) above), but does it have a nice representation for its parameters? The following Lemma gives us an affirmative answer. Lemma 4.1 (SOS Polynomial and Positive Semidefinite Matrix) A polynomial \\(\\sigma(x)\\) is SOS if and only if \\[ \\sigma(x) = [x]_d^T Q [x]_d \\] for some \\(Q \\succeq 0\\), where \\([x]_d\\) is the vector of monomials in \\(x\\) of degree up to \\(d\\). For example, if \\(x \\in \\mathbb{R}^2\\) and \\(d = 2\\), then \\[ [x]_2 = [1,x_1,x_2,x_1^2,x_1x_2,x_2^2]^T. \\] With the choice of \\(\\sigma(x)\\) as SOS polynomials, we are now ready to explicitly search for a nonnegativity certificate in the form of (4.11): \\[\\begin{equation} \\begin{split} \\text{find} &amp; \\quad \\{\\sigma_i \\}_{i=0}^{l_{\\mathrm{ineq}}}, \\{ \\lambda_i \\}_{i=1}^{l_{\\mathrm{eq}}} \\\\ \\text{subject to} &amp; \\quad p_0(x) = \\sigma_0(x) + \\sum_{i=1}^{l_{\\mathrm{ineq}}} \\sigma_i(x) p_{i + l_{\\mathrm{eq}}}(x) + \\sum_{i=1}^{l_{\\mathrm{eq}}} \\lambda_i(x) p_{i}(x), \\\\ &amp; \\quad \\sigma_i \\text{ is SOS}, i=0,\\dots,l_{\\mathrm{ineq}}, \\\\ &amp; \\quad \\lambda_i \\text{ is polynomial}, i=1,\\dots,l_{\\mathrm{eq}} \\\\ &amp; \\quad \\mathrm{deg}(\\sigma_0) \\leq 2 \\kappa, \\mathrm{deg}(\\sigma_i p_{i+l_{\\mathrm{eq}}}) \\leq 2 \\kappa, i=1,\\dots,l_{\\mathrm{ineq}}, \\\\ &amp; \\quad \\mathrm{deg}(\\lambda_i p_i) \\leq 2 \\kappa, i=1,\\dots,l_{\\mathrm{eq}}. \\end{split} \\tag{4.12} \\end{equation}\\] Bounding the Degree. The careful reader realizes that in (4.12) we have added constraints on the degrees of the polynomial multipliers \\(\\sigma_i\\)’s and \\(\\lambda_i\\)’s.4 Precisely, we choose an integer \\(\\kappa\\), which we call the relaxation order, such that \\[ 2 \\kappa \\geq \\max \\{ \\mathrm{deg}(p_i(x)) \\}_{i=0}^{l_{\\mathrm{eq}} + l_{\\mathrm{ineq}}}, \\] and restrict the products \\(\\sigma_i p_{i+l_{\\mathrm{eq}}}\\)’s and \\(\\lambda_i p_i\\)’s to have degrees at most \\(2\\kappa\\). With this, we are explicitly limiting the degrees of the multipliers \\(\\sigma_i\\)’s and \\(\\lambda_i\\)’s, and hence asking the formulation (4.12) to search for a finite number of parameters (otherwise, if the degree of the multipliers is unbounded, then the number of parameters to be searched is infinite). Convex Optimization. The last crucial (and surprising) observation is that the problem (4.12) is a convex optimization! This is due to the following three reasons The polynomial multipliers \\(\\lambda_i\\)’s can be fully parametrized by their coefficients, and these coefficients can be arbitrary vectors. Precisely, if \\(\\lambda(x)\\) is a polynomial with degree up to \\(d\\), then \\[ \\lambda(x) = c^T [x]_d, \\] where \\([x]_d\\) is the vector of monomials in \\(x\\) of degree up to \\(d\\), and \\(c\\) is the vector of coefficients. The SOS multipliers \\(\\sigma_i\\)’s can be fully parametrized by their coefficients, and these coefficients are positive semidefinite matrices, according to Lemma 4.1. The equality constraint of decomposing \\(p_0(x)\\) as a sum of products in (4.12) therefore becomes a set of affine equality constraints on the parameters of \\(\\lambda_i\\)’s and \\(\\sigma_i\\)’s, by matching coefficients of the monomials on the left-hand size and the right-hand side. Therefore, the problem (4.12) is a convex semidefinite program (SDP). There are multiple software packages, e.g., SOSTOOLS, YALMIP, SumOfSquares.py, that allow us to model our problem in the form of (4.12), convert the formulation into SDPs, and pass them to SDP solvers (such as MOSEK). We will see an example of this soon. Extensions. I want to congratulate, and welcome you to enter the world of SOS relaxations! Like I said before, this is an active area of research and the framework I just introduced is just a tip of the iceberg. Therefore, before I end this tutorial, I want to point out several extensions of the SOS framework. Necessary Condition. We have seen that a decomposition in the form of (4.12) is a sufficient condition to prove the nonnegativity of \\(p_0(x)\\). Is it also a necessary condition? That is, for any \\(p_0(x)\\) that is nonnegative on the set \\(\\mathcal{X}\\), does it admit a decomposition in the form of (4.12)? In general, the answer is no, and there exist nonnegative polynomials that cannot be written in the form of SOS decompositions (e.g., the Motzkin’s polynomial). However, with certain assumptions on the set \\(\\mathcal{X}\\), the decomposition (4.12) is also necessary for nonnegativity! A well-known assumption is called the Archimedean condition (which, roughly speaking, requires the set \\(\\mathcal{X}\\) to be compact)). I suggest you to read (Blekherman, Parrilo, and Thomas 2012) for more details. Global Polynomial Optimization. The SOS framework can be used for global optimization of polynomials in a straightforward way. Consider the polynomial optimization problem (POP) \\[ \\min_{x \\in \\mathcal{X}} p_0(x), \\] where one seeks the global minimum of the polynomial \\(p_0(x)\\) on the set \\(\\mathcal{X}\\). A POP is generally a nonconvex optimization problem, and it is difficult to obtain a globally optimal solution. However, with a slight change of perspective, we can write the problem above equivalently as \\[\\begin{equation} \\begin{split} \\max &amp; \\quad \\gamma \\\\ \\text{subject to} &amp; \\quad p_0(x) - \\gamma \\geq 0, \\quad \\forall x \\in \\mathcal{X}. \\end{split} \\tag{4.13} \\end{equation}\\] Basically I want to push the lower bound \\(\\gamma\\) as high as possible. The constraint in (4.13) asks \\(p_0(x) -\\gamma\\) to be nonnegative on \\(\\mathcal{X}\\). With the SOS framework introduced above, we can naturally relax it to \\[\\begin{equation} \\begin{split} \\max &amp; \\quad \\gamma \\\\ \\text{subject to} &amp; \\quad p_0(x) - \\gamma \\quad \\text{is SOS on} \\quad \\mathcal{X}, \\end{split} \\tag{4.14} \\end{equation}\\] where the “SOS on \\(\\mathcal{X}\\)” constraint is exactly the problem (4.12). Therefore, we have relaxed the nonconvex optimization (4.13) into a convex problem (4.14)! Moreover, by increasing the relaxation order \\(\\kappa\\), we obtain a sequence of lower bounds that asymptotically converge to the true global optimum of the nonconvex problem (4.13). This is called Lasserre’s hierarchy of moment-SOS relaxations, originally proposed by Lasserre in the seminal work (Jean B. Lasserre 2001). As this name suggests, the dual problem to the SOS relaxation (4.14) is called the moment relaxation. Lasserre’s hierarchy has recently gained a lot of attention due to the empirical observation in many engineering disciplines that the convergence to global optimum is finite, i.e., by solving the convex problem (4.14) at a finite relaxation order \\(\\kappa\\), an exact global optimizer of the original nonconvex problem (4.13) can be extracted. For a pragmatic introduction to the moment relaxation, I suggest to read Section 2.2 of (Yang and Carlone 2022). For more applications of Lasserre’s hierarchy, please refer to (Jean Bernard Lasserre 2009). Scalability. I have to warn you that there is no free lunch. The fact that so many challenging problems can be relaxed or restated as convex optimization problems should send you an alert. Does this mean that we can use convex optimization to solve all the challenging problems? Well, although we hope this is the case, in practice we are limited by the computational resources. The caveat is that the problem (4.12) and (4.14), despite being convex, grows very large as the dimension \\(n\\) and relaxation order \\(\\kappa\\) increases. Another way of saying this is that, we seek to solve small-to-medium scale nonconvex problems with large-scale convex problems. Unfortunately, today’s SDP solver cannot solve all the problems we formulate, and hence a major research direction in the mathematical optimization community is to develop SDP solvers that are more scalable. You can read (Yang et al. 2022) and references therein for more details. Non-SOS Certificates. Nobody is preventing us to use a different choice of nonnegative polynomial multipliers (other than SOS multipliers) in (4.11). For example, one can use a decomposition as the sum of nonnegative circuit polynomials (Wang 2022) or signomials (Murray, Chandrasekaran, and Wierman 2021). However, to the best of my knowledge, non-SOS certificates are far less popular than SOS certificates. There are many other extensions to the SOS framework, and a complete enumeration is beyond the scope of this lecture notes. For the connection between SOS and theoretical computer science, you can see the lecture notes by Boaz Barak and David Steurer. There are also more recent monographs about SOS, for example (Magron and Wang 2023) and (Nie 2023). I plan to introduce these in more details in an upcoming graduate-level class at Harvard. That was a long detour from Lyapunov analysis! The SOS machinery will come back later when we study multiple other topics in optimal control and estimation. But now let us show how to tackle the problem of computing Lyapunov certificates using the SOS machinery. According to Theorem 4.2, given a set \\(\\mathcal{X}\\) that contains an equilibrium point \\(x^\\star\\), if we can find a Lyapunov function \\(V(x)\\) such that \\(V(x)\\) is positive definite on \\(\\mathcal{X}\\) and \\(\\dot{V}(x)\\) is negative definite on \\(\\mathcal{X}\\), then the equilibrium point \\(x^\\star\\) is locally asymptotically stable. With the SOS machinery, we can search for a \\(V(x)\\) that is a polynomial as \\[\\begin{align} \\text{find} &amp; \\quad V(x) \\\\ \\text{subject to} &amp; \\quad V(x) - \\epsilon_1 \\Vert x - x^\\star \\Vert^2 \\quad \\text{is SOS on} \\quad \\mathcal{X} \\\\ &amp; \\quad - \\epsilon_2 \\Vert x - x^\\star \\Vert^2 - \\frac{\\partial V(x)}{\\partial x} f(x) \\quad \\text{is SOS on} \\quad \\mathcal{X} \\\\ &amp; \\quad V(x^\\star) = 0, \\end{align}\\] where \\(\\epsilon_1, \\epsilon_2 &gt; 0\\) are (small) positive constants. This is a convex optimization problem, just like (4.12) (try to convince yourself my claim is true). Similarly, we can choose a relaxation order \\(\\kappa\\) and solve the above problem. If a solution exists, then we find a valid Lyapunov certificate. Let us apply it to the simple pendulum to synthesize local stability certificates. Example 4.6 (Computing Lyapunov Local Stability Certificate for the Simple Pendulum with Convex Optimization) The SOS framework works with polynomials, so let us first write the pendulum dynamics in polynomial form via a change of coordinate \\(x = [\\mathfrak{s}, \\mathfrak{c}, \\dot{\\theta}]^T\\) with \\(\\mathfrak{s} = \\sin \\theta\\), \\(\\mathfrak{c} = \\cos\\theta\\): \\[ \\begin{cases} \\dot{\\mathfrak{s}} = \\mathfrak{c} \\dot{\\theta} \\\\ \\dot{\\mathfrak{c}} = -\\mathfrak{s} \\dot{\\theta} \\\\ \\ddot{\\theta} = - \\frac{1}{ml^2}(b \\dot{\\theta} + mgl \\mathfrak{s}) \\end{cases}. \\] We will use \\(m = 1, l = 1, b=0.1\\) for our numerical experiment. We want to find a local Lyapunov certificate in the compact set \\[\\begin{equation} \\theta \\in \\left[-\\arccos \\frac{3}{4}, \\arccos \\frac{3}{4} \\right], \\quad \\dot{\\theta} \\in \\left[- \\frac{\\pi}{2}, \\frac{\\pi}{2} \\right]. \\tag{4.15} \\end{equation}\\] In the new coordinates \\(x\\), this is equivalent to the semialgebraic set \\[ \\mathcal{X} = \\left\\{ x \\in \\mathbb{R}^{3} \\mid \\mathfrak{s}^2 + \\mathfrak{c}^2 = 1, \\dot{\\theta}^2 \\leq \\frac{\\pi^2}{4}, \\mathfrak{c} \\geq \\frac{3}{4} \\right\\}. \\] Denoting the bottomright equilibrium point as \\(x_e = [0,1,0]^T\\), and with \\(\\epsilon_1,\\epsilon_2 &gt; 0\\) two positive constants, we can seek a Lyapunov function \\(V(x)\\) that satisfies the following conditions \\[\\begin{align} V(x) \\geq \\epsilon_1 (x - x_e)^T (x - x_e), \\quad \\forall x \\in \\mathcal{X} \\tag{4.16}\\\\ \\dot{V}(x) = \\frac{\\partial V}{\\partial x} \\dot{x} \\leq - \\epsilon_2 (x - x_e)^T (x - x_e), \\quad \\forall x \\in \\mathcal{X} \\tag{4.17}\\\\ V(x_e) = 0, \\quad \\dot{V}(x_e) = 0 \\tag{4.18} \\end{align}\\] where (4.16) ensures \\(V(x)\\) is positive definite, (4.17) ensures \\(\\dot{V}(x)\\) is negative definite, and (4.18) ensures \\(V(x),\\dot{V}(x)\\) vanish at the equilibrium point. To leverage the power of convex optimization, we can relax the positivity constraints as SOS constraints \\[\\begin{align} V(x) - \\epsilon_1 (x - x_e)^T (x - x_e) \\quad \\text{is SOS on} \\quad \\mathcal{X} \\\\ - \\epsilon_2 (x - x_e)^T (x - x_e) - \\frac{\\partial V}{\\partial x} \\dot{x}\\quad \\text{is SOS on} \\quad \\mathcal{X} \\\\ V(x_e) = 0, \\quad \\dot{V}(x_e) = 0. \\end{align}\\] If we limit the degree of \\(V\\) to \\(2\\), choose the relaxation order \\(\\kappa = 2\\), and \\(\\epsilon_1 = \\epsilon_2 = 0.01\\), we obtain a solution \\[ V(x) = 2.7982 \\mathfrak{s}^2 + 0.086248 \\mathfrak{s} \\dot{\\theta} + 2.4548\\mathfrak{c}^2 + 0.88117 \\dot{\\theta}^2 - 16.6277 \\mathfrak{c} + 14.1728 \\] with the time derivative \\[ \\dot{V}(x) = 0.68675 \\mathfrak{s} \\mathfrak{c} \\dot{\\theta} + 0.086248* \\mathfrak{c} \\dot{\\theta}^2 - 0.84523 \\mathfrak{s}^2 - 0.65191 \\mathfrak{s} \\dot{\\theta} - 0.17623 \\dot{\\theta}^2. \\] Plotting \\(V(x)\\) in the constraint set (4.15) using \\((\\theta, \\dot{\\theta})\\) coordinates, we get Figure 4.2: Lyapunov local stability certificate computed via convex optimization. and verify that \\(V(x)\\) is locally positive definite. Plotting \\(\\dot{V}(x)\\) in the constraint set (4.15) using \\((\\theta, \\dot{\\theta})\\) coordinates, we get Figure 4.3: Derivative of the lyapunov local stability certificate computed via convex optimization. and verify that \\(\\dot{V}(x)\\) is locally negative definite. You should try the code for this example here. 4.2 Controlled Systems 4.3 Non-autonomous Systems Lemma 4.2 (Barbalat's Lemma) Let \\(f(t)\\) be differentiable, if \\(\\lim_{t \\rightarrow \\infty} f(t)\\) is finite, and \\(\\dot{f}(t)\\) is uniformly continuous,5 then \\[ \\lim_{t \\rightarrow \\infty} \\dot{f}(t) = 0. \\] Theorem 4.6 (Barbalat's Stability Certificate) If a scalar function \\(V(x,t)\\) satisfies \\(V(x,t)\\) is lower bounded, \\(\\dot{V}(x,t)\\) is negative semidefinite \\(\\dot{V}(x,t)\\) is uniformly continuous then \\(\\dot{V}(x,t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\). Proof. \\(V(x,t)\\) is lower bounded and \\(\\dot{V}\\) is negative semidefinite implies the limit of \\(V\\) as \\(t \\rightarrow \\infty\\) is finite (note that \\(V(x,t) \\leq V(x(0),0)\\)). Then the theorem clearly follows from Barbalat’s Lemma 4.2. References "],["output-feedback.html", "Chapter 5 Output Feedback 5.1 State Observer 5.2 Observer Feedback", " Chapter 5 Output Feedback Consider a continuous-time dynamical system \\[\\begin{equation} \\begin{split} \\dot{x} &amp;= f(x,u) \\\\ y &amp;= h(x,u) \\end{split} \\tag{5.1} \\end{equation}\\] where \\(x(t) \\in \\mathbb{X} \\subseteq \\mathbb{R}^n\\) the state of the system, \\(u(t) \\in \\mathbb{U} \\subseteq \\mathbb{R}^m\\) the control (or input), \\(y(t) \\in \\mathbb{Y} \\subseteq \\mathbb{R}^{d}\\) the output (i.e., measurement) of the state and control, and \\(f,g\\) the evolution and measurement functions (which are sufficiently smooth). 5.1 State Observer For the system (5.1), let us denote \\(X(x_0,t_0;t;u)\\) the solution at time \\(t\\) with input \\(u\\) and initial condition \\(x_0\\) at time \\(t_0\\); when \\(t_0 = 0\\), we write \\(X(x_0;t;u)\\) \\(Y(x_0,t_0;t;u)\\) the output at time \\(t\\) with input \\(u\\) and initial condition \\(x_0\\) at time \\(t_0\\), i.e., \\(Y(x_0,t_0;t;u) = h(X(x_0,t_0;t;u), u(t))\\); when \\(t_0 = 0\\), we write \\(y_{x_0,u}(t)\\); \\(\\mathcal{X}_0\\) a subset of \\(\\mathbb{X}\\) containing the initial conditions we consider; for any \\(x_0 \\in \\mathcal{X}_0\\), we write \\(\\sigma^+_{\\mathcal{X}}(x_0;u)\\) the maximal time of existence of \\(X(x_0,\\cdot;t;u)\\) in a set \\(\\mathcal{X}\\) \\(\\mathcal{U}\\) the set of all sufficiently many times differentiable inputs \\(u: [0,+\\infty) \\rightarrow \\mathbb{U}\\). The problem of state observation is to produce an estimated state \\(\\hat{x}(t)\\) of the true state \\(X(x_0,t_0;t;u)\\) based on knowledge about the system (5.1) and information about the history of inputs \\(u_{[0,t]}\\) and outputs \\(y_{[0,t]}\\), so that \\(\\hat{x}(t)\\) asymptotically converges to \\(X(x_0,t_0;t;u)\\), for any initial condition \\(x_0 \\in \\mathcal{X}_0\\) and any input \\(u \\in \\mathcal{U}\\). There are multiple ways for solving the problem of state observation (see e.g., (Bernard 2019), (Bernard, Andrieu, and Astolfi 2022)). Here we are particularly interested in the approach using a state observer, i.e., a dynamical system whose internal state evolves according to the history of inputs and outputs, from which a state estimation can be reconstructed that guarantees asymptotic convergence to the true state. We formalize this concept below. Definition 5.1 (State Observer) A state observer for system (5.1) is a couple \\((\\mathcal{F},\\mathcal{T})\\) such that \\(\\mathcal{F}: \\mathbb{R}^{l} \\times \\mathbb{R}^{m} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^l\\) is continuous \\(\\mathcal{T}\\) is a family of continuous functions indexed by \\(u \\in \\mathcal{U}\\) where each \\(\\mathcal{T}_u: \\mathbb{R}^l \\times [0,+\\infty) \\rightarrow \\mathbb{R}^n\\) respects the causality condition \\[ \\forall \\tilde{u}: [0,+\\infty) \\rightarrow \\mathbb{R}^m,\\forall t \\in [0,+\\infty), u_{[0,t]} = \\tilde{u}_{[0,t]} \\Rightarrow \\mathcal{F}_u (\\cdot,t) = \\mathcal{F}_{\\tilde{u}}(\\cdot,t). \\] For any \\(u \\in \\mathcal{U}\\), any \\(z_0 \\in \\mathbb{R}^l\\), and any \\(x_0 \\in \\mathcal{X}_0\\) such that \\(\\sigma^+_{\\mathbb{X}}(x_0;u) = +\\infty\\), any solution \\(Z(z_0;t;u,y_{x_0,u})\\)6 to \\[\\begin{equation} \\dot{z} = \\mathcal{F}(z,u,y_{x_0,u}) \\tag{5.2} \\end{equation}\\] initialized at \\(z_0\\) at time \\(0\\) with input \\(u\\) and \\(y_{x_0,u}\\) exists on \\([0,+\\infty)\\) and satisfies \\[\\begin{equation} \\lim_{t \\rightarrow \\infty} \\Vert \\hat{X}(x_0,z_0;t;u) - X(x_0;t;u) \\Vert = 0, \\tag{5.3} \\end{equation}\\] with \\[\\begin{equation} \\hat{X}(x_0,z_0;t;u) = \\mathcal{T}_u(Z(z_0;t;u,y_{x_0,u}),t). \\tag{5.4} \\end{equation}\\] In words, (i) the state observer maintains an internal state (or latent state) \\(z \\in \\mathbb{R}^l\\) that evolves according to the latent dynamics \\(\\mathcal{F}\\) in (5.2), where \\(u\\) and \\(y_{x_0,u}\\) are inputs; (ii) an estimated state can be reconstructed from the internal state using \\(\\mathcal{T}_u\\) as in (5.4); and (iii) the error between the estimated state and the true state (defined by a proper distance function \\(\\Vert \\cdot \\Vert\\) on \\(\\mathbb{X}\\)) converges to zero. If \\(\\mathcal{T}_u\\) is the same for any \\(u \\in \\mathcal{U}\\) and is also time independent, then we say \\(\\mathcal{T}\\) is stationary.7 In this case, we can simply write the observer (5.2) and (5.4) as \\[\\begin{equation} \\begin{split} \\dot{z} &amp;= \\mathcal{F}(z,u,y) \\\\ \\hat{x} &amp;= \\mathcal{T}(z). \\end{split} \\tag{5.5} \\end{equation}\\] If \\(\\hat{x}\\) can be read off directly from \\(z\\), then we say the observer (5.5) is in the given coordinates. A special case of this is when \\(\\hat{x} = z\\), i.e., the internal state of the observer is the same as the system state. 5.1.1 General Design Strategy Theorem 5.1 (Meta Observer) Let \\(F: \\mathbb{R}^p \\times \\mathbb{R}^m \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^p\\), \\(H: \\mathbb{R}^p \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^d\\) and \\(\\mathcal{F}: \\mathbb{R}^p \\times \\mathbb{R}^m \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^p\\) be continuous functions such that \\[\\begin{equation} \\dot{\\hat{\\xi}} = \\mathcal{F}(\\hat{\\xi}, u, \\tilde{y}) \\tag{5.6} \\end{equation}\\] is an observer for \\[\\begin{equation} \\dot{\\xi} = F(\\xi,u,H(\\xi,u)), \\quad \\tilde{y} = H(\\xi,u), \\tag{5.7} \\end{equation}\\] i.e., for any \\(\\xi_0,\\hat{\\xi}_0 \\in \\mathbb{R}^p\\) and any \\(u \\in \\mathcal{U}\\), the solution of the observer (5.6), denoted by \\(\\hat{\\Xi}(\\hat{\\xi}_0;t;u;\\tilde{y}_{\\xi_0,u})\\), and the solution of the true system (5.7), denoted by \\(\\Xi(\\xi_0;t;u)\\), satisfy \\[\\begin{equation} \\lim_{t \\rightarrow \\infty} \\Vert \\hat{\\Xi}(\\hat{\\xi}_0;t;u;\\tilde{y}_{\\xi_0,u}) - \\Xi(\\xi_0;t;u) \\Vert = 0. \\tag{5.8} \\end{equation}\\] Note that the observer (5.6) is stationary and in the given coordinates for system (5.7). Indeed the internal state of the observer is the same as the system state. Now suppose for any \\(u \\in \\mathcal{U}\\), there exists a continuous function (i.e., coordinate transformation) \\(T_u: \\mathbb{R}^n \\times \\mathbb{R} \\rightarrow \\mathbb{R}^p\\) and a subset \\(\\mathcal{X}\\) of \\(\\mathbb{X}\\) such that For any \\(x_0 \\in \\mathcal{X}_0\\) such that \\(\\sigma^+_{\\mathbb{X}}(x_0;u) = + \\infty\\), \\(X(x_0;\\cdot;u)\\) remains in \\(\\mathcal{X}\\) There exists a concave \\(\\mathcal{K}\\)8 function \\(\\rho\\) and a positive number \\(\\bar{t}\\) such that \\[ \\Vert x_a - x_b \\Vert \\leq \\rho (| T_u(x_a,t) - T_u(x_b,t) |), \\quad \\forall x_a,x_b \\in \\mathcal{X}, t \\geq \\bar{t}, \\] i.e., \\(x \\mapsto T_u(x,t)\\) becomes injective on \\(\\mathcal{X}\\),9 uniformly in time and space, after a certain time \\(\\bar{t}\\). \\(T_u\\) transforms the system (5.1) into the system (5.7), i.e., for all \\(x \\in \\mathcal{X}\\) and all \\(t \\geq 0\\), we have \\[\\begin{equation} L_{(f,1)} T_u(x,t) = F(T_u(x,t),u,h(x,u)), \\quad h(x,u) = H(T_u(x,t),u), \\tag{5.9} \\end{equation}\\] where \\(L_{(f,1)} T_u(x,t)\\) is the Lie derivative of \\(T_u\\) along the vector field \\((f,1)\\) \\[ L_{(f,1)} T_u(x,t) = \\lim_{\\tau \\rightarrow 0} \\frac{ T_u (X(x,t;t+\\tau;u),t+\\tau) - T_u(x,t) }{\\tau}. \\] \\(T_u\\) respects the causality condition \\[ \\forall \\tilde{u}: [0,+\\infty) \\rightarrow \\mathbb{R}^m, \\forall t \\in [0,+\\infty), u_{[0,t]} = \\tilde{u}_{[0,t]} \\Rightarrow T_u(\\cdot,t) = T_{\\tilde{u}}(\\cdot,t). \\] Then, for any \\(u \\in \\mathcal{U}\\), there exists a function \\(\\mathcal{T}_u: \\mathbb{R}^p \\times [0,+\\infty) \\rightarrow \\mathcal{X}\\) (satisfying the causality condition) such that for any \\(t \\geq \\bar{t}\\), \\(\\xi \\mapsto \\mathcal{T}_u (\\xi, t)\\) is uniformly continuous on \\(\\mathbb{R}^p\\) and satisfies \\[ \\mathcal{T}_u \\left( T_u(x,t),t \\right) = x, \\forall x \\in \\mathcal{X}. \\] Moreover, denoting \\(\\mathcal{T}\\) the family of functions \\(\\mathcal{T}_u\\) for \\(u \\in \\mathcal{U}\\), the couple \\((\\mathcal{F}, \\mathcal{T})\\) is an observer for the system (5.1) initialized in \\(\\mathcal{X}_0\\). Proof. See Theorem 1.1 in (Bernard 2019). A simpler version of Theorem 5.1 where the coordinate transformation \\(T_u\\) is stationary and fixed for all \\(u\\) is stated below as a corollary. Corollary 5.1 (Meta Observer with Fixed Transformation) Let \\(F: \\mathbb{R}^p \\times \\mathbb{R}^m \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^p\\), \\(H: \\mathbb{R}^p \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^d\\) and \\(\\mathcal{F}: \\mathbb{R}^p \\times \\mathbb{R}^m \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^p\\) be continuous functions such that (5.6) is an observer for (5.7). Suppose there exists a continuous coordinate transformation \\(T: \\mathbb{R}^p \\rightarrow \\mathbb{R}^n\\) and a compact subset \\(\\Omega\\) of \\(\\mathbb{R}^n\\) such that For any \\(x_0 \\in \\mathcal{X}_0\\) such that \\(\\sigma^+_{\\mathbb{X}}(x_0;u) = + \\infty\\), \\(X(x_0;\\cdot;u)\\) remains in \\(\\Omega\\) \\(x \\mapsto T(x)\\) is injective on \\(\\Omega\\) \\(T\\) transforms the system (5.1) into system (5.7) \\[ L_f T(x) = F(T(x),u,h(x,u)), \\quad h(x,u) = H(T(x),u), \\] where \\(L_f T(x)\\) is the Lie derivative of \\(T(x)\\) along \\(f\\) \\[ L_f T(x) = \\lim_{\\tau \\rightarrow 0} \\frac{ T(X(x,t;t+\\tau;u)) - T(x)}{\\tau}. \\] Then, there exists a uniformly continuous function \\(\\mathcal{T}:\\mathbb{R}^p \\rightarrow \\mathbb{R}^{n}\\) such that \\[ \\mathcal{T}(T(x)) = x, \\quad \\forall x \\in \\Omega, \\] and \\((\\mathcal{F},\\mathcal{T})\\) is an observer for system (5.1) initialized in \\(\\mathcal{X}_0\\). Theorem 5.1 and Corollary 5.1 suggest the following general observer design strategy: Find an injective coordinate transformation \\(T_u\\) (that may be time-varying and also dependent on \\(u\\)) that transforms the original system (5.1) with coordinate \\(x\\) into a new system (5.7) with coordinate \\(\\xi\\) Design an observer (5.6), \\(\\hat{\\xi}\\), for the new system Compute a left inverse, \\(\\mathcal{T}_u\\), of the transformation \\(T_u\\) to recover a state estimation \\(\\hat{x}\\) of the original system. The transformed systems (5.7) are typically referred to as normal forms, or in my opinion, templates. Of course, the general design strategy is rather conceptual, and in order for it to be practical, we have to answer three questions. What templates do we have, what are their associated observers, and what are the conditions for the observers to be asymptotically converging? What kinds of (nonlinear) systems can be transformed into the templates, and how to perform the transformation? How to invert the coordinate transformation? Is it analytical or does it require numerical approximation? In the following sections, we will study several representative normal forms and answer the above questions. Before presenting the results, let us first introduce several notions of observability. Definition 5.2 (Observability) Consider an open subset \\(\\mathcal{L}\\) of the state space \\(\\mathbb{X} \\subseteq \\mathbb{R}^n\\) of system (5.1). The system (5.1) is said to be Distinguishable on \\(\\mathcal{L}\\) for some input \\(u(t)\\), if for all \\((x_a,x_b) \\in \\mathcal{L} \\times \\mathcal{L}\\), \\[ y_{x_a,u}(t) = y_{x_b,u}(t), \\forall t \\in [0,\\min\\left\\{\\sigma^+_{\\mathbb{X}}(x_a;u), \\sigma^+_{\\mathbb{X}}(x_b;u) \\right\\}] \\Longrightarrow x_a = x_b \\] Instantaneously distinguishable on \\(\\mathcal{L}\\) for some input \\(u(t)\\), if for all \\((x_a,x_b) \\in \\mathcal{L} \\times \\mathcal{L}\\), and for all \\(\\bar{t} \\in (0, \\min\\left\\{\\sigma^+_{\\mathbb{X}}(x_a;u), \\sigma^+_{\\mathbb{X}}(x_b;u) \\right\\})\\), \\[ y_{x_a,u}(t) = y_{x_b,u}(t), \\forall t \\in [0,\\bar{t}) \\Longrightarrow x_a = x_b \\] Uniformly observable on \\(\\mathcal{L}\\) if it is distinguishable on \\(\\mathcal{L}\\) for any input \\(u(t)\\) (not only for \\(u \\in \\mathcal{U}\\)) Uniformly instantaneously observable on \\(\\mathcal{L}\\) if it is instantaneously observable on \\(\\mathcal{L}\\) for any input \\(u(t)\\) (not only for \\(u \\in \\mathcal{U}\\)). Moreover, let \\(\\mathcal{X}\\) be a subset of \\(\\mathbb{X}\\) such that \\(\\mathrm{cl}(\\mathcal{X})\\), i.e., the closure of \\(\\mathcal{X}\\), is contained in \\(\\mathcal{L}\\). Then the system (5.1) is said to be Backward \\(\\mathcal{L}\\)-distinguishable on \\(\\mathcal{X}\\) for some input \\(u(t)\\), if for any \\((x_a,x_b) \\in \\mathcal{X} \\times \\mathcal{X}\\) such that \\(x_a \\neq x_b\\), there exists \\(t \\in (\\max\\left\\{ \\sigma^{-}_{\\mathcal{L}}(x_a;u), \\sigma^{-}_{\\mathcal{L}}(x_b;u) \\right\\},0]\\) such that \\(y_{x_a,u}(t) \\neq y_{x_b,u}(t)\\), or in words similar to the definition of distinguishable on \\(\\mathcal{L}\\), for all \\((x_a,x_b) \\in \\mathcal{X} \\times \\mathcal{X}\\) \\[ y_{x_a,u}(t) = y_{x_b,u}(t), \\forall t \\in (\\max\\left\\{\\sigma^{-}_{\\mathcal{L}}(x_a;u), \\sigma^{-}_{\\mathcal{L}}(x_b;u) \\right\\},0] \\Longrightarrow x_a = x_b. \\] 5.1.2 Luenberger Template Consider an instance of the normal form (5.7) as follows: \\[\\begin{equation} \\dot{\\xi} = A \\xi + B(u,y), \\quad y = C \\xi, \\tag{5.10} \\end{equation}\\] where \\(A,C\\) are constant matrices, and \\(B(u,y)\\) can depend nonlinearly on \\(u\\) and \\(y\\). For this template, we have the well-known Luenberger observer. Theorem 5.2 (Luenberger Observer) If the pair \\((A,C)\\) is detectable (see Theorem A.9), then there exists a matrix \\(K\\) such that \\(A-KC\\) is Hurwitz and the system \\[\\begin{equation} \\dot{\\hat{\\xi}} = A \\hat{\\xi} + B(u,y) + K(y - C \\hat{\\xi}) \\tag{5.11} \\end{equation}\\] is an observer for (5.10). Proof. Define \\(e(t) = \\xi(t) - \\hat{\\xi}(t)\\). In that case, \\[\\begin{equation} \\dot{e}(t) = [A - KC] e(t) \\tag{5.12} \\end{equation}\\] Solving (5.12), we obtain \\[\\begin{equation} e(t) = \\mathrm{exp}[(A - KC)t] e(0) \\end{equation}\\] Then, if the real components of the eigenvalues of \\(A - KC\\) are strictly negative (i.e., \\(A - KC\\) is Hurwitz), then \\(e(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\), independent of the initial error \\(e(0) = \\xi(0) - \\hat{\\xi}(0)\\). From Theorem A.9, we know that \\((A,C)\\) being detectable implies the existence of \\(K\\) such that \\(A - KC\\) is Hurwitz. If one is further interested in estimating the convergence rate of the Luenberger observer, then one can use the result from Corollary A.1. Particularly, one can solve the Lyapunov equation \\[ (A - KC)^T P + P (A - KC) = - I \\] to obtain \\(P\\). Then the convergence rate of \\(\\Vert e \\Vert\\) towards zero is \\(\\frac{0.5}{\\lambda_{\\max}(P)}\\). The Luenberger observer is an elegant result in observer design (and even in control theory) that has far-reaching impact. In my opinion, the essence of observer design is twofold: (i) to simulate the dynamics when the state estimation is correct, and (ii) to correct the state estimation from observation when it is off. These two pieces of ideas are evident in (5.11): the observer is a copy of the original dynamics (\\(A \\hat{\\xi} + B(u,y)\\)) plus a feedback correction from the difference between the “imagined” observation \\(C\\hat{\\xi}\\) and the true observation \\(y\\). You may think the Luenberger template is restricting because it requires the system to be linear (up to the only nonlinearly in \\(B(u,y)\\)). However, it turns out the Luenberger template is already quite useful, as I will show in the following pendulum example. Example 5.1 (Luenberger Observer for A Simple Pendulum) Consider a simple pendulum dynamics model \\[\\begin{equation} x = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix}, \\quad \\dot{x} = \\begin{bmatrix} \\dot{\\theta} \\\\ - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ \\frac{1}{ml^2} \\end{bmatrix} u, \\quad y = \\theta, \\tag{5.13} \\end{equation}\\] where \\(\\theta\\) the angular position of the pendulum from the vertical line, \\(m &gt; 0\\) the mass of the pendulum, \\(l &gt; 0\\) the length, \\(g\\) the gravitational constant, \\(b &gt; 0\\) the dampling coefficient, and \\(u\\) the control input (torque). We assume we can only observe the angular position of the pendulum in (5.13), e.g., using a camera, but not the angular velocity. Our goal is to construct an observer that can provide a full state estimation. We first note that the pendulum dynamics (5.13) can actually be written in the (linear) Luenberger template (5.10) as10 \\[\\begin{equation} \\begin{split} \\dot{x} &amp; = \\underbrace{\\begin{bmatrix} 0 &amp; 1 \\\\ 0 &amp; - \\frac{b}{ml^2} \\end{bmatrix}}_{=:A} x + \\underbrace{\\begin{bmatrix} 0 \\\\ \\frac{u - mgl \\sin \\theta }{ml^2} \\end{bmatrix}}_{=:B(u,y)} \\\\ y &amp; = \\underbrace{\\begin{bmatrix} 1 &amp; 0 \\end{bmatrix}}_{=:C} x \\end{split}. \\tag{5.14} \\end{equation}\\] In order for us to use the Luenberger observer, we need to check if the pair \\((A,C)\\) is detectable. We can easily find the eigenvalues and eigenvectors of \\(A\\): \\[ A \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 0,\\quad A \\begin{bmatrix} - \\frac{ml^2}{b} \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ - \\frac{b}{ml^2} \\end{bmatrix} = - \\frac{b}{ml^2} \\begin{bmatrix} - \\frac{ml^2}{b} \\\\ 1 \\end{bmatrix}. \\] The first eigenvalue has real part equal to \\(0\\). However, \\[ C \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 1 \\neq 0. \\] According to Theorem A.9, we conclude \\((A,C)\\) is detectable. In fact, the pair \\((A,C)\\) is more than just detectable, it is indeed observable (according to Theorem A.7). Therefore, the poles of \\(A - KC\\) can be arbitrarily placed. Finding \\(K\\). Now we need to find \\(K\\). An easy choice of \\(K\\) is \\[ K = \\begin{bmatrix} k \\\\ 0 \\end{bmatrix}, \\quad A - KC = \\begin{bmatrix} - k &amp; 1 \\\\ 0 &amp; - \\frac{b}{ml^2} \\end{bmatrix}. \\] With \\(k &gt; 0\\), we know \\(A- KC\\) is guaranteed to be Hurwitz (the two eigenvalues of \\(A-KC\\) are \\(-k\\) and \\(-b/ml^2\\)), and we have obtained an observer! We can also estimate the convergence rate of this observer. Let us use \\(m=1,g=9.8,l=1,b=0.1\\) as parameters of the pendulm dynamics. According to Theorem 5.2, we solve the Lyapunov equation \\[ (A - KC)^T P + P(A - KC) = -I \\] and \\(\\gamma = \\frac{0.5}{\\lambda_{\\max}(P)}\\) will be our best estimate of the convergence rate (of \\(\\Vert e \\Vert = \\Vert \\hat{x} - x \\Vert\\) towards zero). Table 5.1 below shows the convergence rates computed for different values of \\(k\\). We can see that as \\(k\\) is increased, the convergence rate estimation is also increased. However, it appears that \\(0.1\\) is the best convergence rate we can achieve, regardless of how large \\(k\\) is. Table 5.1: Convergence rate estimation of the Luenberger observer for a simple pendulm. \\(k\\) \\(0.1\\) \\(1\\) \\(10\\) \\(100\\) \\(1000\\) \\(10000\\) \\(\\gamma\\) \\(0.0019\\) \\(0.0523\\) \\(0.0990\\) \\(0.1000\\) \\(0.1000\\) \\(0.1000\\) Optimal \\(K\\). Is it true that \\(0.1\\) is the best convergence rate, or in other words, what is the best \\(K\\) that maximizes the convergence rate \\(\\gamma\\)? A natural way (and my favorite way) to answer this question is to formulate an optimization problem. \\[\\begin{equation} \\begin{split} \\min_{P,K} &amp; \\quad \\lambda_{\\max}(P) \\\\ \\text{subject to} &amp; \\quad (A - KC)^T P + P (A - KC) = -I \\\\ &amp; \\quad P \\succeq 0 \\end{split} \\tag{5.15} \\end{equation}\\] The above formulation seeks the best possible \\(K\\) that minimizes \\(\\lambda_{\\max}(P)\\) which, according to \\(\\gamma = 0.5 / \\lambda_{\\max}(P)\\), also maximizes \\(\\gamma\\). However, problem (5.15) is not a convex formulation due to the bilinear term \\(PK\\). Nevertheless, via a simple change of variable \\(H = PK\\), we arrive at the following convex formulation \\[\\begin{equation} \\begin{split} \\min_{P,H} &amp; \\quad \\lambda_{\\max}(P) \\\\ \\text{subject to} &amp; \\quad A^T P - C^T H^T + PA - H C = - I \\\\ &amp; \\quad P \\succeq 0 \\end{split} \\tag{5.16} \\end{equation}\\] Problem (5.16) is a semidefinite programming problem (SDP), that can be modeled and solved by off-the-shelf tools. We can recover \\(K = P^{-1} H\\) from (5.16) after it is solved. Interestingly, solving problem (5.16) verifies that the minimum \\(\\lambda_{\\max}(P)\\) is \\(5\\) and the maximum converge rate is \\(0.1\\). An optimal solution of (5.16) is \\[ P^\\star = \\begin{bmatrix} 2.4923 &amp; 0 \\\\ 0 &amp; 5 \\end{bmatrix}, \\quad K^\\star = \\begin{bmatrix} 0.2006 \\\\ 0.4985 \\end{bmatrix}. \\] You should check out the Matlab code of this example here. 5.1.3 State-affine Template Consider an instance of the normal form (5.7) where the dynamics is linear in \\(\\xi\\), but the coefficients are time-varying and dependent on the input and output \\[\\begin{equation} \\dot{\\xi} = A(u,y) \\xi + B(u,y), \\quad y = C(u) \\xi. \\tag{5.17} \\end{equation}\\] The difference between the state-affine template (5.17) and the Luenberger template (5.10) is that the linear matrices \\(A,C\\) are allowed to depend nonlinearly on the input \\((u,y)\\). Kalman and Bucy originally proposed an observer for linear time-varying systems (Kalman and Bucy 1961). The result is later extened by (Besançon, Bornard, and Hammouri 1996) and (Hammouri and Leon Morales 1990) to deal with coefficient matrices dependent on the control. The following theorem is a direct extension of the result from (Besançon, Bornard, and Hammouri 1996) and (Hammouri and Leon Morales 1990) by considering \\((u,y)\\) as an augmented control input. Before presenting the theorem, we need to introduce the following terminology. Definition 5.3 (Linear Time-Varying System) For a linear time-varying system of the form \\[\\begin{equation} \\dot{\\chi} = A(\\nu) \\chi, \\quad y = C(\\nu) \\chi, \\tag{5.18} \\end{equation}\\] with input \\(\\nu\\) and output \\(y\\), we define the transition matrix \\(\\Psi_\\nu\\) as the unique solution to \\[ \\Psi_\\nu (t,t) = I, \\quad \\frac{\\partial \\Psi_\\nu}{\\partial \\tau}(\\tau,t) = A(\\nu(\\tau)) \\Psi_\\nu (\\tau, t). \\] Note that the transition matrix is used to express the solution to (5.18) because it satisfies \\[ \\chi(\\chi_0,t_0;t;\\nu) = \\Psi_\\nu (t,t_0) \\chi_0. \\] the observability grammian as \\[ \\Gamma_\\nu (t_0,t_1) = \\int_{t_0}^{t_1} \\Psi_\\nu (\\tau,t_0)^T C(\\nu(\\tau))^T C(\\nu(\\tau)) \\Psi_\\nu (\\tau,t_0) d\\tau. \\] the backward observability grammian as \\[ \\Gamma_\\nu^b (t_0,t_1) = \\int_{t_0}^{t_1} \\Psi_\\nu (\\tau,t_1)^T C(\\nu(\\tau))^T C(\\nu(\\tau)) \\Psi_\\nu (\\tau,t_1) d\\tau. \\] We now introduce the Kalman-Bucy Observer for the state-affine template (5.17). Theorem 5.3 (Kalman-Bucy Observer) Let \\(y_{\\xi_0,u}(t) = C(u(t)) \\Xi (\\xi_0;t;u)\\) be the output of system (5.17) at time \\(t\\) with initialization \\(\\xi_0\\) and control \\(u\\). Suppose the control \\(u\\) satisfies For any \\(\\xi_0\\), \\(t \\mapsto A(u(t),y_{\\xi_0,u}(t))\\) is bounded by \\(A_{\\max}\\) For any \\(\\xi_0\\), the augmented input \\(\\nu = (u,y_{\\xi_0,u})\\) is regularly persistent for the dynamics \\[\\begin{equation} \\dot{\\chi} = A(\\nu) \\chi , \\quad y = C(\\nu) \\chi \\tag{5.19} \\end{equation}\\] uniformly with respect to \\(\\xi_0\\). That is, there exist strictly positive numbers \\(t_0,\\bar{t}\\), and \\(\\alpha\\) such that for any \\(\\xi_0\\) and any time \\(t \\geq t_0 \\geq \\bar{t}\\), \\[ \\Gamma_v^b (t-\\bar{t}, t) \\succeq \\alpha I, \\] where \\(\\Gamma_v^b\\) is the backward observability grammian associated with system (5.19). Then, given any positive definite matrix \\(P_0\\), there exist \\(\\alpha_1,\\alpha_2 &gt; 0\\) such that for any \\(\\lambda \\geq 2 A_{\\max}\\) and any \\(\\xi_0 \\in \\mathbb{R}^p\\), the matrix differential equation \\[\\begin{equation} \\dot{P} = -\\lambda P - A(u,y)^T P - P A(u,y) + C(u)^T C(u) \\tag{5.20} \\end{equation}\\] initialized at \\(P(0) = P_0\\) admits a unique solution satisfying \\(P(t)=P(t)^T\\) and \\[ \\alpha_2 I \\succeq P(t) \\succeq \\alpha_1 I. \\] Moreover, the system \\[\\begin{equation} \\dot{\\hat{\\xi}} = A(u,y) \\hat{\\xi} + B(u,y) + K (y - C(u)\\hat{\\xi}) \\tag{5.21} \\end{equation}\\] with a time-varying gain matrix \\[\\begin{equation} K = P^{-1} C(u)^T \\tag{5.22} \\end{equation}\\] is an observer for the state-affine system (5.17). Let us work out an example of the Kalman-Bucy Observer for nonlinear systems. Example 5.2 (Kalman-Bucy Observer for A Simple Pendulum) Let us reconsider the pendulum dynamics (5.13) but this time try to design a Kalman-Bucy observer. We first write the pendulum dynamics in a new coordinate system so that it is in the state-affine normal form (5.17). We choose \\(\\xi = [\\mathfrak{s},\\mathfrak{c},\\dot{\\theta}]^T\\) with \\(\\mathfrak{s} = \\sin \\theta\\) and \\(\\mathfrak{c} = \\cos \\theta\\). Clearly, we will be able to observe \\(y = [\\mathfrak{s},\\mathfrak{c}]^T\\) in this new coordinate. The state-affine normal form of the pendulum dynamics reads \\[\\begin{equation} \\begin{split} \\dot{\\xi} = \\begin{bmatrix} \\mathfrak{c} \\dot{\\theta} \\\\ - \\mathfrak{s} \\dot{\\theta} \\\\ - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\mathfrak{s} ) + \\frac{1}{ml^2} u \\end{bmatrix} &amp; = \\underbrace{\\begin{bmatrix} 0 &amp; 0 &amp; \\mathfrak{c} \\\\ 0 &amp; 0 &amp; -\\mathfrak{s} \\\\ 0 &amp; 0 &amp; -\\frac{b}{ml^2} \\end{bmatrix}}_{=:A(u,y)} \\xi + \\underbrace{\\begin{bmatrix} 0 \\\\ 0 \\\\ \\frac{u - mgl \\mathfrak{s}}{ml^2} \\end{bmatrix}}_{=:B(u,y)} \\\\ y &amp; = \\underbrace{\\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix}}_{=:C(u)} \\xi \\end{split}. \\tag{5.23} \\end{equation}\\] Note that \\(C(u)\\) is in fact time-invariant, and \\(B(u,y)\\) only depends on \\(u\\); but we adopt the same notation as the general state-affine template (5.17). In order to use the Kalman-Bucy observer in Theorem 5.3, we need to verify the boundedness of \\(A(u,y)\\), and the regular persistence of (5.19). Boundedness of \\(A(u,y)\\). We can easily show the boundedness of \\(A(u,y)\\) by writing \\[ \\Vert A(u,y) \\xi \\Vert = \\Vert \\xi_3 (\\mathfrak{c} - \\mathfrak{s} - b/ml^2) \\Vert \\leq |\\xi_3| \\sqrt{3} \\sqrt{\\mathfrak{c}^2 + \\mathfrak{s}^2 + b^2 / m^2 l^4} \\leq \\Vert \\xi \\Vert \\sqrt{3 + 3b^2 / m^2 l^4}. \\] Therefore, we can take \\(A_{\\max} = \\sqrt{3 + 3b^2 / m^2 l^4}\\). Regular persistence. We write the backward observability grammian of system (5.19) \\[ \\Gamma_\\nu^b(t - \\bar{t},t) = \\int_{t - \\bar{t}}^t \\Psi_\\nu(\\tau,t)^T C^T C \\Psi_\\nu (\\tau, t) d \\tau = \\int_{t - \\bar{t}}^t \\Psi_\\nu(\\tau,t)^T \\underbrace{\\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}}_{=:\\tilde{C}} \\Psi_\\nu (\\tau, t) d \\tau. \\] \\(\\Gamma_\\nu^b(t - \\bar{t},t) \\succeq \\alpha I\\) if and only if \\[ w^T \\Gamma_\\nu^b(t - \\bar{t},t) w \\geq \\alpha, \\quad \\forall w \\in \\mathbb{R}^3, \\Vert w \\Vert = 1. \\] With this, we develop \\(w^T \\Gamma_\\nu^b(t - \\bar{t},t) w\\) \\[\\begin{equation} \\begin{split} w^T \\Gamma_\\nu^b(t - \\bar{t},t) w &amp;= \\int_{t - \\bar{t}}^t s^T \\tilde{C} s d\\tau, \\\\ &amp; = \\int_{t - \\bar{t}}^t \\left( s_1^2 + s_2^2 \\right) d\\tau, \\quad s = \\Psi_\\nu (\\tau, t) w \\end{split} \\tag{5.24} \\end{equation}\\] and observe that \\(s = \\Psi_\\nu (\\tau,t) w\\) is equivalent to \\[ w = (\\Psi_\\nu (\\tau,t))^{-1} s = \\Psi_\\nu (t,\\tau) s, \\] that is, \\(w\\) is the solution of \\(\\dot{\\xi} = A(\\nu) \\xi\\) at time \\(t\\) with initial condition \\(s\\) at time \\(\\tau \\leq t\\). Equivalently, this is saying \\(s\\) is the initial condition of \\(\\dot{\\xi} = A(\\nu) \\xi\\) at time \\(\\tau \\leq t\\) such that its solution at time \\(t\\) is \\(w\\). Note that from (5.24) it is clearly that \\(\\int_{t - \\bar{t}}^t \\left( s_1^2 + s_2^2 \\right) d\\tau \\geq 0\\), and \\(\\int_{t - \\bar{t}}^t \\left( s_1^2 + s_2^2 \\right) d\\tau = 0\\) if and only if \\(s_1^2 + s_2^2 = 0\\), or equivalently \\(s_1 = s_2 = 0\\) for any \\(\\tau \\in [t - \\bar{t}, t]\\). We then take a closer look at the system \\(\\dot{\\xi} = A(\\nu) \\xi\\): \\[\\begin{equation} \\begin{split} \\dot{\\xi}_1 &amp;= \\mathfrak{c} \\xi_3 \\\\ \\dot{\\xi}_2 &amp;= -\\mathfrak{s} \\xi_3 \\\\ \\dot{\\xi}_3 &amp;= - \\frac{b}{ml^2} \\xi_3. \\end{split} \\tag{5.25} \\end{equation}\\] If the solution of \\(\\xi_3\\) at time \\(t\\) is \\(w_3\\), then \\[ \\xi_3(\\tau) = w_3 e^{\\frac{b}{ml^2}(t - \\tau)}, \\quad \\tau \\leq t. \\] We can now claim it is impossible that \\(s_1 = s_2 = 0\\) at any time \\(\\tau \\in [t - \\bar{t}, t]\\). We can show this by contradiction. First of all, \\(s_1 = s_2 = 0\\) at \\(\\tau = t\\) implies \\(w_1 = w_2 = 0\\) and hence \\(w_3 = \\pm 1\\). This implies \\(\\xi_3 \\neq 0\\) for any \\(\\tau \\in [t - \\bar{t}, t]\\). Then, \\(s_1 = 0, \\forall \\tau \\in [t - \\bar{t}, t]\\) implies \\(\\dot{\\xi}_1 = 0\\) which, due to \\(\\xi_3 \\neq 0\\), implies \\(\\mathfrak{c} = 0\\) for all \\(\\tau\\). Similarly, \\(s_2 = 0, \\forall \\tau \\in [t - \\bar{t}, t]\\) implies \\(\\dot{\\xi}_2 = 0\\) and \\(\\mathfrak{s} = 0\\). This creates a contradiction because \\(\\mathfrak{c}^2 + \\mathfrak{s}^2 = 1\\) and \\(\\mathfrak{c}, \\mathfrak{s}\\) cannot be simultaneously zero. The above reasoning proves that the backward observability Grammian is positive definite, which is, however, still insufficient for the Kalman-Bucy observer. We need a stronger uniformly positive definite condition on \\(\\Gamma_\\nu^b\\), i.e., to find \\(t_0, \\bar{t}\\) and \\(\\alpha&gt;0\\) so that \\(\\Gamma_\\nu^b(t-\\bar{t},t) \\succeq \\alpha I\\) for all \\(t \\geq t_0\\). If the control \\(u\\) is unbounded, then sadly, one can show that the uniform positive definite condition fails to hold, as left by you to show in the following exercise. Exercise 5.1 (Counterexample for Kalman-Bucy Observer) Show that, if the control \\(u\\) is unbounded, then for any \\(\\alpha &gt; 0\\), \\(t_0 \\geq \\bar{t} &gt; 0\\), there exists \\(t \\geq t_0\\) such that \\(\\Gamma_\\nu^b(t - \\bar{t},t) \\prec \\alpha I\\). (Hint: consider a controller that spins the pendulum faster and faster such that in time \\(\\bar{t}\\) it has rotated \\(2k\\pi\\), in this case the angular velocity becomes unobservable because we are not sure how many rounds the pendulum has rotated.) Fortunately, if the control \\(u\\) is bounded, then we can prove the uniform positive define condition holds for \\(\\Gamma_\\nu^b(t - \\bar{t},t)\\). The following proof is given by Weiyu Li. Without loss of generality, let \\(\\frac{b}{ml^2} = 1\\). Assume \\(u\\) is bounded such that the third entry of \\(B(u,y)\\) in (5.23) is bounded by \\(\\beta &gt; 0\\) \\[ \\left| \\frac{u - mgl \\mathfrak{s}}{ml^2} \\right| \\leq \\beta. \\] Assuming the initial velocity of the pendulum is \\(\\dot{\\theta}(0) = \\dot{\\theta}_0\\), we know \\(\\dot{\\theta}(t)\\) is bounded by \\[ \\dot{\\theta}(t) \\in \\left[ c_1(1-\\beta)e^{-t} - \\beta , c_2(1-\\beta)e^{-t} + \\beta \\right], \\] where \\(c_1,c_2\\) are constants chosen to satisfy the initial condition. Clearly, for all \\(t &gt; 0\\), we see \\(\\dot{\\theta}(t)\\) is bounded, and hence we know \\(\\dot{\\mathfrak{c}}\\) and \\(\\dot{\\mathfrak{s}}\\) are bounded (due to \\(\\mathfrak{c}\\) and \\(\\mathfrak{s}\\) are bounded). Intuitively, what we have just shown says that when the control is bounded, the measurements \\(\\mathfrak{c}\\) and \\(\\mathfrak{s}\\) will have bounded time derivatives. (This will help us analyze the auxiliary system (5.25).) Now back to checking regular persistence of the auxilary system (5.25). We will discuss two cases: (1) \\(w_3^2 &gt; 1 - \\delta\\), and (2) \\(w_3^2 \\leq 1 - \\delta\\), for some constant \\(\\delta &lt; 0.5\\) determined later. \\(w_3^2 &gt; 1 - \\delta &gt; 0.5\\). In this case we have \\(w_1^2 + w_2^2 = 1 - w_3^2 &lt; \\delta\\), and hence \\(w_1^2 &lt; \\delta\\), \\(w_2^2 &lt; \\delta\\). On the other hand, from (5.25) we have \\[ \\dot{\\xi}_1^2(\\tau) + \\dot{\\xi}_2^2(\\tau) = \\xi_3^2 = w_3^2 e^{2 (t - \\tau)} &gt; w_3^2 &gt; 1 - \\delta, \\quad \\forall \\tau &lt; t. \\] Without loss of generality assume \\(\\dot{\\xi}_1(t)^2 &gt; (1-\\delta)/2\\). As \\(\\dot{\\xi}_1 = \\mathfrak{c} \\xi_3\\) and both \\(\\mathfrak{c}\\) and \\(\\xi_3\\) have bounded derivatives, we know \\(\\dot{\\xi}_1\\) will not change sign for some duration \\(T\\) that is independent from the choice of \\(\\delta\\) (because the time derivatives of \\(\\mathfrak{c}\\) and \\(\\xi_3\\) do not depend on \\(\\delta\\)). That is \\(|\\dot{\\xi}_1| &gt; \\sqrt{(1-\\delta)/2} &gt; 1/2\\) for \\(\\tau \\in [t - T,t]\\). Consequently, \\[ |\\xi_1(t - \\tau)| &gt; \\frac{1}{2} \\tau - |w_1| &gt; \\frac{1}{2} \\tau - \\sqrt{\\delta}, \\quad \\tau \\in [0,T]. \\] Choosing \\(\\delta\\) small enough, we have \\(|\\xi_1(t - \\tau)| &gt; 0.25 \\tau\\) for \\(\\tau \\in [0.5T,T]\\). Then we have \\[ \\Gamma_\\nu^b(t - T, t) \\succ [(0.25 \\times 0.5 T)^2 \\times 0.5T]I. \\] \\(w_3^2 \\leq 1-\\delta\\). In this case \\(w_1^2 + w_2^2 = 1 - w_3^2 \\geq \\delta\\), and at least one of \\(w_1\\) and \\(w_2\\) has absolute value larger than \\(\\sqrt{\\delta/2}\\). Because the derivatives of \\(\\xi_1\\) and \\(\\xi_2\\) are both bounded, we know \\(\\xi_1\\) and \\(\\xi_2\\) will remain large for some constant time. Thus there is a uniform lower bound. The intuition of the above proof is simple: when \\(\\xi_1\\) and/or \\(\\xi_2\\) already have large absolute value (case 2), we can find a time window such that \\(\\xi_1\\) and/or \\(\\xi_2\\) remain large in that time window; when \\(\\xi_1\\) and/or \\(\\xi_2\\) are small (case 1), using the observation that their time derivatives are large (because \\(w_3\\) is large), together with the fact that these derivatives remain large (because the derivative of these derivatives are bounded), we can also find a time window that \\(\\xi_1\\) and/or \\(\\xi_2\\) are large (back in time). Therefore, the backward observability Grammian is uniformly positive definite. 5.1.4 Kazantzis-Kravaris-Luenberger (KKL) Template In Luenberger’s original paper about observer design for linear systems (Luenberger 1964), the goal was to transform a linear system \\[ \\dot{x} = F x, \\quad y = C x \\] into a Hurwitz form \\[\\begin{equation} \\dot{\\xi} = A \\xi + B y \\tag{5.26} \\end{equation}\\] with \\(A\\) a Hurwitz (stable) matrix. If such a transformation is available, then the following system \\[ \\dot{\\hat{\\xi}} = A \\hat{\\xi} + B y, \\] which is nothing but a copy of the dynamics (5.26), is in fact an observer. This is because the error \\(e = \\hat{\\xi} - \\xi\\) evolves as \\[ \\dot{e} = A e, \\] which implies that \\(e\\) tends to zero regardless of the initial error \\(e(0)\\). Luenberger proved that when \\((F,C)\\) is observable, a stationary transformation \\(\\xi = T x\\) with \\(p = n\\), i.e., \\(T \\in \\mathbb{R}^{n\\times n}\\), always exists and is unique, for any matrix \\(A\\) that is Hurwitz and \\((A,B)\\) that is controllable. This is based on the fact that \\[\\begin{align} (A T + B C) x =A \\xi + B y =\\dot{\\xi} = T \\dot{x} = TF x, \\forall x \\\\ \\Longleftrightarrow AT + BC = TF, \\end{align}\\] known as the Sylvester equation, admits a unique and invertible solution \\(T\\). A natural extension of Luenberger’s original idea is to find a transformation that converts the nonlinear system (5.1) into the following form \\[\\begin{equation} \\dot{\\xi} = A \\xi + B(u,y), \\quad y = H(\\xi,u), \\tag{5.27} \\end{equation}\\] with \\(A\\) a Hurwitz matrix (but \\(H\\) can be nonlinear, as opposed to the Luenberger template in Theorem 5.2). If such a transformation can be found, then we can design a similar observer that copies the dynamics (5.27) \\[\\begin{equation} \\dot{\\hat{\\xi}} = A \\hat{\\xi} + B(u,y). \\tag{5.28} \\end{equation}\\] We refer to such a nonlinear Luenberger template the Kazantzis-Kravaris-Luenberger (KKL) template, due to the seminal work (Kazantzis and Kravaris 1998). The KKL template, once found, is nice in the sense that (i) the observer (5.28) is a simple copy of the dynamics and also very easy to implement (as opposed to the Kalman-Bucy observer); and (ii) checking if the matrix \\(A\\) is Hurwitz is easy, at least when \\(A\\) has reasonable size, (e.g., compared to checking the regular persistence condition in the state-affine template in Theorem 5.3). However, the KKL template is difficult to realize in the sense that (i) what kind of nonlinear systems can be converted to (5.27), and (ii) for those systems, how do we find the coordinate transformation? Recent works have leveraged deep learning to learn the coordinate transformation, for example in (Janny et al. 2021), (Niazi et al. 2023), (Miao and Gatsis 2023). Before hammering the problem with deep learning, let us look at the fundamentals of the KKL observer. 5.1.4.1 Autonomous Systems Consider the autonomous version of system (5.1) without control \\[\\begin{equation} \\dot{x} = f(x), \\quad y = h(x), \\tag{5.29} \\end{equation}\\] where \\(x \\in \\mathbb{X} \\subseteq \\mathbb{R}^n, y \\in \\mathbb{Y} \\subseteq \\mathbb{R}^d\\). The following result, established by (Andrieu and Praly 2006), states that the KKL observer exists under mild conditions. Theorem 5.4 (KKL Observer for Autonomous Systems) Assum \\(\\mathcal{X}\\) and \\(\\mathcal{L}\\) are open bounded sets in \\(\\mathbb{X}\\) (the state space) such that \\(\\mathrm{cl}(\\mathcal{X})\\) is contained in \\(\\mathcal{L}\\) and the system (5.29) is backward \\(\\mathcal{L}\\)-distinguishable on \\(\\mathcal{X}\\) (cf. Definition 5.2). Then there exists a strictly positive number \\(\\gamma\\) and a set \\(\\mathcal{S}\\) of zero Lebesgue measure in \\(\\mathbb{C}^{n+1}\\) such that denoting \\(\\Omega = \\{ \\lambda \\in \\mathbb{C} \\mid \\mathrm{Re}(\\lambda) &lt; - \\gamma \\}\\), for any \\((\\lambda_1,\\dots,\\lambda_{n+1}) \\in \\Omega^{n+1} \\backslash \\mathcal{S}\\), there exists a function \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^{(n+1)d}\\) uniformly injective on \\(\\mathcal{X}\\) satisfying \\[ L_f T(x) = A T(x) + B(h(x)) \\] with \\[\\begin{align} A = \\tilde{A} \\otimes I_d, \\quad B(y) = (\\tilde{B} \\otimes I_d ) y \\\\ \\tilde{A} = \\begin{bmatrix} \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_{n+1} \\end{bmatrix} \\quad \\tilde{B} = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}. \\end{align}\\] Moreover, if \\(\\mathcal{X}\\) is backward invariant, then \\(T\\) is unique and defined by \\[\\begin{equation} T(x) = \\int_{-\\infty}^0 e^{-A\\tau} B(h(X(x,\\tau))) d\\tau. \\tag{5.30} \\end{equation}\\] Remark. The function \\(T\\) in Theorem 5.4 takes complex numbers. To simulate the observer \\[ \\dot{\\hat{\\xi}} = A \\hat{\\xi} + B(y), \\] one needs to implement in real numbers, for each \\(\\lambda_i\\) and \\(j \\in [d]\\) \\[ \\dot{\\hat{\\xi}}_{\\lambda_i,j} = \\begin{bmatrix} - \\mathrm{Re}(\\lambda_i) &amp; - \\mathrm{Im}(\\lambda_i) \\\\ \\mathrm{Im}(\\lambda_i) &amp; - \\mathrm{Re}(\\lambda_i) \\end{bmatrix} \\hat{\\xi}_{\\lambda_i,j} + \\begin{bmatrix} y_j \\\\ 0 \\end{bmatrix}. \\] Therefore, the dimension of the observer is \\(2 \\times d (n+1)\\). Theorem 5.4 states that as long as the system (5.29) is backward distinguishable, then there exists a stationary transformation \\(T\\) that can transform the system to a new coordinate system \\(\\xi\\) such that the dynamics in \\(\\xi\\) is Hurwitz. A closer look at the structure of \\(A\\) and \\(B\\) reveals that the coordinate transformation needs to satisfy \\(n+1\\) differential equations of the form \\[ \\frac{\\partial T_{\\lambda}}{\\partial x}(x) \\dot{x} = \\lambda T_{\\lambda} (x) + y \\] where each \\(T_{\\lambda}\\) transforms the state \\(x\\) into a new coordinate having the same dimension of \\(y\\). Clearly, if \\(T = (T_\\lambda)\\), i.e., there is a single \\(\\lambda\\), then \\(T\\) is not uniformly injective (as the dimension of \\(\\xi\\) is \\(d &lt; n\\)). Consequently, by choosing \\[ T = (T_{\\lambda_1},\\dots,T_{\\lambda_{n+1}}), \\] the uniform injectivity of \\(T\\) is ensured. However, the difficulty lies in the computation of \\(T\\) (and \\(T_\\lambda\\)), let alone its inverse (that recovers \\(x\\) from \\(\\xi\\)). Even though \\(\\mathcal{X}\\) is backward invariant, the formulation (5.30) is difficult to compute. I tried very hard to find a coordinate transformation \\(T\\) that can convert the non-controlled pendulum dynamics into the KKL form but did not succeed. You should let me know if you were able to find one! Nevertheless, the following example shows you the flavor of how such a transformation may look like for a different system. Example 5.3 (KKL Observer for an Oscillator with Unknown Frequency) Consider a harmonic oscillator with unknown frequency \\[ \\begin{cases} \\dot{x}_1 = x_2 \\\\ \\dot{x}_2 = - x_1 x_3 \\\\ \\dot{x}_3 = 0 \\end{cases}, \\quad y = x_1 \\] Consider the coordinate transformation \\[ T_{\\lambda_i} (x) = \\frac{\\lambda_i x_1 - x_2}{\\lambda_i^2 + x_3}, \\quad \\lambda_i &gt; 0, i=1,\\dots,p. \\] We have \\[\\begin{align} \\frac{\\partial T_{\\lambda_i}(x)}{\\partial x} \\dot{x} &amp;= \\left\\langle \\begin{bmatrix} \\frac{\\lambda_i}{\\lambda_i^2 + x_3} \\\\ \\frac{-1 }{\\lambda_i^2 + x_3} \\\\ \\frac{x_2 - \\lambda_i x_1}{(\\lambda_i^2 + x_3)^2} \\end{bmatrix}, \\begin{bmatrix} x_2 \\\\ -x_1 x_3 \\\\ 0 \\end{bmatrix} \\right \\rangle = \\frac{\\lambda_i x_2 + x_1 x_3}{\\lambda_i^2 + x_3} \\\\ -\\lambda_i T_{\\lambda_i}(x) + y &amp;= \\frac{-\\lambda_i^2 x_1 + \\lambda_i x_2 + x_1 \\lambda_i^2 + x_1 x_3}{\\lambda_i^2 + x_3} = \\frac{\\lambda_i x_2 + x_1 x_3}{\\lambda_i^2 + x_3} \\end{align}\\] Therefore, with \\[ \\xi = T(x) = [T_{\\lambda_1}(x), T_{\\lambda_2}(x),\\dots,T_{\\lambda_p}(x)]^T, \\] we have \\[ \\dot{\\xi} = \\underbrace{\\begin{bmatrix} - \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; -\\lambda_p \\end{bmatrix}}_{A} \\xi + \\begin{bmatrix}1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} y \\] with \\(A\\) clearly Hurwitz. With some extra arguments (cf. Section 8.1.1 in (Bernard 2019)), one can see that the transformation \\(T\\) is injective with \\(p \\geq 4\\) distinct \\(\\lambda_i\\)’s. Therefore, this is a valid KKL observer. The final issue that one needs to think about is, since the observer is estimating \\(\\hat{\\xi}\\), how to recover \\(\\hat{x}\\)? In this example, there is actually no analytical formula for recovering \\(\\hat{x}\\) from \\(\\hat{\\xi}\\). In this case, one approach is to solve the following optimization problem \\[ \\hat{x} = \\arg\\min_{x} \\Vert \\hat{\\xi} - T(x) \\Vert^2, \\] which may be quite expensive. A more general treatment is given in Section 8.2.2 in (Bernard 2019). 5.1.4.2 Controlled Systems 5.1.5 Triangular Template 5.1.6 Design with Convex Optimization Consider a nonlinear system \\[\\begin{equation} \\dot{x} = f(x) + \\psi(u,y), \\quad y = Cx \\tag{5.31} \\end{equation}\\] where \\(x \\in \\mathbb{X} \\subseteq \\mathbb{R}^n\\), \\(y \\in \\mathbb{R}^d\\), \\(C\\) a constant matrix, and \\(\\psi(u,y)\\) a nonlinear function. We assume that \\(f(x)\\) is a polynomial vector map (i.e., each entry of \\(f\\) is a polynomial function in \\(x\\)). Certainly the formulation in (5.31) is not as general as (5.1), but it is general enough to include many examples in robotics. Recall that I said the essence of observer design is to (i) simulate the dynamics when the state estimation is correct, and (ii) to correct the state estimation from observation when it is off. Therefore, we wish to design an observer for (5.31) in the following form \\[\\begin{equation} \\dot{\\hat{x}} = \\underbrace{f(\\hat{x}) + \\psi(u,y)}_{\\text{dynamics simulation}} + \\underbrace{K(y - \\hat{y},y)(C \\hat{x} - y)}_{\\text{feedback correction}}, \\tag{5.32} \\end{equation}\\] where, compared to the Luenberger observer (5.11), we allow the gain matrix \\(K\\) to be nonlinear functions of the true observation \\(y\\) and the estimated observation \\(\\hat{y}\\). With the observer (5.32), the dynamics on the estimation error \\(e = \\hat{x} - x\\) becomes \\[ \\dot{e} = f(x + e) - f(x) + K(Ce,Cx)C e. \\] If we can find a Lyapunov-like function \\(V(e)\\) so that \\(V(e)\\) is positive definite and \\(\\dot{V}(e)\\) is negative definite, then Lyapunov stability theorem 4.3 tells us that \\(e=0\\) is asymptotically stable. Because we do not know the gain matrix \\(K\\) either, we need to jointly search for \\(V\\) and \\(K\\) (that are polynomials). Mathematically, this is \\[\\begin{equation} \\begin{split} \\text{find} &amp; \\quad V, K \\\\ \\text{subject to} &amp; \\quad V(0) = 0, \\quad V(e) &gt; 0, \\forall e \\neq 0 \\\\ &amp; \\quad \\dot{V}(e) = \\frac{\\partial V}{\\partial e} \\left( f(x + e) - f(x) + K(Ce,Cx)C e \\right) &lt; 0, \\forall e \\neq 0, \\forall x \\in \\mathbb{X} \\\\ &amp; \\quad V(e) \\geq \\epsilon \\Vert e \\Vert^2, \\forall e \\end{split} \\tag{5.33} \\end{equation}\\] where the last constraint is added to make sure \\(V(e)\\) is radially unbounded. Furthermore, if we replace the second constraint by \\(\\dot{V}(e) \\leq - \\lambda V(e)\\), then we can guarantee \\(V(e)\\) converges to zero exponentially. Problem (5.33), however, is not a convex optimization problem, due to the term \\(\\frac{\\partial V}{\\partial e} K\\) being bilinear in the coefficients of \\(V\\) and \\(K\\). Nevertheless, as shown in (Ebenbauer, Renz, and Allgower 2005), we can use a reparameterization trick to formulate a stronger version of (5.33) as follows. \\[\\begin{equation} \\begin{split} \\text{find} &amp; \\quad V, Q(Ce), M(Ce,Cx) \\\\ \\text{subject to} &amp; \\quad V(0) = 0, \\quad V(e) &gt; 0, \\forall e \\neq 0 \\\\ &amp; \\quad \\frac{\\partial V}{\\partial e} = e^T Q(Ce), \\quad Q(Ce) \\succ 0 \\\\ &amp; \\quad e^T Q(Ce) \\left( f(x + e) - f(x) \\right) + e^T M(Ce,Cx) C e &lt; 0, \\forall e \\neq 0, \\forall x \\in \\mathbb{X} \\\\ &amp; \\quad V(e) \\geq \\epsilon \\Vert e \\Vert^2, \\forall e \\end{split} \\tag{5.34} \\end{equation}\\] Clearly, if we can solve problem (5.34), then \\[ K = Q(Ce)^{-1} M(Ce,Cx) \\] is the right gain matrix for the formulation (5.33). Let us bring this idea to action in our pendulum example. Example 5.4 (Pendulum Observer with Convex Optimization) With \\(x = [\\mathfrak{s}, \\mathfrak{c}, \\dot{\\theta}]^T\\) (\\(\\mathfrak{s} = \\sin \\theta, \\mathfrak{c} = \\cos \\theta\\)), we can write the pendulum dynamics as \\[ \\dot{x} = \\underbrace{\\begin{bmatrix} \\mathfrak{c} \\dot{\\theta} \\\\ - \\mathfrak{s} \\dot{\\theta} \\\\ - \\frac{b}{ml^2} \\dot{\\theta} \\end{bmatrix}}_{=:f(x)} + \\underbrace{\\begin{bmatrix} 0 \\\\ 0 \\\\ \\frac{u - mgl \\mathfrak{s}}{ml^2} \\end{bmatrix}}_{=: \\psi(u,y)}, \\quad y = \\underbrace{\\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix}}_{=:C} x \\] Clearly \\(f(x)\\) is a polynomial. Solving the convex optimization problem (5.34), we obtain a solution \\[ V(e) = 0.5954 e_1^2 + 0.5954 e_2^2 + 0.9431 e_3^2 \\] \\[ Q(Ce) = \\begin{bmatrix} 0.4603 e_2^2 + 1.1909 &amp; -0.4603 e_1 e_2 &amp; 0 \\\\ -0.4603 e_1 e_2 &amp; 0.4603 e_1^2 + 1.1909 &amp; 0 \\\\ 0 &amp; 0 &amp; 1.8863 \\end{bmatrix} \\] \\[ M(Ce,Cx) = \\begin{bmatrix} \\substack{-2.0878 e_1^2 - 0.8667 e_2^2 - \\\\ 0.4588 (y_1^2 + y_2^2) - 0.4885} &amp; - 0.8667 e_1 e_2 \\\\ - 0.8667 e_1 e_2 &amp; \\substack{-0.8667 e_1^2 - 2.0878 e_2^2 - \\\\ 0.4588 (y_1^2 + y_2^2) - 0.4885} \\\\ -1.1909 y_2 &amp; 1.1909 y_1 \\end{bmatrix} \\] Simulating this observer, we verify that the observer is in fact exponentially converging, as shown in Fig. 5.1. The Matlab code for formulating and solving the convex optimization (5.34) can be found here. The code for simulating the observer can be found here. Figure 5.1: Simulation of the pendulum observer design from convex optimization 5.2 Observer Feedback Now that we have good ways to design a state observer, we will see how we can use the observer for feedback control. Example 5.5 (Pendulum Stabilization with A Luenberger Observer) In Example 5.1, we have written the dynamics of a pendulum, and the dynamics of a Luenberger observer as \\[\\begin{align} \\dot{x} &amp;= A x + B(u,y) \\\\ \\dot{\\hat{x}} &amp;= A \\hat{x} + B(u,y) + KC (x - \\hat{x}) \\end{align}\\] We wish to understand (so we can optimize) the behavior of this system under certain control input \\(u\\). To do so, let us denote \\(e = \\hat{x} - x\\), and write the above dynamics as \\[\\begin{align} \\dot{x} &amp;= A x + B(u,Cx) \\\\ \\dot{e} &amp; = (A - KC) e \\end{align}\\] Denoting \\(z = [x,e]^T\\), we have the augmented dynamics \\[ \\dot{z} = \\underbrace{\\begin{bmatrix} A &amp; 0 \\\\ 0 &amp; A - KC \\end{bmatrix}}_{=:F} z + \\underbrace{\\begin{bmatrix} B(u,Dz) \\\\ 0 \\end{bmatrix}}_{=:G(z,u)} \\] We want to stabilize the system at \\(z_0 = [\\pi,0,0,0]^T\\) (the upright position) subject to control bounds \\(u \\in \\mathbb{U} = [-u_{\\max},u_{\\max}]\\). We need to find a control Lyapunov function (CLF), \\(V(z)\\), that satisfies the following constraints: \\[ V(z_0) = 0 \\] \\[ V(z) &gt; 0 \\quad \\forall z \\in \\{z: V(z) &lt; \\rho, z \\neq z_0 \\} \\] \\[ \\inf_{u \\in \\mathbb{U}} [L_F V(z) + L_G V(z)] \\leq 0 \\quad \\forall z \\in \\mathcal{Z} \\] where \\(L_F V\\) and \\(L_G V\\) are the Lie derivatives of \\(V\\) along \\(F\\) and \\(G(z,u)\\), respectively. \\(\\mathcal{Z}\\) is the set of all possible augmented states. The CLF will define the set of admissible control inputs \\(U\\). \\[ U = \\{ u: L_f V(z) + L_g V(z)u \\leq 0 \\} \\] To find the smallest-magnitude control input such that \\(u \\in K\\), we may use a quadratic program: \\[ \\min_{u \\in \\mathcal{U}} ||u||^2 \\] \\[ \\mathrm{s.t.} \\quad L_f V(z) + L_g V(z)u \\leq -c V(z) \\] where \\(c\\) is some positive constant. The challenge now is in choosing a suitable \\(V(z)\\). References "],["adaptivecontrol.html", "Chapter 6 Adaptive Control 6.1 Model-Reference Adaptive Control 6.2 Certainty-Equivalent Adaptive Control", " Chapter 6 Adaptive Control 6.1 Model-Reference Adaptive Control Basic flow for designing an adaptive controller Design a control law with variable parameters Design an adaptation law for adjusting the control parameters Analyze the convergence of the closed-loop system The control law design at the first step typically requires the designer to know what a good controller is if the true parameters were actually known, e.g., from feedback linearization (Appendix D), sliding control (Appendix E) etc. The design of the adaptation law typically comes from analyzing the dynamics of the tracking error, which as we will see often appears in the form of Lemma 6.1. The convergence of the closed-loop system is usually analyzed with the help of a Lyapunov-like function introduced in Chapter 4. Lemma 6.1 (Basic Lemma) Let two signals \\(e(t)\\) and \\(\\phi(t)\\) be related by \\[\\begin{equation} e(t) = H(p)[k \\phi(t)^T v(t)] \\tag{6.1} \\end{equation}\\] where \\(e(t)\\) a scalar output signal, \\(H(p)\\) a strictly positive real (SPR) transfer function, \\(k\\) an unknown real number with known sign, \\(\\phi(t) \\in \\mathbb{R}^m\\) a control signal, and \\(v(t) \\in \\mathbb{R}^m\\) a measurable input signal. If the control signal \\(\\phi(t)\\) satisfies \\[\\begin{equation} \\dot{\\phi}(t) = - \\mathrm{sgn}(k) \\gamma e(t) v(t) \\tag{6.2} \\end{equation}\\] with \\(\\gamma &gt; 0\\) a positive constant, then \\(e(t)\\) and \\(\\phi(t)\\) are globally bounded. Moreover, if \\(v(t)\\) is bounded, then \\[ \\lim_{t \\rightarrow \\infty} e(t) = 0. \\] Proof. Let the state-space representation of (6.1) be \\[\\begin{equation} \\dot{x} = A x + b [k \\phi^T v], \\quad e = c^T x. \\tag{6.3} \\end{equation}\\] Since \\(H(p)\\) is SPR, it follows from the Kalman-Yakubovich Lemma C.1 that there exist \\(P,Q \\succ 0\\) such that \\[ A^T P + P A = -Q, \\quad Pb = c. \\] Let \\[ V(x,\\phi) = x^T P x + \\frac{|k|}{\\gamma} \\phi^T \\phi, \\] clearly \\(V\\) is positive definite (i.e., \\(V(0,0)=0\\), and \\(V(x,\\phi) &gt; 0\\) for all \\(x \\neq 0, \\phi \\neq 0\\)). The time derivative of \\(V\\) along the trajectory defined by (6.3) with \\(\\phi\\) chosen as in (6.2) is \\[\\begin{align} \\dot{V} &amp; = \\frac{\\partial V}{\\partial x} \\dot{x} + \\frac{\\partial V}{\\partial \\phi} \\dot{\\phi} \\\\ &amp;= x^T (PA + A^T P) x + 2 x^T P b (k \\phi^T v) + \\frac{2|k|}{\\gamma} \\phi^T (- \\mathrm{sgn}(k) \\gamma e v) \\\\ &amp; = - x^T Q x + 2 (x^T c)(k\\phi^T v) - 2 \\phi^T (e v) \\\\ &amp; = - x^T Q x \\leq 0. \\end{align}\\] As a result, we know \\(x\\) and \\(\\phi\\) must be bounded (\\(V(x(t),\\phi(t)) \\leq V(x(0),\\phi(0))\\) is bounded). Since \\(e = c^T x\\), we know \\(e\\) must be bounded as well. If the input signal \\(v\\) is also bounded, then \\(\\dot{x}\\) is bounded as seen from (6.3). Because \\(\\ddot{V} = -2x^T Q \\dot{x}\\) is now bounded, we know \\(\\dot{V}\\) is uniformly continuous. Therefore, by Barbalat’s stability certificate (Theorem 4.6), we know \\(\\dot{V}\\) tends to zero as \\(t\\) tends to infinity, which implies \\(\\lim_{t \\rightarrow \\infty} x(t) = 0\\) and hence \\(\\lim_{t \\rightarrow \\infty} e(t) = 0\\). 6.1.1 First-Order Systems Consider the first-order single-input single-output (SISO) system \\[\\begin{equation} \\dot{x} = - a x + b u \\tag{6.4} \\end{equation}\\] where \\(a\\) and \\(b\\) are unknown groundtruth parameters. However, we do assume that the sign of \\(b\\) is known. What if the sign of \\(b\\) is unknown too? Let \\(r(t)\\) be a reference trajectory, e.g., a step function or a sinusoidal function, and \\(x_d(t)\\) be a desired system trajectory that tracks the reference \\[\\begin{equation} \\dot{x}_d = - a_d x_d + b_d r(t), \\tag{6.5} \\end{equation}\\] where \\(a_d,b_d &gt; 0\\) are user-defined constants. Note that the transfer function from \\(r\\) to \\(x_d\\) is \\[ x_d = \\frac{b_d}{p + a_d} r \\] and the system is stable. Review basics of transfer function. The goal of adaptive control is to design a control law and an adaptation law such that the tracking error of the system \\(x(t) - x_d(t)\\) converges to zero. Control law. We design the control law as \\[\\begin{equation} u = \\hat{a}_r(t) r + \\hat{a}_x(t) x \\tag{6.6} \\end{equation}\\] where \\(\\hat{a}_r(t)\\) and \\(\\hat{a}_x(t)\\) are time-varying feedback gains that we wish to adapt. The closed-loop dynamics of system (6.4) with the controller (6.6) is \\[ \\dot{x} = - a x + b (\\hat{a}_r r + \\hat{a}_x x) = - (a - b \\hat{a}_x) x + b \\hat{a}_r r. \\] With the equation above, the reason for choosing the control law (6.6) is clear: if the system parameters \\((a,b)\\) were known, then choosing \\[\\begin{equation} a_r^\\star = \\frac{b_d}{b}, \\quad a_x^\\star = \\frac{a - a_d}{b} \\tag{6.7} \\end{equation}\\] leads to the closed-loop dynamics \\(\\dot{x} = - a_d x + b_d r\\) that is exactly what we want in (6.5). However, in adaptive control, since the true parameters \\((a,b)\\) are not revealed to the control designer, an adaptation law is needed to dynamically adjust the gains \\(\\hat{a}_r\\) and \\(\\hat{a}_x\\) based on the tracking error \\(x(t) - x_d(t)\\). Adaptation law. Let \\(e(t) = x(t) - x_d(t)\\) be the tracking error, and we develop its time derivative \\[\\begin{align} \\dot{e} &amp;= \\dot{x} - \\dot{x}_d \\\\ &amp;= - a_d (x - x_d) + (a_d - a + b\\hat{a}_x)x + (b \\hat{a}_r - b_d) r \\\\ &amp; = - a_d e + b\\underbrace{(\\hat{a}_x - \\hat{a}_x^\\star)}_{=:\\tilde{a}_x} x + b \\underbrace{(\\hat{a}_r - \\hat{a}_r^\\star )}_{=:\\tilde{a}_r} r \\\\ &amp; = - a_d e + b (\\tilde{a}_x x + \\tilde{a}_r r) \\tag{6.8} \\end{align}\\] where \\(\\tilde{a}_x\\) and \\(\\tilde{a}_r\\) are the gain errors w.r.t. the optimal gains in (6.7) if the true parameters were known. The error dynamics (6.8) is equivalent to the following transfer function \\[\\begin{equation} e = \\frac{1}{p + a_d} b(\\tilde{a}_x x + \\tilde{a}_r r) = \\frac{1}{p + a_d} \\left(b \\begin{bmatrix} \\tilde{a}_x \\\\ \\tilde{a}_r \\end{bmatrix}^T \\begin{bmatrix} x \\\\ r \\end{bmatrix} \\right), \\tag{6.9} \\end{equation}\\] which is in the form of (6.1). Therefore, we choose the adaptation law \\[\\begin{equation} \\begin{bmatrix} \\dot{\\tilde{a}}_x \\\\ \\dot{\\tilde{a}}_r \\end{bmatrix} = - \\mathrm{sgn}(b) \\gamma e \\begin{bmatrix} x \\\\ r \\end{bmatrix}. \\tag{6.10} \\end{equation}\\] Tracking convergence. With the control law (6.6) and the adaptation law (6.10), we can prove that the tracking error converges to zero, using Lemma 6.1. With \\(\\tilde{a}=[\\tilde{a}_x, \\tilde{a}_r]^T\\), let \\[\\begin{equation} V(e,\\tilde{a}) = e^2 + \\frac{|b|}{\\gamma} \\tilde{a}^T \\tilde{a} \\tag{6.11} \\end{equation}\\] be a positive definite Lyapunov function candidate with time derivative \\[ \\dot{V} = - 2a_d e^2 \\leq 0. \\] Clearly, \\(e\\) and \\(\\tilde{a}\\) are both bounded. Assuming the reference trajectory \\(r\\) is bounded, we know \\(x_d\\) is bounded (due to (6.5)) and hence \\(x\\) is bounded (due to \\(e = x - x_d\\) being bounded). Consequently, from the error dynamics (6.8) we know \\(\\dot{e}\\) is bounded, which implies \\(\\ddot{V} = -4a_d e \\dot{e}\\) is bounded and \\(\\dot{V}\\) is uniformly continuous. By Barbalat’s stability certificate 4.6, we conlude \\(e(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\). It is always better to combine mathematical analysis with intuitive understanding. Can you explain intuitively why the adaptation law (6.10) makes sense? (Hint: think about how the control should react to a negative/positive tracking error.) Parameter convergence. We have shown the control law (6.6) and the adaptation law (6.10) guarantee to track the reference trajectory. However, is it guaranteed that the gains of the controller (6.6) also converge to the optimal gains in (6.7)? We will now show that the answer is indefinite and it depends on the reference trajectory \\(r(t)\\). Because the tracking error \\(e\\) converges to zero, and \\(e\\) is the output of a stable filter (6.9), we know the input \\(b(\\tilde{a}_x x + \\tilde{a}_r r)\\) must also converge to zero. On the other hand, the adaptation law (6.10) shows that both \\(\\dot{\\tilde{a}}_x\\) and \\(\\dot{\\tilde{a}}_r\\) converge to zero (due to \\(e\\) converging to zero and \\(x\\), \\(r\\) being bounded). As a result, we know \\(\\tilde{a} = [\\tilde{a}_x,\\tilde{a}_r]^T\\) converges to a constant that satisfies \\[\\begin{equation} v^T \\tilde{a} = 0, \\quad v = \\begin{bmatrix} x \\\\ r \\end{bmatrix}, \\tag{6.12} \\end{equation}\\] which is a single linear equation of \\(\\tilde{a}\\) with time-varying coeffients. Constant reference: no guaranteed convergence. Suppose \\(r(t) \\equiv r_0 \\neq 0\\) for all \\(t\\). From (6.5) we know \\(x = x_d = \\alpha r_0\\) when \\(t \\rightarrow \\infty\\), where \\(\\alpha\\) is the constant DC gain of the stable filter. Therefore, the linear equation (6.12) reduces to \\[ \\alpha \\tilde{a}_x + \\tilde{a}_r = 0. \\] This implies that \\(\\tilde{a}\\) does not necessarily converge to zero. In fact, it converges to a straight line in the parameter space. Persistent excitation: guaranteed convergence. However, when the signal \\(v\\) satisfies the so-called persistent excitation condition, which states that for any \\(t\\), there exists \\(T, \\beta &gt; 0\\) such that \\[\\begin{equation} \\int_{t}^{t+T} v v^T d\\tau \\geq \\beta I, \\tag{6.13} \\end{equation}\\] then \\(\\tilde{a}\\) is guaranteed to converge to zero. To see this, we multiply (6.12) by \\(v\\) and integrate it from \\(t\\) to \\(t+T\\), which gives rise to \\[ \\left( \\int_{t}^{t+T} vv^T d\\tau \\right) \\tilde{a} = 0. \\] By the persistent excitation condition (6.13), we infer that \\(\\tilde{a} = 0\\) is the only solution. It remains to understand under what conditions of the reference trajectory \\(r(t)\\) can we guarantee the persistent excitation of \\(v\\). We leave it as an exercise for the reader to show, if \\(r(t)\\) contains at least one sinusoidal component, then the persistent excitation condition of \\(v\\) is guaranteed. Exercise 6.1 (Extension to Nonlinear Systems) Design a control law and an adaptation law for the following system \\[ \\dot{x} = - a x - c f(x) + b u \\] with unknown true parameters \\((a,b,c)\\) (assume the sign of \\(b\\) is known) and known nonlinearity \\(f(x)\\) to track a reference trajectory \\(r(t)\\). Analyze the convergence of tracking error and parameter estimation error. 6.1.2 High-Order Systems Consider an \\(n\\)-th order nonlinear system \\[\\begin{equation} q^{(n)} + \\sum_{i=1}^n \\alpha_i f_i(x,t) = bu \\tag{6.14} \\end{equation}\\] where \\(x=[q,\\dot{q},\\ddot{q},\\dots,q^{(n-1)}]^T\\) is the state of the system, \\(f_i\\)’s are known nonlinearities, \\((\\alpha_1,\\dots,\\alpha_n,b)\\) are unknown parameters of the system (with \\(\\mathrm{sgn}(b)\\) known). The goal of adaptive control is to control the system (6.14) trajectory to follow a desired trajectory \\(q_d(t)\\) despite no knowing the true parameters. To facilitate the derivation of the adaptive controller, let us divide both sides of (6.14) by \\(b\\) \\[\\begin{equation} h q^{(n)} + \\sum_{i=1}^n a_i f_i(x,t) = u \\tag{6.15} \\end{equation}\\] where \\(h = 1 / b\\) and \\(a_i = \\alpha_i / b\\). Control law. Recall that the choice of the control law is tyically inspired by the control design if the true system parameters were known. We will borrow ideas from sliding control (Appendix E). Known parameters. Let \\(e = q(t) - q_d(t)\\) be the tracking error, and define the following combined error \\[ s = e^{(n-1)} + \\lambda_{n-2} e^{(n-2)} + \\dots + \\lambda_0 e = \\Delta(p) e \\] where \\(\\Delta(p) = p^{n-1} + \\lambda_{n-2} p^{(n-2)} + \\dots + \\lambda_0\\) is a stable polynomial with user-chosen coeffients \\(\\lambda_0,\\dots,\\lambda_{n-2}\\). The rationale for defining the combined error \\(s\\) is that the convergence of \\(e\\) to zero can be guaranteed by the convergence of \\(s\\) to zero (when \\(\\Delta(p)\\) is stable). Note that \\(s\\) can be equivalently written as \\[\\begin{align} s &amp; = (q^{(n-1)} - q_d^{(n-1)}) + \\lambda_{n-2} e^{(n-2)} + \\dots + \\lambda_0 e \\\\ &amp; = q^{(n-1)} - \\underbrace{ \\left( q_d^{(n-1)} - \\lambda_{n-2} e^{(n-2)} - \\dots - \\lambda_0 e \\right) }_{q_r^{(n-1)}}. \\end{align}\\] Now consider the control law \\[\\begin{equation} u = h q_r^{(n)} - ks + \\sum_{i=1}^n a_i f_i(x,t) \\tag{6.16} \\end{equation}\\] where \\[ q_r^{(n)} = q_d^{(n)} - \\lambda_{n-2} e^{(n-1)} - \\dots - \\lambda_0 \\dot{e} \\] and \\(k\\) is a design constant that has the same sign as \\(h\\). This choice of control, plugged into the system dynamics (6.15), leads to \\[\\begin{align} h q^{(n)} + \\sum_{i=1}^n a_i f_i(x,t) = h q_r^{(n)} - ks + \\sum_{i=1}^n a_i f_i(x,t) \\Longleftrightarrow \\\\ h \\left( q^{(n)} - q_r^{(n)} \\right) + ks = 0 \\Longleftrightarrow \\\\ h \\dot{s} + ks = 0, \\end{align}\\] which guarantees the exponential convergence of \\(s\\) to zero (note that \\(h\\) and \\(k\\) have the same sign), and hence the convergence of \\(e\\) to zero. Unknown parameters. Inspired by the control law with known parameters in (6.16), we design the adapative control law as \\[\\begin{equation} u = \\hat{h} q_r^{(n)} - ks + \\sum_{i=1}^n \\hat{a}_i f_i(x,t), \\tag{6.17} \\end{equation}\\] where the time-varying gains \\(\\hat{h},\\hat{a}_1,\\dots,\\hat{a}_n\\) will be adjusted by an adaptation law. Adaptation law. Inserting the adapative control law (6.17) into the system dynamics (6.15), we obtain \\[\\begin{align} h \\dot{s} + ks = \\tilde{h} q_r^{(n)} + \\sum_{i=1}^n \\tilde{a}_i f_i (x,t) \\Longleftrightarrow \\\\ s = \\frac{1}{p + k/h} \\frac{1}{h} \\underbrace{ \\left( \\begin{bmatrix} \\tilde{h} \\\\ \\tilde{a}_1 \\\\ \\vdots \\\\ \\tilde{a}_n \\end{bmatrix}^T \\begin{bmatrix} q_r^{(n)} \\\\ f_1(x,t) \\\\ \\vdots \\\\ f_n(x,t) \\end{bmatrix} \\right)}_{=:\\phi^T v} \\tag{6.18} \\end{align}\\] where \\(\\tilde{h} = \\hat{h} - h\\) and \\(\\tilde{a}_i = \\hat{a}_i - a_i,i=1,\\dots,n\\). Again, (6.18) is in the familiar form of (6.1), which naturally leads to the following adaptation law with \\(\\gamma &gt; 0\\) a chosen constant \\[\\begin{equation} \\dot{\\phi} = \\begin{bmatrix} \\dot{\\tilde{h}} \\\\ \\dot{\\tilde{a}}_1 \\\\ \\vdots \\\\ \\dot{\\tilde{a}}_n \\end{bmatrix} = - \\mathrm{sgn}(h) \\gamma s \\begin{bmatrix} q_r^{(n)} \\\\ f_1(x,t) \\\\ \\vdots \\\\ f_n(x,t) \\end{bmatrix}. \\tag{6.19} \\end{equation}\\] Tracking and parameter convergence. With the following Lyapunov function \\[\\begin{equation} V(s,\\phi) = |h| s^2 + \\frac{1}{\\gamma} \\phi^T \\phi, \\quad \\dot{V}(s,\\phi) -2|k| s^2, \\tag{6.20} \\end{equation}\\] the global convergence of \\(s\\) to zero can be easily shown. For parameter convergence, it is easy to see that when \\(v\\) satisfies the persistent excitation condition, we have that \\(\\phi\\) converges to zero. (However, the relationship between the reference trajectory \\(q_d(t)\\) and the persistent excitation of \\(v\\) becomes nontrivial due to the nonlinearities \\(f_i\\).) 6.1.3 Robotic Manipulator So far our focus has been on systems with a single input (\\(u \\in \\mathbb{R}\\)). In the following, we will show that similar techniques can be applied to adapative control of systems with multiple inputs, particularly, trajectory control of a robotic manipulator. Let \\(q \\in \\mathbb{R}^n\\) be the joint angles of a multi-link robotic arm, and \\(\\dot{q} \\in \\mathbb{R}^n\\) be the joint velocities. The dynamics of a robotic manipulator reads \\[\\begin{equation} H(q) \\ddot{q} + C(q,\\dot{q})\\dot{q} + g(q) = \\tau, \\tag{6.21} \\end{equation}\\] where \\(H(q) \\in \\mathbb{S}^{n}_{++}\\) is the manipulator inertia matrix (that is positive definite), \\(C(q,\\dot{q})\\dot{q}\\) is a vector of centripetal and Coriolis torques (with \\(C(q,\\dot{q}) \\in \\mathbb{R}^{n \\times n}\\)), and \\(g(q)\\) denotes gravitational torques. Figure 6.1: Planar two-link manipulator Example 6.1 (Planar Two-link Manipulator) The dynamics of a planar two-link manipulator in Fig. 6.1 is \\[\\begin{equation} \\begin{bmatrix} H_{11} &amp; H_{12} \\\\ H_{21} &amp; H_{22} \\end{bmatrix} \\begin{bmatrix} \\ddot{q}_1 \\\\ \\ddot{q}_2 \\end{bmatrix} + \\begin{bmatrix} - h \\dot{q}_2 &amp; -h (\\dot{q}_1 + \\dot{q}_2) \\\\ h \\dot{q}_1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\dot{q}_1 \\\\ \\dot{q}_2 \\end{bmatrix} = \\begin{bmatrix} \\tau_1 \\\\ \\tau_2 \\end{bmatrix}, \\tag{6.22} \\end{equation}\\] where \\[\\begin{align} H_{11} &amp; = a_1 + 2 a_3 \\cos q_2 + 2 a_4 \\sin q_2 \\\\ H_{12} &amp; = H_{21} = a_2 + a_3 \\cos q_2 + a_4 \\sin q_2 \\\\ H_{22} &amp;= a_2 \\\\ h &amp;= a_3 \\sin q_2 - a_4 \\cos q_2 \\end{align}\\] with \\[\\begin{align} a_1 &amp;= I_1 + m_1 l_{c1}^2 + I_e + m_e l_{ce}^2 + m_e l_1^2 \\\\ a_2 &amp;= I_e + m_e l_{ce}^2 \\\\ a_3 &amp;= m_e l_1 l_{ce} \\cos \\delta_e \\\\ a_4 &amp;= m_e l_1 l_{ce} \\sin \\delta_e. \\end{align}\\] As seen from the above example, the parameters \\(a\\) (which are nonlinear functions of the physical parameters such as mass and length) enter linearly in \\(H\\) and \\(C\\) (\\(g(q)\\) is ignored because the manipulator is on a horizontal plane). The goal of the control design is to have the manipulator track a desired trajectory \\(q_d(t)\\). Known parameters. When the parameters are known, we follow the sliding control design framework. Let \\(\\tilde{q} = q(t) - q_d(t)\\) be the tracking error, and define the combined error \\[ s = \\dot{\\tilde{q}} + \\Lambda \\tilde{q} = \\dot{q} - \\underbrace{\\left( \\dot{q}_d - \\Lambda \\tilde{q} \\right)}_{\\dot{q}_r} \\] where \\(\\Lambda \\in \\mathbb{S}^n_{++}\\) is a user-chosen positive definite matrix (in general we want \\(-\\Lambda\\) to be Hurwitz). In this case, \\(s \\rightarrow 0\\) implies \\(\\tilde{q} \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\). Choosing the control law (coming from feedback linearization Appendix D) \\[\\begin{equation} \\tau = H \\ddot{q}_r - K_D s + C \\dot{q} + g(q) \\tag{6.23} \\end{equation}\\] with \\(K_D \\in \\mathbb{S}^n_{++}\\) positive definite leads to the closed-loop dynamics \\[ H \\dot{s} + K_D s = 0 \\Longleftrightarrow \\dot{s} = - H^{-1} K_D s. \\] Because the matrix \\(H^{-1} K_D\\) is the product of two positive definite matrices (recall \\(H\\) is positive definite and so is \\(H^{-1}\\)), it has strictly positive real eigenvalues.11 Hence, \\(- H^{-1} K_D\\) is Hurwitz and \\(s\\) is guaranteed to converge to zero. Control law. A closer look at the controller (6.23) allows us to write it in the following form \\[\\begin{align} \\tau &amp;= H \\ddot{q}_r + C(s + \\dot{q}_r) + g(q) - K_D s \\\\ &amp;= H \\ddot{q}_r + C \\dot{q}_r + g(q) + (C - K_D) s \\\\ &amp;= Y (q,\\dot{q},\\dot{q}_r,\\ddot{q}_r) a + (C - K_D) s \\end{align}\\] where \\(a \\in \\mathbb{R}^m\\) contains all the parameters and \\(Y \\in \\mathbb{R}^{n \\times m}\\) is the matrix that collects all the coeffients of \\(a\\) in \\(H \\ddot{q}_r + C \\dot{q}_r + g(q)\\). As a result, we design the adapative control law to be \\[\\begin{equation} \\tau = Y \\hat{a} - K_D s, \\tag{6.24} \\end{equation}\\] with \\(\\hat{a}\\) the time-varying parameter that we wish to adapt. Note that here we have done something strange: the adapative control law does not exactly follow the controller (6.23) in the known-parameter case.12 We first separated \\(s\\) from \\(\\dot{q}\\) and wrote \\(Ya = H \\ddot{q}_r + C \\dot{q}_r + g\\) instead of \\(Ya = H \\ddot{q}_r + C \\dot{q} + g\\); then we dropped the “\\(C\\)” matrix in front of \\(s\\) in the adapative control law. The reason for doing this will soon become clear when we analyze the tracking convergence. Adaptation law and tracking convergence. Recall that the key of adapative control is to design a control law and an adaptation law such that global converge of the tracking error \\(s\\) can be guaranteed by a Lyapunov function. Looking at the previous Lyapunov functions in (6.11) and (6.20), we see that they both contain a positive definite term in the tracking error \\(s\\) (or \\(e\\) if in first-order systems) and another positive definite term in the parameter error \\(\\tilde{a}\\). This hints us that we may try a Lyapunov candidate function of the following form \\[\\begin{equation} V = \\frac{1}{2} \\left( s^T H s + \\tilde{a} \\Gamma^{-1} \\tilde{a} \\right), \\tag{6.25} \\end{equation}\\] where \\(\\Gamma \\in \\mathbb{S}^m_{++}\\) is a constant positive definite matrix, and \\(\\tilde{a} = \\hat{a} - a\\) is the parameter error. The next step would be to derive the time derivative of \\(V\\), which, as we can expect, will contain a term that involves \\(\\dot{H}\\) and complicates our analysis. Fortunately, the following lemma will help us. Lemma 6.2 For the manipulator dynamics (6.21), there exists a way to define \\(C\\) such that \\(\\dot{H} - 2C\\) is skew-symmetric. Proof. See Section 9.1, page 399-402 in (Slotine, Li, et al. 1991). You should also check if this is true for the planar two-link manipulator dynamics in Example 6.1. With Lemma 6.2, the time derivative of \\(V\\) in (6.25) reads \\[\\begin{align} \\dot{V} &amp; = s^T H \\dot{s} + \\frac{1}{2} s^T \\dot{H} s + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} \\\\ &amp;= s^T (H \\ddot{q} - H \\ddot{q}_r) + \\frac{1}{2} s^T \\dot{H} s + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} \\\\ &amp;= s^T (\\tau - C \\dot{q} - g - H \\ddot{q}_r ) + \\frac{1}{2} s^T \\dot{H} s + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} \\tag{6.26} \\\\ &amp;= s^T (\\tau - H \\ddot{q}_r - C (s + \\dot{q}_r) - g) + \\frac{1}{2} s^T \\dot{H} s + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} \\\\ &amp;= s^T (\\tau - H \\ddot{q}_r - C \\dot{q}_r - g) + \\frac{1}{2} s^T (\\dot{H}- 2C)s + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} \\tag{6.27}\\\\ &amp;= s^T (\\tau - H \\ddot{q}_r - C \\dot{q}_r - g) + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} \\\\ &amp;=s^T(Y\\hat{a} - K_D s - Ya) + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} \\tag{6.28}\\\\ &amp;=s^T Y \\tilde{a} + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} - s^T K_D s \\tag{6.29}, \\end{align}\\] where we used the manipulator dynamics (6.21) to rewrite \\(H\\ddot{q}\\) in (6.26), used \\(\\dot{H} - 2C\\) is skew-symmetric in (6.27), invoked the adapative control law (6.24) and reused \\(Ya = H \\ddot{q}_r + C \\dot{q}_r + g(q)\\) in (6.28). The derivation above explains why the choice of the control law in (6.24) did not exactly follow its counterpart when the parameters are known: we need to use \\(s^T Cs\\) to cancel \\(\\frac{1}{2} s^T \\dot{H} s\\) in (6.27). We then wonder if we can design \\(\\dot{\\tilde{a}}\\) such that \\(\\dot{V}\\) in (6.29) is negative semidefinie? This turns out to be straightforward with the adaptation law \\[\\begin{equation} \\dot{\\tilde{a}} = -\\Gamma Y^T s, \\tag{6.30} \\end{equation}\\] to make \\(s^T Y \\tilde{a} + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}}\\) vanish and so \\[ \\dot{V} = - s^T K_D s \\leq 0. \\] We are not done yet. To show \\(s\\) converges to zero (which is implied by \\(\\dot{V}\\) converges to zero), by Barbalat’s stability certificate 4.6, it suffices to show \\[ \\ddot{V} = -2 s^T K_D \\dot{s} \\] is bounded. We already know \\(s\\) and \\(\\tilde{a}\\) are bounded, due to the fact that \\(V\\) in (6.25) is bounded. Therefore, we only need to show \\(\\dot{s}\\) is bounded. To do so, we plug the adapative control law (6.24) into the manipulator dynamics (6.21) and obtain \\[ H \\dot{s} + (C + K_D) s = Y\\tilde{a}, \\] which implies the boundedness of \\(\\dot{s}\\) (note that \\(H\\) is uniformly positive definite, i.e., \\(H \\succeq \\alpha I\\) for some \\(\\alpha &gt; 0\\)). This concludes the analysis of the tracking convergence \\(s \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\). 6.2 Certainty-Equivalent Adaptive Control References "],["app-lti-system-theory.html", "A Linear System Theory A.1 Stability A.2 Controllability and Observability A.3 Stabilizability And Detectability", " A Linear System Theory Thanks to Shucheng Kang for writing this Appendix. A.1 Stability A.1.1 Continuous-Time Stability Consider the continuous-time linear time-invariant (LTI) system \\[\\begin{equation} \\dot{x} = A x. \\tag{A.1} \\end{equation}\\] Definition A.1 (Asymptotic and Marginal Stability) The LTI system (A.1) is “asymptotically stable” if \\(x(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\) for every initial condition \\(x_0\\) “marginally stable” if \\(x(t) \\nrightarrow 0\\) but remains bounded as \\(t \\rightarrow \\infty\\) for every initial condition \\(x_0\\) “stable” if it is either asymptotically or marginally stable “unstable” if it is not stable One can show that \\(A\\)’s eigenvalues determine the LTI system’s stability, as the following Theorem states: Theorem A.1 (Criteria for Asymptotic and Marginal Stability) The LTI system (A.1) is asymptotically stable if \\(\\text{Re} (\\lambda_i) &lt; 0\\) for all \\(i\\) marginally stable if \\(\\text{Re} (\\lambda_i) \\le 0\\) for all \\(i\\) and there exists at least one \\(i\\) for which \\(\\text{Re} (\\lambda_i) = 0\\) stable if \\(\\text{Re} (\\lambda_i) \\le 0\\) for all \\(i\\) unstable if \\(\\text{Re} (\\lambda_i) &gt; 0\\) for at least one \\(i\\) Proof. Here we only represent the proof of (1). Similar procedure can be adopted for the proof of (2) - (4). Suppose \\(A \\in \\mathbb{C}^{n \\times n}\\). There exists a similarity transformation matrix \\(V := \\begin{bmatrix} v_1 &amp; \\dots &amp; v_n \\end{bmatrix}\\), s.t. \\(A = V J V^{-1}\\), where \\(J\\) is the Jordan form of \\(A\\) (Please note that \\(A\\) may be non-diagonalizable so eigenvalue decomposition will fail in general.) Since \\(\\left\\{v_1 \\dots v_n\\right\\}\\) spans \\(\\mathbb{C}^n\\), we can denote the state \\(x(t)\\) as \\(V a(t)\\). Therefore, under the coordinate transformation \\(V^{-1}\\), \\[\\begin{equation*} \\dot{x}(t) = A x(t) \\Longrightarrow \\dot{a}(t) = J a(t) \\end{equation*}\\] Now suppose \\(J\\) has \\(I\\) Jordan blocks with corresponding block sizes \\(K_1, \\dots K_I\\) and eigenvalues \\(\\lambda_1, \\dots, \\lambda_I\\). Inside the \\(i\\)’s Jordan block, the differential equations are: \\[\\begin{equation*} \\begin{split} \\dot{a}_{i,1}(t) &amp; = \\lambda_i \\cdot a_{i,1}(t) \\\\ \\dot{a}_{i,2}(t) &amp; = a_{i,1}(t) + \\lambda_i \\cdot a_{i,2}(t) \\\\ \\vdots &amp; \\\\ \\dot{a}(t)_{i,K_i} &amp; = a_{i,K_i-1}(t) + \\lambda_i \\cdot a_{i,K_i}(t) \\end{split} \\end{equation*}\\] With Laplace transformation, we represent the above equations in the frequency domain: \\[\\begin{equation*} \\begin{split} s \\cdot A_{i,1}(s) - a_{i,1}(0) &amp; = \\lambda_i \\cdot A_{i,1}(s) \\\\ s \\cdot A_{i,2}(s) - a_{i,2}(0) &amp; = A_{i,1}(s) + \\lambda_i \\cdot A_{i,2}(s) \\\\ \\vdots &amp; \\\\ s \\cdot A_{i,K_i}(s) - a_{i,K_i}(0) &amp; = A_{i,K_i-1}(s) + \\lambda_i \\cdot A_{i,K_i}(s) \\end{split} \\end{equation*}\\] where \\(A_{i,k}(s)\\) corresponds to \\(a_{i,k}(t)\\)’s Laplace transformation. By solving the above algebraic equations, \\[\\begin{equation*} A_{i,k}(s) = \\sum_{j=1}^{k} \\frac{ a_{i, k+1-j}(0) }{ (s - \\lambda_i)^j }, \\quad \\forall k \\in \\left\\{1 \\dots K_i\\right\\} \\end{equation*}\\] Via the inverse Laplace transformation: \\[\\begin{equation*} a_{i,k}(t) = \\sum_{j=1}^{k} \\frac{ a_{i, k+1-j}(0) }{ (j-1)! } \\cdot e^{\\lambda_i t} t^{j-1}, \\quad \\forall k \\in \\left\\{1 \\dots K_i\\right\\} \\end{equation*}\\] Since \\(\\text{Re}(\\lambda_i) &lt; 0\\), no matter how we choose \\(a_{i,k}(0)\\), \\(a_{i,k}(t)\\) will always converge to \\(0\\). A.1.2 Discrete-Time Stability Consider the discrete-time linear time-invariant (LTI) system \\[\\begin{equation} x_{t+1} = A x_t. \\tag{A.2} \\end{equation}\\] Theorem A.2 (Stability of Discrete-Time LTI System) The discrete-time LTI system (A.2) is asymptotically stable if \\(|\\lambda_i| &lt; 1\\) for all \\(i\\) marginally stable if \\(|\\lambda_i| \\leq 1\\) for all \\(i\\) and there exists at least one \\(i\\) for which \\(|\\lambda_i| = 1\\) stable if \\(|\\lambda_i| \\leq 1\\) for all \\(i\\) unstable if \\(|\\lambda_i| &gt; 1\\) for at least one \\(i\\). Note that \\(|\\lambda_i| &lt; 1\\) means the eigenvalue lies strictly inside the unit circle in the complex plane. A.1.3 Lyapunov Analysis Theorem A.3 (Lyapunov Equation) The following is equivalent for a linear time-invariant system \\(\\dot{x} = A x\\) The system is globally asymptotically stable, i.e., \\(A\\) is Hurwitz and \\(\\lim_{t \\rightarrow \\infty} x(t) = 0\\) regardless of the initial condition; For any positive definite matrix \\(Q\\), the unique solution \\(P\\) to the Lyapunov equation \\[\\begin{equation} A^T P + P A = -Q \\tag{A.3} \\end{equation}\\] is positive definite. Proof. (a): \\(2 \\Rightarrow 1\\). Suppose we are given two positive definite matrices \\(P, Q \\succ 0\\) that satisfies the Lyapunov equation (A.3). Define a scalar function \\[ V(x) = x^T P x. \\] It is clear that \\(V &gt; 0\\) for any \\(x \\neq 0\\) and \\(V(x) = 0\\) (i.e., \\(V(x)\\) is positive definite). We also see \\(V(x)\\) is radially unbounded because: \\[ V(x) \\geq \\lambda_{\\min}(P) \\Vert x \\Vert^2 \\Rightarrow \\lim_{x \\rightarrow \\infty} V(x) \\rightarrow \\infty. \\] The time derivative of \\(V\\) reads \\[ \\dot{V} = 2 x^T P \\dot{x} = x^T (A^T P + P A) x = - x^T Q x. \\] Clearly, \\(\\dot{V} &lt; 0\\) for any \\(x \\neq 0\\) and \\(\\dot{V}(0) = 0\\). According to Lyapunov’s global stability theorem 4.3, we conclude the linear system \\(\\dot{x} = Ax\\) is globally asymptotically stable at \\(x = 0\\). (b): \\(1 \\Rightarrow 2\\). Suppose \\(A\\) is Hurwitz, we want to show that, for any \\(Q \\succ 0\\), there exists a unique \\(P \\succ 0\\) satisfying the Lyapunov equation (A.3). In fact, consider the matrix \\[ P = \\int_{t=0}^{\\infty} e^{A^T t} Q e^{At} dt. \\] Because \\(A\\) is Hurwitz, the integral exists, and clearly \\(P \\succ 0\\) due to \\(Q \\succ 0\\). To show this choice of \\(P\\) satisfies the Lyapunov equation, we write \\[\\begin{align} A^T P + P A &amp;= \\int_{t=0}^{\\infty} \\left( A^T e^{A^T t} Q e^{At} + e^{A^T t} Q e^{At} A \\right) dt \\\\ &amp;=\\int_{t=0}^{\\infty} d \\left( e^{A^T t} Q e^{At} \\right) \\\\ &amp; = e^{A^T t} Q e^{At}\\vert_{t = \\infty} - e^{A^T t} Q e^{At}\\vert_{t = 0} = - Q, \\end{align}\\] where the last equality holds because \\(e^{A \\infty} = 0\\) (recall \\(A\\) is Hurwitz). To show the uniqueness of \\(P\\), we assume that there exists another matrix \\(P&#39;\\) that also satisfies the Lyapunov equation. Therefore, \\[\\begin{align} P&#39; &amp;= e^{A^T t} P&#39; e^{At} \\vert_{t=0} - e^{A^T t} P&#39; e^{At} \\vert_{t=\\infty} \\\\ &amp;= - \\int_{t=0}^{\\infty} d \\left( e^{A^T t} P&#39; e^{At} \\right) \\\\ &amp;= - \\int_{t=0}^{\\infty} e^{A^T t} \\left( A^T P&#39; + P&#39; A \\right) e^{At} dt \\\\ &amp; = \\int_{t=0}^{\\infty} e^{A^T t} Q e^{At} dt = P, \\end{align}\\] leading to \\(P&#39; = P\\). Hence, the solution is unique. Convergence rate estimation. We now show that Theorem A.3 can allow us to quantify the convergence rate of a (stable) linear system towards zero. For a Hurwitz linear system \\(\\dot{x} = Ax\\), let us pick a positive definite matrix \\(Q\\). Theorem A.3 tells us we can find a unique \\(P \\succ 0\\) satisfying the Lyapunov equation (A.3). In this case, we can upper bound the scalar function \\(V = x^T P x\\) as \\[ V \\leq \\lambda_{\\max}(P) \\Vert x \\Vert^2. \\] The time derivative of \\(V\\) is \\(\\dot{V} = - x^T Q x\\), which can be upper bounded by \\[\\begin{align} \\dot{V} &amp; \\leq - \\lambda_{\\min} (Q) \\Vert x \\Vert^2 \\\\ &amp; = - \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max} (P)} \\underbrace{ \\left( \\lambda_{\\max} (P) \\Vert x \\Vert^2 \\right)}_{\\geq V} \\\\ &amp; \\leq - \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max} (P)} V. \\end{align}\\] Denoting \\(\\gamma(Q) = \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max}(P)}\\), the above inequality implies \\[ V(0) e^{-\\gamma(Q) t} \\geq V(t) = x^T P x \\geq \\lambda_{\\min}(P) \\Vert x \\Vert^2. \\] As a result, \\(\\Vert x \\Vert^2\\) converges to zero exponentially with a rate at least \\(\\gamma(Q)\\), and \\(\\Vert x \\Vert\\) converges to zero exponentially with a rate at least \\(\\gamma(Q) / 2\\). Best convergence rate estimation. I have used \\(\\gamma (Q)\\) to make it explict that the rate \\(\\gamma\\) depends on the choice of \\(Q\\), because \\(P\\) is computed from the Lyapunov equation as an implicit function of \\(Q\\). Naturally, choosing different \\(Q\\) will lead to different \\(\\gamma (Q)\\). So what is the choice of \\(Q\\) that maximizes the convergence rate estimation? Corollary A.1 (Maximum Convergence Rate Estimation) \\(Q = I\\) maximizes the convergence rate estimation. Proof. let us denote \\(P_0\\) as the solution to the Lyapunov equation with \\(Q = I\\) \\[ A^T P_0 + P_0 A = - I. \\] Let \\(P\\) be the solution corresponding to a different choice of \\(Q\\) \\[ A^T P + P A = - Q. \\] Without loss of generality, we can assume \\(\\lambda_{\\min}(Q) = 1\\), because rescaling \\(Q\\) will recale \\(P\\) by the same factor, which does not affect \\(\\gamma(Q)\\). Subtracting the two Lyapunov equations above we get \\[ A^T (P - P_0) + (P - P_0) A = - (Q - I). \\] Since \\(Q - I \\succeq 0\\) (due to \\(\\lambda_{\\min}(Q) = 1\\)), we know \\(P - P_0 \\succeq 0\\) and \\(\\lambda_{\\max} (P) \\geq \\lambda_{\\max} (P_0)\\). As a result, \\[ \\gamma(Q) = \\frac{\\lambda_{\\min}(Q)}{\\lambda_{\\max}(P)} = \\frac{\\lambda_{\\min}(I)}{\\lambda_{\\max}(P)} \\leq \\frac{\\lambda_{\\min}(I)}{\\lambda_{\\max}(P_0)} = \\gamma(I), \\] and \\(Q = I\\) maximizes the convergence rate estimation. A.2 Controllability and Observability Consider the following linear time-invariant (LTI) system \\[\\begin{equation} \\tag{A.4} \\begin{split} \\dot{x} = A x + B u \\\\ y = C x + D u \\end{split} \\end{equation}\\] where \\(x \\in \\mathbb{R}^n\\) the state, \\(u \\in \\mathbb{R}^m\\) the control input, \\(y \\in \\mathbb{R}^p\\) the output, and \\(A,B,C,D\\) are constant matrices with proper sizes. If we know the initial state \\(x(0)\\) and the control inputs \\(u(t)\\) over a period of time \\(t \\in [0, t_1]\\), the system trajectory \\((x(t), y(t))\\) can be determined as \\[\\begin{equation} \\tag{A.5} \\begin{split} x(t) &amp; = e^{At} x(0) + \\int_{0}^{t} e^{A(t-\\tau)} B u(\\tau) d\\tau \\\\ y(t) &amp; = C x(t) + D u(t) \\end{split} \\end{equation}\\] To study the internal structure of linear systems, two important properties should be considered: controllability and observability. In the following analysis, we will see that they are actually dual concepts. Their definitions (Chen 1984) are given below. Definition A.2 (Controllability) The LTI system (A.4), or the pair \\((A, B)\\), is controllable, if for any initial state \\(x(0) = x_0\\) and final state \\(x_f\\), there exists a sequence of control inputs that transfer the system from \\(x_0\\) to \\(x_f\\) in finite time. Definition A.3 (Observability) The LTI system (A.4), or the pair \\((C, A)\\), is observable, if for any unknown initial state \\(x(0)\\), there exists a finite time \\(t_1 &gt; 0\\), such that knowing \\(y\\) and \\(u\\) over \\([0, t_1]\\) suffices to determine \\(x(0)\\). Sometimes it will become more convenient for us to analyze the system (A.4) under another coordinate basis, i.e., \\(z = T x\\), where the coordinate transformation \\(T\\) is nonsingular (i.e., full-rank). Define \\(A&#39; = TAT^{-1}, B&#39; = PB, C&#39; = CT^{-1}, D&#39; = D\\), we get \\[\\begin{equation*} \\begin{split} \\dot{z} = A&#39; z + B&#39; u \\\\ y = C&#39; z + D&#39; u \\end{split} \\end{equation*}\\] Since the coordinate transformation only changes the system’s coordinate basis, physical properties like controllability and observability will not change. A.2.1 Cayley-Hamilton Theorem In the analysis of controllability and observability, Cayley Hamilton Theorem lays the foundation. The statement of the theory and its (elegant) proof are given blow. Some useful corollaries are also presented. Theorem A.4 (Cayley-Hamilton) Let \\(A \\in \\mathbb{C}^{n \\times n}\\) and denote the characteristic polynomial of \\(A\\) as \\[ \\text{det}(\\lambda I - A) = \\lambda^n + a_1 \\lambda^{n-1} + \\dots + a_n \\in \\mathbb{C}[\\lambda], \\] which is a polynomial in a single variable \\(\\lambda\\) with coefficients \\(a_1,\\dots,a_n\\). Then \\[ A^n + a_1 A^{n-1} + \\dots + a_n I = 0 \\] Proof. Define the adjugate of \\(\\lambda I - A\\) as \\[ B = \\text{adj}(\\lambda I - A) \\] From \\(B\\)’s definition, we have \\[\\begin{equation} (\\lambda I - A) B = \\text{det}(\\lambda I - A) I = (\\lambda^n + a_1 \\lambda^{n-1} + \\dots + a_n) I \\tag{A.6} \\end{equation}\\] Also, \\(B\\) is a polynomial matrix over \\(\\lambda\\), whose maximum degree is no more than \\(n - 1\\). Therefore, we write \\(B\\) as follows: \\[ B = \\sum_{i=0}^{n-1} \\lambda^i B_i \\] where \\(B_i\\)’s are constant matrices. In this way, we unfold \\((\\lambda I - A)B\\): \\[\\begin{equation} \\tag{A.7} \\begin{split} (\\lambda I - A) B &amp; = (\\lambda I - A) \\sum_{i=0}^{n-1} \\lambda^i B_i \\\\ &amp; = \\lambda^n B_{n-1} + \\sum_{i=1}^{n-1} \\lambda^i (-A B_i + B_{i-1}) - A B_0 \\end{split} \\end{equation}\\] Since \\(\\lambda\\) can be arbitrarily set, matching the coefficients of (A.6) and (A.7), we have \\[\\begin{equation*} \\begin{split} B_{n-1} &amp; = I \\\\ -A B_i + B_{i-1} &amp; = a_{n-i} I, \\quad i = 1 \\dots n - 1 \\\\ -A B_0 &amp; = a_n I \\end{split} \\end{equation*}\\] Thus, we have \\[\\begin{equation*} \\begin{split} &amp; B_{n-1} \\cdot A^n + \\sum_{i=1}^{n-1} (-A B_i + B_{i-1}) \\cdot A^i + (-A B_0) \\cdot I \\\\ = &amp; I \\cdot A^n + \\sum_{i=1}^{n-1} (a_{n-i} I) \\cdot A^i + (a_n I) \\cdot I \\\\ = &amp; A^n + a_1 A^{n-1} + a_2 A^{n-2} + \\dots + a_n I \\end{split} \\end{equation*}\\] On the other hand, one can easily check that \\[ B_{n-1} \\cdot A^n + \\sum_{i=1}^{n-1} (-A B_i + B_{i-1}) \\cdot A^i + (-A B_0) \\cdot I = 0 \\] since each term offsets completely. Therefore, \\[ A^n + a_1 A^{n-1} + a_2 A^{n-2} + \\dots + a_n I = 0, \\] concluding the proof. Here are some corollaries of the Cayley-Hamilton Theorem. Corollary A.2 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}, k \\ge n\\), \\(A^k B\\) is a linear combination of \\(B, AB, A^2B, \\dots, A^{n-1}B\\). Proof. Directly from Cayley Hamilton Theorem, \\(A^n\\) can be expressed as a linear combination of \\(I, A, A^2, \\dots, A^{n-1}\\). By recursion, it is easy to show that for all \\(m &gt; n\\), \\(A^m\\) is also a linear combination of \\(I, A, A^2, \\dots, A^{n-1}\\). Post-multiply both sides with \\(B\\), we get what we want. Corollary A.3 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}, k &gt; n\\), the following equality always holds: \\[ \\text{rank}(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix}) = \\text{rank}(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix}) \\] Proof. First prove LHS \\(\\le\\) RHS. \\(\\forall v \\in \\mathbb{C}^n\\) such that \\[ v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix} = v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1}B &amp; \\dots A^{k-1}B \\end{bmatrix} = 0 \\] \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\) must hold. Second prove LHS \\(\\ge\\) RHS. For any \\(v \\in \\mathbb{C}^n\\) such that \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\) and any \\(k &gt; n\\), by Corollary A.2, there exists a sequence \\(c_i, i = 0 \\dots n-1\\) satisfy the following: \\[ v^* A^k B = v^* \\sum_{i=0}^{n-1} c_i A^i B = 0 \\] Therefore, \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix} = 0\\). Corollary A.4 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}\\), define \\[ \\mathcal{C} = \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} \\] If \\(\\text{rank}(\\mathcal{C}) = k_1 &lt; n\\), there exist a similarity transformation \\(T\\) such that \\[ T A T^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, T B = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\] where \\(\\bar{A}_c \\in \\mathbb{C}^{k_1 \\times k_1}, \\bar{B}_c \\in \\mathbb{C}^{k_1 \\times m}\\). Moreover, the matrix \\[\\begin{equation*} \\bar{\\mathcal{C}} := \\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c \\end{bmatrix} \\end{equation*}\\] has full row rank. Proof. Since \\(\\mathcal{C}\\) is not full row rank, we pick \\(k_1\\) linearly independent columns from \\(\\mathcal{C}\\). Denote them as \\(q_1\\dots q_{k_1}\\), \\(q_i \\in \\mathbb{C}^n\\). Then, we arbitrarily set other \\(n-k_1\\) vectors \\(q_{k_1+1} \\dots q_{n}\\) as long as \\[ Q = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\] is invertible. Define the similarity transformation matrix by \\(T = Q^{-1}\\). Note that \\(A q_i\\) can be seen as a column picked from \\(A^{k} B, k \\in \\left\\{1 \\dots n\\right\\}\\), which is guaranteed to be a linear combination of \\(B, AB, \\dots, A^{n-1}B\\) from Cayley Hamilton Theorem. Thus, \\(A q_i\\) is bound to be a linear transformation of columns from \\(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = \\mathcal{C}\\). Since \\(q_1\\dots q_{k_1}\\) is the largest linearly independent column vector set from \\(\\mathcal{C}\\), this implies \\(A q_i\\) can be expressed as a linear combination of \\(q_1\\dots q_{k_1}\\): \\[\\begin{equation*} \\begin{split} A Q &amp; = A T^{-1} = A \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} = T^{-1} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} \\end{split} \\end{equation*}\\] Similarly, \\(B\\) itself is part of \\(\\mathcal{C}\\). Therefore, each column of \\(B\\) is naturally a linear combination of \\(q_1 \\dots q_{k_1}\\): \\[\\begin{equation*} \\begin{split} B = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{split} = T^{-1} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] To see \\(\\bar{\\mathcal{C}}\\) has full row rank, note that \\(\\text{rank} \\mathcal{C} = k_1\\) and \\[\\begin{equation*} \\mathcal{C} = T^{-1} \\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{n - 1} \\bar{B}_c \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; \\dots &amp; 0 \\end{bmatrix} \\end{equation*}\\] Thus, \\[\\text{rank}\\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{n - 1} \\bar{B}_c \\end{bmatrix} = k_1. \\] By Corollary A.3, \\(\\text{rank}\\bar{\\mathcal{C}} = k_1\\). The following Corollary is especially useful in the study of pole assignment in the single-input-multiple-output (SIMO) LTI system. Corollary A.5 For any \\(A \\in \\mathbb{C}^{n \\times n}, b \\in \\mathbb{C}^{n}\\), if \\[\\begin{equation*} \\mathcal{C} = \\begin{bmatrix} b &amp; Ab &amp; \\dots &amp; A^{n-1}b \\end{bmatrix} \\in \\mathbb{C}^{n \\times n} \\end{equation*}\\] has full rank, then there exists a similarity transformation \\(T\\) such that \\[\\begin{equation*} T A T^{-1} = A_1 := \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix}, \\quad T b = b_1 := \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation*}\\] where \\(a_1, \\dots, a_n\\) are the coefficients of \\(A\\)’s characteristic polynomial: \\[\\begin{equation*} \\det(A - \\lambda I) = \\lambda^{n} + a_1 \\lambda^{n-1} + \\dots + a_n \\lambda \\end{equation*}\\] Proof. Since \\(\\mathcal{C}\\) is invertible, define its inverse \\[\\begin{equation*} \\mathcal{C}^{-1} = \\begin{bmatrix} M_1 \\\\ M_2 \\\\ \\dots \\\\ M_n \\end{bmatrix} \\end{equation*}\\] where \\(M_i \\in \\mathbb{C}^{1 \\times n}\\). Then, \\[\\begin{equation*} I = \\mathcal{C}^{-1} \\mathcal{C} = \\begin{bmatrix} M_1 b &amp; M_1 Ab &amp; \\dots &amp; M_1 A^{n-1}b \\\\ M_2 b &amp; M_2 Ab &amp; \\dots &amp; M_2 A^{n-1}b \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ M_n b &amp; M_n Ab &amp; \\dots &amp; M_n A^{n-1}b \\end{bmatrix} \\Longrightarrow \\begin{cases} M_n A^{n-1}b = 1 \\\\ M_n A^ib = 0, \\ i = 0,\\dots, n-2 \\end{cases} \\end{equation*}\\] Now we claim that the transformation matrix \\(T\\) can be constructed as follows: \\[\\begin{equation*} T = \\begin{bmatrix} M_n A^{n-1} \\\\ M_n A^{n-2} \\\\ \\dots \\\\ M_n \\end{bmatrix} \\end{equation*}\\] We first show \\(T\\) is invertible by calculating \\(T \\mathcal{C}\\): \\[\\begin{equation*} T \\mathcal{C} = \\begin{bmatrix} M_n A^{n-1}b &amp; \\star &amp; \\dots &amp; \\star \\\\ M_n A^{n-2}b &amp; M_n A^{n-1}b &amp; \\dots &amp; \\star \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ M_n b &amp; M_n Ab &amp; \\dots &amp; M_n A^{n-1}b \\end{bmatrix} = \\begin{bmatrix} 1 &amp; \\star &amp; \\dots &amp; \\star \\\\ 0 &amp; 1 &amp; \\dots &amp; \\star \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 \\end{bmatrix} \\end{equation*}\\] Then we calculate \\(Tb\\) and \\(TA\\): \\[\\begin{equation*} \\begin{split} Tb &amp; = \\begin{bmatrix} M_n A^{n-1}b \\\\ M_n A^{n-2}b \\\\ \\vdots \\\\ M_n b \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\\\ T A &amp; = \\begin{bmatrix} M_n A^n \\\\ M_n A^{n-1} \\\\ \\vdots \\\\ M_n A \\end{bmatrix} = \\begin{bmatrix} -M_n \\cdot \\sum_{i=0}^{n-1} a_{n-i} A^i \\\\ M_n A^{n-1} \\\\ \\vdots \\\\ M_n A \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} M_n A^{n-1} \\\\ M_n A^{n-2} \\\\ \\vdots \\\\ M_n A \\\\ M_n \\end{bmatrix} = A_1 T \\end{split} \\end{equation*}\\] where the penultimate equality uses Cayley Hamilton Theorem. A.2.2 Equivalent Statements for Controllability There are a few equivalent statements to express an LTI system’s controllability that one should be familiar with: Theorem A.5 (Equivalent Statements for Controllability) The following statements are equivalent (Chen 1984), (Zhou, Doyle, and Glover 1996): \\((A, B)\\) is controllable. The matrix \\[ W_c(t) := \\int_{0}^{t} e^{A\\tau} B B^* e^{A^* \\tau} d\\tau \\] is positive definite for any \\(t &gt; 0\\). The controllability matrix \\[ \\mathcal{C} = \\begin{bmatrix} B &amp; AB &amp; A^2 B &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} \\] has full row rank. The matrix \\([A - \\lambda I, B]\\) has full row rank for all \\(\\lambda \\in \\mathbb{C}\\). Let \\(\\lambda\\) and \\(x\\) be any eigenvalue and any corresponding left eigenvector \\(A\\), i.e., \\(x^* A = x^* \\lambda\\), then \\(x^* B \\ne 0\\). The eigenvalues of \\(A+BF\\) can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs) by a suitable choice of \\(F\\). If, in addition, all eigenvalues of \\(A\\) have negative real parts, then the unique solution of \\[ A W_c + W_c A^* = -B B^* \\] is positive definite. The solution is called the controllability Gramian and can be expressed as \\[ W_c = \\int_{0}^{\\infty} e^{A \\tau} B B^* e^{A^* \\tau} d\\tau \\] Proof. (\\(1. \\Rightarrow 2.\\)) Prove by contradiction. Assume that \\((A, B)\\) is controllable but \\(W_c(t_1)\\) is singular for some \\(t_1 &gt; 0\\). This implies there exists a real vector \\(v \\ne 0 \\in \\mathbb{R}^n\\), s.t. \\[ v^* W_c(t_1) v = v^* (\\int_{0}^{t_1} e^{At} B B^* e^{A^*t} dt) v = \\int_{0}^{t_1} v^* (e^{At} B B^* e^{A^*t}) v \\ dt = 0 \\] Since \\(e^{At} BB^* e^{A^*t} \\succeq 0\\) for all \\(t\\), we must have \\[\\begin{equation*} \\begin{split} &amp; v^* (e^{At} B B^* e^{A^*t}) v = \\parallel v^* B e^{At} \\parallel^2 = 0, \\quad \\forall t \\in [0, t_1] \\\\ \\Longrightarrow &amp; v^* B e^{At} = 0, \\quad \\forall t \\in [0, t_1] \\end{split} \\end{equation*}\\] Setting \\(x(t_1) = 0\\), from (A.5), we have \\[ 0 = e^{A t_1} x(0) + \\int_{0}^{t_1} e^{A (t_1 - \\tau)} B u(\\tau) d\\tau = 0 \\] Pre-multiply the above equation by \\(v^*\\), then \\[ 0 = v^* e^{A t_1} x(0) \\] Since \\(x(0)\\) can be chosen arbitrarily, we set \\(x(0) = v e^{-A t_1}\\), which results in \\(v = 0\\). Contradiction! (\\(2. \\Rightarrow 1.\\)) For any \\(x(0) = x_0, t_1 &gt; 0, x(t_1) = x_1\\), since \\(W_c(t_1) \\succ 0\\), we set the control inputs as \\[ u(t) = -B^* e^{A^*(t_1 - t)} W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\] We claim that the picked \\(u(t)\\) satisfies (A.5) by \\[\\begin{equation*} \\begin{split} &amp; e^{At} x_0 + \\int_{0}^{t_1} e^{A(t_1-t)} B u(t) dt \\\\ &amp; = e^{At} x_0 - \\int_{0}^{t_1} e^{A(t_1-t)} B B^* e^{A^*(t_1-t)} dt \\cdot W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\\\ &amp; \\overset{\\tau = t_1-t}{=} e^{At} x_0 - \\underbrace{\\int_{0}^{t_1} e^{A\\tau} BB^* e^{A^*\\tau} d\\tau}_{W_c(t_1)} \\cdot W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\\\ &amp; = e^{At} x_0 - [e^{At_1} x_0 - x_1] = x_1 \\end{split} \\end{equation*}\\] (\\(2. \\Rightarrow 3.\\)) Prove by contradiction. Suppose \\(W_c(t) \\succ 0, \\forall t &gt; 0\\) but \\(\\mathcal{C}\\) is not of full row rank. Then there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[ v^* A^k B = 0, \\quad k = 0 \\dots n - 1 \\] By Corollary A.2, we have \\[ v^* A^k B = 0, \\ \\forall k \\in \\mathbb{N} \\Longrightarrow v^* e^{At} B = 0, \\ \\forall t &gt; 0 \\] which implies \\[ v^* W_c(t) v = v^* (\\int_{0}^{t} e^{A\\tau} B B^* e^{A^*\\tau} d\\tau) v = 0, \\quad \\forall t &gt; 0 \\] Contradiction! (\\(3. \\Rightarrow 2.\\)) Prove by contradiction. Suppose \\(\\mathcal{C}\\) has full row rank but \\(W_c(t_1)\\) is singular at some \\(t_1 &gt; 0\\). Then, similar to the proof in (\\(1. \\Rightarrow 2.\\)), there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\(F(t) := v^* e^{At} B \\equiv 0, \\forall t \\in [0, t_1]\\). Since \\(F(t)\\) is infinitely differentiable, we get its \\(i\\)’s derivative at \\(t=0\\), where \\(i = 0, 1, \\dots n-1\\). This results in \\[\\begin{equation*} \\left. \\frac{d^i F}{dt^i} \\right|_{t=0} = \\left. v^* A^{i} e^{At} B \\right|_{t=0} = v^* A^i B = 0, \\quad i = 0 \\dots n-1 \\end{equation*}\\] Thus, \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\). Contradiction! (\\(3. \\Rightarrow 4.\\)) Proof by contradiction. Suppose \\([A - \\lambda I, B]\\) does not have full row rank for some \\(\\lambda \\in \\mathbb{C}\\). Then, there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\(v^* [A - \\lambda I, B] = 0\\). This implies \\(v^* A = v^* \\lambda\\) and \\(v^* B = 0\\). On the other hand, \\[\\begin{equation*} v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1}B \\end{bmatrix} = v^* \\begin{bmatrix} B &amp; \\lambda B &amp; \\dots &amp; \\lambda^{n-1} B \\end{bmatrix} = 0 \\end{equation*}\\] Contradiction! (\\(4. \\Rightarrow 5.\\)) Proof by contradiction. If there exists a left eigenvector and eigenvalue pair \\((x, \\lambda)\\), s.t. \\(x^* A = \\lambda x^*\\) while \\(x^*B = 0\\), then \\(x^* [A - \\lambda I, B] = 0\\). Contradiction! (\\(5. \\Rightarrow 3.\\)) Proof by contradiction. If the controllability matrix \\(\\mathcal{C}\\) does not have full row rank, i.e., \\(\\text{rank}(\\mathcal{C}) = k &lt; n\\). Then, from Corollary A.4, there exists a similarity transformation \\(T\\), s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_{c} &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] where \\(\\bar{A}_c \\in \\mathbb{R}^{k \\times k}, \\bar{A}_{\\bar{c}} \\in \\mathbb{R}^{(n-k) \\times (n-k)}\\). Now arbitrarily pick one of \\(\\bar{A}_{\\bar{c}}\\)’s left eigenvector \\(x_{\\bar{c}}\\) and its corresponding eigenvalue \\(\\lambda_1\\). Define the vector \\(x = \\begin{bmatrix} 0 \\\\ x_{\\bar{c}} \\end{bmatrix}\\). Then, \\[\\begin{equation*} \\begin{split} x^* (TAT^{-1}) = \\begin{bmatrix} 0 &amp; x_{\\bar{c}}^* \\end{bmatrix} \\begin{bmatrix} \\bar{A}_{c} &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} &amp; = \\begin{bmatrix} 0 &amp; x_{\\bar{c}}^* \\bar{A}_{\\bar{c}} \\end{bmatrix} = \\begin{bmatrix} 0 &amp; \\lambda_1 x_{\\bar{c}}^* \\end{bmatrix} = \\lambda_1 x^* \\\\ x^* (TB) &amp; = \\begin{bmatrix} 0 &amp; x_{\\bar{x}} \\end{bmatrix} \\begin{bmatrix} B_{\\bar{c}} \\\\ 0 \\end{bmatrix} = 0 \\end{split} \\end{equation*}\\] which implies \\((TAT^{-1}, TB)\\) is not controllable. However, similarity transformation does not change controllability. Contradiction! (\\(6. \\Rightarrow 1.\\)) Prove by contradiction. If \\((A, B)\\) is not controllable, i.e., \\(\\text{rank}(\\mathcal{C}) = k &lt; n\\). Then from Corollary A.4, there exists a similarity transformation \\(T\\) s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] Now arbitrarily pick \\(F \\in \\mathbb{R}^{m\\times n}\\) and define \\(FT^{-1} = [F_1, F_2]\\), where \\(F_1 \\in \\mathbb{R}^{m\\times k}, F_2 \\in \\mathbb{R}^{m\\times (n-k)}\\). Thus, \\[\\begin{equation*} \\begin{split} \\text{det}(A+BF-\\lambda I) &amp; = \\text{det}\\left( T^{-1} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} T + T^{-1} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} F - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right) \\\\ &amp; = \\text{det}\\left( T^{-1} \\left\\{ \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} + \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} FT^{-1} - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right\\} T \\right) \\\\ &amp; = \\text{det}\\left( \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} + \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\begin{bmatrix} F_1 &amp; F_2 \\end{bmatrix} - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right) \\\\ &amp; = \\text{det} \\begin{bmatrix} \\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1 &amp; \\bar{A}_{12} + \\bar{B}_c F_2 \\\\ 0 &amp; \\bar{A}_{\\bar{c}} - \\lambda I_2 \\end{bmatrix} \\\\ &amp; = \\text{det}(\\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1) \\cdot \\text{det}(\\bar{A}_{\\bar{c}} - \\lambda I_2) \\end{split} \\end{equation*}\\] where \\(I_1\\) is the identity matrix of size \\(k\\). Similarly, \\(I_2\\) of size \\(n-k\\). Thus, at least \\(n-k\\) eigenvalues of \\(A+BF\\) cannot be freely assigned by choosing \\(F\\). Contradiction! (\\(1. \\Rightarrow 6.\\)) Here we only represent the SIMO case. For the MIMO case, the proof is far more complex. Interesting readers can refer to (Davison and Wonham 1968) (the shortest proof I can find). Since there is only one input, the matrix \\(B\\) degenerate to vector \\(b\\). From Corollary A.5, there exist a similarity transformation matrix \\(T\\), s.t. \\[\\begin{equation*} T A T^{-1} = A_1 := \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix}, \\quad T b = b_1 := \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation*}\\] For any \\(F \\in \\mathbb{C}^{1 \\times n}\\), denote \\(FT^{-1}\\) as \\([f_1, f_2, \\dots, f_n]\\). Calculating the characteristic polynomial of \\(A + bF\\): \\[\\begin{equation*} \\begin{split} \\text{det}(\\lambda I - A - bF) &amp; = \\text{det}(\\lambda I - T^{-1}A_1 T - T^{-1} b_1 F) \\\\ &amp; = \\text{det}(\\lambda I - A_1 - b_1 F T^{-1}) \\\\ &amp; = \\text{det} \\begin{bmatrix} \\lambda + a_1 - f_1 &amp; \\lambda + a_2 - f_2 &amp; \\dots &amp; \\lambda + a_{n-1} - f_{n-1} &amp; \\lambda + a_n - f_n \\\\ -1 &amp; \\lambda &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; -1 &amp; \\lambda \\end{bmatrix} \\\\ &amp; = \\lambda^n + (a_1 - f_1) \\lambda^{n-1} + \\dots + (a_n - f_n) \\end{split} \\end{equation*}\\] By choosing \\([f_1, f_2, \\dots, f_n]\\), \\(A+bF\\)’s eigenvalues can be arbitrarily set. (\\(7. \\Rightarrow 1.\\)) Prove by contradiction. Assume that \\((A, B)\\) is not controllable. Then from 2., there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\) and \\(t_1 &gt; 0\\), \\[\\begin{equation*} F(t) = v^* e^{At} B = 0, \\quad \\forall t \\in [0, t_1] \\end{equation*}\\] Now consider \\(F(z) = v^* e^{Az} B, z\\in \\mathcal{C}\\), which is a vector of analytic function in complex analysis. For a arbitrary \\(t_2 \\in (0, t_1)\\), we have \\(F^{(i)}(t_2) = 0, \\forall i \\in \\mathbb{N}\\). Then, by invoking the fact from complex analysis: “Let \\(G\\) a connected open set and \\(f: G \\rightarrow \\mathbb{C}\\) be analytic, then \\(f \\equiv 0\\) on \\(G\\), if and only if there is a point \\(a \\in G\\) such that \\(f^{(i)}(a) = 0, \\forall n \\in \\mathbb{N}\\)”, we have \\(f(z) \\equiv 0, \\forall z \\in \\mathbb{C}\\). On the other hand, however, \\(W_c \\succ 0\\) implies there exists \\(t_3 &gt; 0\\), such that for the above \\(v\\), we have \\(v^* e^{At_3} B \\ne 0\\). Contradiction! (\\(1. \\Rightarrow 7.\\)) Since \\((A, B)\\) is controllable, from 2., \\(W_c(t) \\succ 0, \\forall t\\). Therefore, \\(W_c \\succ 0\\). The existence and uniqueness of the solution for \\(AW_c + W_cA^* = -BB^*\\) can be obtained directly from the proof of Theorem A.3, by setting \\(Q\\) there to be positive semidefinite. A.2.3 Duality Although controllability and observability seemingly have no direct connections from their definitions A.2 and A.3, the following theorem (Chen 1984) states their tight relations. Theorem A.6 (Theorem of Duality) The pair \\((C,A)\\) is observable if and only if \\((A^*,C^*)\\) is controllable. Proof. We first show that \\((C,A)\\) is observable if and only if the \\(n \\times n\\) matrix \\(W_o(t) = \\int_{0}^{t} e^{A^*\\tau} C^*C e^{A\\tau}\\) is positive definite (nonsingular) for any \\(t&gt;0\\): “\\(\\Longleftarrow\\)”: From (A.5), given initial state \\(x(0)\\) and the inputs \\(u(t)\\), \\(y(t)\\) can be expressed as \\[\\begin{equation*} y(t) = Ce^{At} x(0) + C \\int_{0}^{t} e^{A(t-\\tau)} Bu(\\tau) d\\tau + Du(t) \\end{equation*}\\] Define a known function \\(\\bar{y}(t)\\) as \\(y(t) - C \\int_{0}^{t} e^{A(t-\\tau)} Bu(\\tau) d\\tau - Du(t)\\) and we will get \\[\\begin{equation*} Ce^{At} x(0) = \\bar{y}(t) \\end{equation*}\\] Pre-multiply the above equation by \\(e^{A^*t}C^*\\) and integrate it over \\([0,t_1]\\) to yield \\[\\begin{equation*} (\\int_{0}^{t_1} e^{A^*t} C^*C e^{At} dt) x(0) = W_o(t_1) x(0) = \\int_{0}^{t_1} e^{A^*t} C^* \\bar{y}(t) dt \\end{equation*}\\] Since \\(W_o(t_1) \\succ 0\\), \\[\\begin{equation*} x(0) = W_o(t_1)^{-1} \\int_{0}^{t_1} e^{A^*t} C^* \\bar{y}(t) dt \\end{equation*}\\] can be observed. “\\(\\Longrightarrow\\)”: Prove by contradiction. Suppose \\((C,A)\\) is observable but there exists \\(t_1 &gt;0\\), s.t. \\(W_o(t_1)\\) is singular. This implies there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} v^* W_o(t_1) v = 0 \\Longrightarrow Ce^{At} v \\equiv 0, \\ \\forall t \\in [0,t_1] \\end{equation*}\\] Similar to the proof of Theorem A.5 (\\(7. \\Rightarrow 1.\\)), we can use conclusions from complex analysis to claim that \\(Ce^{At} v \\equiv 0, \\forall t &gt;0\\). On the other hand, we set \\(u(t) \\equiv 0\\), which results in \\(y(t) = Ce^{At}x(0)\\). In this case \\(x(0) = 0\\) and \\(x(0) = v \\ne 0\\) will lead to the same output responses \\(y(t)\\) over \\(t&gt;0\\), which implies \\((C,A)\\) is not observable. Contradiction! Next we show the duality of controllability and observability: From (1) we know \\((C,A)\\) is controllable if and only of \\[\\begin{equation*} \\int_{0}^{t} e^{A^*\\tau} C^*C e^{A\\tau} d\\tau = \\int_{0}^{t} e^{(A^*)\\tau} (C^*)^* (C^*) e^{(A^*)^*\\tau} d\\tau \\end{equation*}\\] is nonsingular for all \\(t &gt;0\\). The latter is exactly the definition of \\((A^*, C^*)\\)’s controllability Gramian \\(W_c(t)\\). A.2.4 Equivalent Statements for Observability With the Theorem of Duality A.6, we can directly write down the equivalent statements of observability without any additional proofs: Theorem A.7 (Equivalent Statements for Observability) The following statements are equivalent (Chen 1984), (Zhou, Doyle, and Glover 1996): \\((C, A)\\) is observable. The matrix \\[\\begin{equation*} W_o(t) := \\int_{0}^{t} e^{A^*\\tau} C^* C e^{A\\tau} d\\tau \\end{equation*}\\] is positive definite for any \\(t&gt;0\\). The observability matrix \\[\\begin{equation*} \\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\\\ CA^2 \\\\ \\dots \\\\ CA^{n-1} \\end{bmatrix} \\end{equation*}\\] has full column rank. The matrix \\(\\begin{bmatrix} A - \\lambda I \\\\ C \\end{bmatrix}\\) has full column rank for all \\(\\lambda \\in \\mathbb{C}\\). Let \\(\\lambda\\) and \\(y\\) be any eigenvalue and any corresponding right eigenvector of \\(A\\), i.e., \\(Ay = \\lambda y\\), then \\(Cy \\ne 0\\). The eigenvalues of \\(A+LC\\) can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs) by a suitable choice of \\(L\\). \\((A^*, C^*)\\) is controllable. If, in addition, all eigenvalues of \\(A\\) have negative parts, then the unique solution of \\[\\begin{equation*} A^* W_o + W_o A = -C^* C \\end{equation*}\\] is positive definite. The solution is called the observability Gramian and can be expressed as \\[\\begin{equation*} W_o = \\int_{0}^{\\infty} e^{A^*\\tau} C^* C e^{A\\tau} d\\tau \\end{equation*}\\] A.3 Stabilizability And Detectability To define stabilizability and detectability of an LTI system, we first introduce the concept of system mode, which can be naturally derived from the fifth definition of controllability A.5 (observability A.7). Definition A.4 (System Mode) \\(\\lambda\\) is a mode of an LTI system, if it is an eigenvalue of \\(A\\). The mode \\(\\lambda\\) is said to be: stable, if \\(\\text{Re}\\lambda &lt; 0\\), controllable, if \\(x^* B \\ne 0\\) for all left eigenvectors of \\(A\\) associated with \\(\\lambda\\), observable, if \\(C x \\ne 0\\) for all right eigenvectors of \\(A\\) associated with \\(\\lambda\\). Otherwise, the mode is said to be uncontrollable (unobservable). With the concept of system mode, the fifth definition of controllability A.5 (observability A.7) can be restated as An LTI system is controllable (observable) if and only if all modes are controllable (observable). Stabilizability (detectability) is defined similarly via loosening part of controllability (observability) conditions. Definition A.5 (Stabilizability) An LTI system is said to be stabilizable if all of its unstable modes are controllable. Definition A.6 (Detectability) An LTI system is said to be detectable if all of its unstable modes are observable. Like in the case of controllability and observability, duality also holds in stabilizability and detectability. Moreover, similarity transformation will not influence an LTI system’s stabilizability and detectability. A.3.1 Equivalent Statements for Stabilizability Theorem A.8 (Equivalent Statements for Stabilizability) The following statements are equivalent (Zhou, Doyle, and Glover 1996): \\((A,B)\\) is stabilizable. For all \\(\\lambda\\) and \\(x\\) such that \\(x^* A = \\lambda x^*\\) and \\(\\text{Re} \\lambda \\ge 0\\), \\(x^* B \\ne 0\\). The matrix \\([A-\\lambda I, B]\\) has full rank for all \\(\\text{Re} \\lambda \\ge 0\\). There exists a matrix \\(F\\) such that \\(A+BF\\) are Hurwitz. Proof. (\\(1. \\Leftrightarrow 2.\\)) Directly from stabilizability’s definition. (\\(2. \\Leftrightarrow 3.\\)) If 2. holds but 3. not hold, then there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} v^* [A-\\lambda I, B] = 0 \\Leftrightarrow v^* A = \\lambda v^*, v^* B = 0, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] Contradiction! Vice versa. (\\(4. \\Rightarrow 2.\\)) Prove by contradiction. Suppose there \\(x \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} x^* [A-\\lambda I, B] = 0 \\Leftrightarrow x^* A = \\lambda x^*, x^* B = 0, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] Thus, for any \\(F\\), \\[\\begin{equation*} x^* (A+BF) = \\lambda x^*, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] On the other hand, suppose \\(A+BF\\) has \\(I\\) Jordon blocks, with each equipped with an eigenvalue \\(\\eta_i, i = 1\\dots I\\) (note that \\(\\eta_\\alpha\\) may be equal to \\(\\eta_\\beta\\), i.e., they are equivalent eigenvalues with different Jordon blocks). Since \\(A+BF\\)’s eigenvalues all have negative real parts, \\(\\text{Re} (\\eta_i) &lt; 0, i = 1\\dots I\\). For each \\(\\eta_i,i \\in \\left\\{1\\dots i\\right\\}\\), denote its \\(K_i\\) generalized left eigenvectors as \\(v_{i,1}, v_{i,2}, \\dots v_{i,K_i}\\). By definition, \\(\\sum_{i=1}^{I} K_i = n\\) and \\[\\begin{equation*} \\begin{split} v_{i,1}^* (A+BF) &amp; = v_{i,1}^* \\cdot \\eta_i \\\\ v_{i,2}^* (A+BF) &amp; = v_{i,1}^* + v_{i,2}^* \\cdot \\eta_i \\\\ &amp; \\vdots \\\\ v_{i,K_i}^* (A+BF) &amp; = v_{i,K_i-1}^* + v_{i,K_i}^* \\cdot \\eta_i \\end{split} \\end{equation*}\\] for all \\(i \\in \\left\\{1\\dots i\\right\\}\\). Also, \\(v_{i,k},i=1\\dots I, k=1\\dots K_i\\) are linearly independent and spans \\(\\mathbb{C}^n\\). Therefore, \\[\\begin{equation*} x^* = \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* \\end{equation*}\\] which leads to \\[\\begin{equation*} \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* (A+BF) = \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot \\lambda \\cdot v_{i,k}^* \\end{equation*}\\] Since \\(v_{i,k}\\)’s are \\(A+BF\\)’s generalized eigenvectors, we have \\[\\begin{equation*} \\begin{split} &amp; \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* \\cdot (A+BF) \\\\ = &amp; \\sum_{i=1}^{I} \\left\\{ \\xi_{i,1} \\cdot \\eta_i \\cdot v_{i,1}^* + \\sum_{k=2}^{K_i} \\xi_{i,k} (v_{i,k-1}^* + \\eta_i \\cdot v_{i,k}^* ) \\right\\} \\\\ = &amp; \\sum_{i=1}^{I} \\left\\{ \\sum_{k=1}^{K_i - 1} (\\xi_{i,k}\\cdot \\eta_i + \\xi_{i,k+1}) v_{i,k}^* + \\xi_{i,K_i} \\cdot \\eta_i \\cdot v_{i,K_i}^* \\right\\} \\end{split} \\end{equation*}\\] Combining the above two equations: \\[\\begin{equation*} \\sum_{i=1}^{I} \\left\\{ \\sum_{k=1}^{K_i - 1} \\left[ \\xi_{i,k}\\cdot (\\eta_i - \\lambda) + \\xi_{i,k+1} \\right] v_{i,k}^* + \\xi_{i,K_i} \\cdot (\\eta_i - \\lambda) \\cdot v_{i,K_i}^* = 0 \\right\\} \\end{equation*}\\] Since \\(v_{i,k}\\)’s are linearly independent, for any \\(i \\in \\left\\{i\\dots I\\right\\}\\): \\[\\begin{equation*} \\begin{split} \\xi_{i,1} \\cdot (\\eta_i - \\lambda) + \\xi_{i,2} &amp; = 0 \\Rightarrow \\xi_{i,2} = (-1) \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda) \\\\ \\xi_{i,2} \\cdot (\\eta_i - \\lambda) + \\xi_{i,3} &amp; = 0 \\Rightarrow \\xi_{i,3} = (-1)^2 \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^2 \\\\ &amp; \\vdots \\\\ \\xi_{i,K_i-1} \\cdot (\\eta_i - \\lambda) + \\xi_{i,K_i} &amp; = 0 \\Rightarrow \\xi_{i,K_i} = (-1)^{K_i-1} \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^{K_i-1} \\\\ \\xi_{i,K_i} \\cdot (\\eta_i - \\lambda) &amp; = 0 \\end{split} \\end{equation*}\\] Thus, \\[\\begin{equation*} (-1)^{K_i-1} \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^{K_i} = 0 \\end{equation*}\\] Denote \\(\\xi_{i,1}\\) as \\(r_1 e^{\\theta_1}\\), \\((\\eta_i - \\lambda)\\) as \\(r_2 e^{\\theta_2}\\). Since \\(\\text{Re} \\lambda \\ge 0, \\text{Re}(\\eta_i) &lt; 0\\), \\(r_2 &gt; 0\\). On the other hand, the following equation suggests \\[\\begin{equation*} r_1 r_2^{K_i-1} e^{j[\\theta_1 + \\theta_2 (K_i-1)]} = 0 \\end{equation*}\\] Thus, \\(r_1\\) has to be \\(0\\), which implies \\(\\xi_{i,1} = 0\\). By recursion, \\(\\xi_{i,k} = 0, \\forall k = 1\\dots K_i\\). Contradiction! (\\(1. \\Rightarrow 4.\\)) If \\((A,B)\\) is controllable, then from Theorem (thm:lticontrollable)’s sixth definition, we can freely assign the poles of \\(A+BF\\) via choosing \\(F\\) properly. Otherwise, if \\((A,B)\\) is uncontrollable, then from Corollary A.4 and proof of Theorem A.5 (\\(6. \\Rightarrow 1.\\)), there exists a similarity transformation \\(T\\), s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] and \\[\\begin{equation*} \\text{det}(A+BF-\\lambda I) = \\underbrace{\\text{det}(\\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1)}_{\\chi_c(\\lambda)} \\cdot \\underbrace{\\text{det}(\\bar{A}_{\\bar{c}} - \\lambda I_2)}_{\\chi_{\\bar{c}}(\\lambda)} \\end{equation*}\\] where \\(\\bar{A}_c \\in \\mathbb{C}^{k_1 \\times k_1}\\), \\(I_1\\) identity matrix of size \\(k_1\\), \\([F_1,F_2] = FT^{-1}\\), and \\(k_1 = \\text{rank} \\mathcal{C}\\). Additionally, \\((\\bar{A}_c, \\bar{B}_c)\\) is controllable. Thus, \\(\\chi_c(\\lambda)\\)’s zeros can be freely assigned by choosing proper \\(F\\), i.e., system modes with \\(\\chi_c(\\lambda)\\) is controllable, regardless of its stability. On the other hand, system modes with \\(\\chi_{\\bar{c}}(\\lambda)\\) must be stable. Otherwise, we cannot affect it by assigning \\(F\\), which is a contradiction to statement (1). Therefore, \\((TAT^{-1}, TB)\\) is stabilizable. Since similarity transformation does not change stabilizability, \\((A,B)\\) is stabilizable. A.3.2 Equivalent Statements for Detectability Thanks to duality, we can directly write down the equivalent statements of observability without any additional proofs: Theorem A.9 (Equivalent Statements for Detectability) The following statements are equivalent (Zhou, Doyle, and Glover 1996): \\((C,A)\\) is detectable. For all \\(\\lambda\\) and \\(x\\) such that \\(A x = \\lambda x\\) and \\(\\text{Re} \\lambda \\ge 0\\), \\(C x \\ne 0\\). The matrix \\(\\begin{bmatrix} A - \\lambda I \\\\ C \\end{bmatrix}\\) has full rank for all \\(\\text{Re} \\lambda \\ge 0\\). There exists a matrix \\(L\\) such that \\(A+LC\\) are Hurwitz. \\((A^*, C^*)\\) is stabilizable. References "],["appconvex.html", "B Convex Analysis and Optimization", " B Convex Analysis and Optimization "],["the-kalman-yakubovich-lemma.html", "C The Kalman-Yakubovich Lemma", " C The Kalman-Yakubovich Lemma Lemma C.1 (Kalman-Yakubovich) Consider a controllable linear time-invariant system \\[ \\dot{x} = A x + b u \\\\ y = c^T x. \\] The transfer function \\[ h(p) = c^T (p I - A)^{-1} b \\] is strictly positive real (SPR) if and only if there exist positive definite matrices \\(P\\) and \\(Q\\) such that \\[ A^T P + P A = - Q \\\\ Pb = c. \\] "],["feedbacklinearization.html", "D Feedback Linearization", " D Feedback Linearization "],["slidingcontrol.html", "E Sliding Control", " E Sliding Control "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
