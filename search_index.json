[["index.html", "Semidefinite Optimization and Relaxation Preface", " Semidefinite Optimization and Relaxation Heng Yang 2024-01-19 Preface This is the textbook for Harvard ENG-SCI 257: Semidefinite Optimization and Relaxation. Information about the offerings of the class is listed below. 2024 Spring Time: Mon/Wed 2:15 - 3:30pm Location: Science and Engineering Complex, 1.413 Instructor: Heng Yang Teaching Fellow: Safwan Hossain Syllabus Acknowledgment "],["notation.html", "Notation", " Notation We will use the following standard notation throughout this book. Basics \\(\\mathbb{R}^{}\\) real numbers \\(\\mathbb{R}^{}_{+}\\) nonnegative real \\(\\mathbb{R}^{}_{++}\\) positive real \\(\\mathbb{Z}\\) integers \\(\\mathbb{N}\\) nonnegative integers \\(\\mathbb{R}^{n}\\) \\(n\\)-D column vector \\(\\mathbb{R}^{n}_{+}\\) nonnegative orthant \\(\\mathbb{R}^{n}_{++}\\) positive orthant \\(e_i\\) standard basic vector \\(\\Delta_n := \\{x \\in \\mathbb{R}^n_{+} \\mid \\sum x_i = 1 \\}\\) standard simplex Matrices \\(\\mathbb{R}^{m \\times n}\\) \\(m \\times n\\) real matrices \\(\\mathbb{S}^{n}\\) \\(n\\times n\\) symmetric matrices \\(\\mathbb{S}^{n}_{+}\\) \\(n\\times n\\) positive semidefinite matrices \\(\\mathbb{S}^{n}_{++}\\) \\(n\\times n\\) positive definite matrices \\(\\langle A, B \\rangle\\) or \\(\\bullet\\) inner product in \\(\\mathbb{R}^{m \\times n}\\) \\(\\mathrm{tr}(A)\\) trace of \\(A \\in \\mathbb{R}^{n \\times n}\\) \\(A^\\top\\) matrix transpose \\(\\det(A)\\) matrix determinant \\(\\mathrm{rank}(A)\\) rank of a matrix \\(\\mathrm{diag}(A)\\) diagonal of a matrix \\(A\\) as a vector \\(\\mathrm{Diag}(a)\\) turning a vector into a diagonal matrix \\(\\mathrm{BlkDiag}(A,B,\\dots)\\) block diagonal matrix with blocks \\(A,B,\\dots\\) \\(\\succeq 0\\) and \\(\\preceq 0\\) positive / negative semidefinite \\(\\succ 0\\) and \\(\\prec 0\\) positive / negative definite \\(\\lambda_{\\max}\\) and \\(\\lambda_{\\min}\\) maximum / minimum eigenvalue \\(\\sigma_{\\max}\\) and \\(\\sigma_{\\min}\\) maximum / minimum singular value \\(\\mathrm{vec}(A)\\) vectorization of \\(A \\in \\mathbb{R}^{m \\times n}\\) \\(\\mathrm{svec}(A)\\) symmetric vectorization of \\(A \\in \\mathbb{S}^{n}\\) \\(\\Vert A \\Vert_\\mathrm{F}\\) Frobenius norm Geometry \\(\\Vert a \\Vert_{p}\\) \\(p\\)-norm \\(\\Vert a \\Vert\\) \\(2\\)-norm \\(B(o,r)\\) ball with center \\(o\\) and radius \\(r\\) \\(\\mathrm{conv}(S)\\) convex hull of set \\(S\\) \\(\\mathrm{cone}(S)\\) conical hull of set \\(S\\) \\(\\mathrm{int}(S)\\) interior of set \\(S\\) \\(\\partial S\\) boundary of set \\(S\\) \\(P^\\circ\\) polar dual of convex body \\(\\mathrm{SO}(d)\\) special orthogonal group of dimension \\(d\\) \\(\\mathcal{S}^{d-1}\\) unit sphere in \\(\\mathbb{R}^{d}\\) Optimization KKT Karush–Kuhn–Tucker LP linear program QP quadratic program SOCP second-order cone program SDP semidefinite program Algebra \\(\\mathbb{R}[x]\\) polynomial ring in \\(x\\) with real coefficients \\(\\deg\\) degree of a monomial / polynomial \\(\\mathbb{R}[x]_d\\) polynomials in \\(x\\) of degree up to \\(d\\) \\([x]_d\\) vector of monomials of degree up to \\(d\\) \\([\\![x ]\\!]_d\\) vector of monomials of degree \\(d\\) "],["background.html", "Chapter 1 Mathematical Background 1.1 Convexity 1.2 Convex Geometry", " Chapter 1 Mathematical Background Convexity Convex Optimization Convex Geometry Linear Programming 1.1 Convexity A very important notion in modern optimization is that of convexity. To a large extent, an optimization problem is “easy” if it is convex, and “difficult” when convexity is lost, i.e., nonconvex. We give a basic review of convexity here and refer the reader to (Rockafellar 1970), (Boyd and Vandenberghe 2004), and (Bertsekas, Nedic, and Ozdaglar 2003) for comprehensive treatments. We will work on a finite-dimensional real vector space, which we will identify with \\(\\mathbb{R}^{n}\\). Definition 1.1 (Convex Set) A set \\(S\\) is convex if \\(x_1,x_2 \\in S\\) implies \\(\\lambda x_1 + (1-\\lambda) x_2 \\in S\\) for any \\(\\lambda \\in [0,1]\\). In other words, if \\(x_1,x_2 \\in S\\), then the line segment connecting \\(x_1\\) and \\(x_2\\) lies inside \\(S\\). Conversely, a set \\(S\\) is nonconvex if Definition 1.1 does not hold. A hyperplane is a common convex set defined as \\[ P = \\{ x \\in \\mathbb{R}^{n} \\mid \\langle c, x \\rangle = d \\} \\] for some \\(c \\in \\mathbb{R}^{n}\\) and scalar \\(d\\). A halfspace is a convex set defined as \\[ H = \\{ x \\in \\mathbb{R}^{n} \\mid \\langle c, x \\rangle \\geq d \\}. \\] An important property of a convex set is that we can certify when a point is not in the set. This is usually done via a separation theorem. Theorem 1.1 (Separation Theorem) Let \\(S_1,S_2\\) be two convex sets in \\(\\mathbb{R}^{n}\\) and \\(S_1 \\cap S_2 = \\emptyset\\), then there exists a hyperplane that separates \\(S_1\\) and \\(S_2\\), i.e., there exists \\(c\\) and \\(d\\) such that \\[\\begin{equation} \\begin{split} \\langle c, x \\rangle \\geq d, &amp; \\forall x \\in S_1,\\\\ \\langle c, x \\rangle \\leq d, &amp; \\forall x \\in S_2. \\end{split} \\tag{1.1} \\end{equation}\\] Further, if \\(S_1\\) is compact (i.e., closed and bounded) and \\(S_2\\) is closed, then the separation is strict, i.e., the inequalities in (1.1) are strict. The strict separation theorem is used typically when \\(S_1\\) is a single point (hence compact). The intersection of convex sets is always convex (try to prove this). 1.2 Convex Geometry Definition 1.2 (Extreme Point) Given a convex set, a point \\(x \\in S\\) is extreme if \\[ \\forall x_1, x_2 \\in S, \\exists \\lambda \\in (0,1) \\text { such that } x = \\lambda x_1 + (1-\\lambda) x_2 \\Longrightarrow x = x_1 = x_2. \\] In other words, any line segment that contains \\(x\\) and lies inside \\(S\\) must be \\(x\\) itself. References "],["sdp.html", "Chapter 2 Semidefinite Optimization", " Chapter 2 Semidefinite Optimization Positive Semidefinite Matrices Spectrahedra "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
