# Mathematical Background {#background}

## Convexity {#background:convexity}

A very important notion in modern optimization is that of _convexity_. To a large extent, an optimization problem is "easy" if it is convex, and "difficult" when convexity is lost, i.e., _nonconvex_. We give a basic review of convexity here and refer the reader to [@rockafellar70-convexanalysis], [@boyd04book-convex], and [@bertsekas03book-convex] for comprehensive treatments.

We will work on a finite-dimensional real vector space, which we will identify with $\Real{n}$.

::: {.definitionbox}
::: {.definition #ConvexSet name="Convex Set"}
A set $S$ is convex if $x_1,x_2 \in S$ implies $\lambda x_1 + (1-\lambda) x_2 \in S$ for any $\lambda \in [0,1]$. In other words, if $x_1,x_2 \in S$, then the line segment connecting $x_1$ and $x_2$ lies inside $S$.
:::
:::

Conversely, a set $S$ is nonconvex if Definition \@ref(def:ConvexSet) does not hold. 

Given $x_1, x_2 \in S$, $\lambda x_1 + (1-\lambda) x_2$ is called a _convex combination_ when $\lambda \in [0,1]$. For convenience, we will use the following notation
\begin{equation}
\begin{split}
(x_1,x_2) = \cbrace{\lambda x_1 + (1-\lambda) x_2 \mid \lambda \in (0,1)}, \\ [x_1,x_2] = \cbrace{\lambda x_1 + (1-\lambda) x_2 \mid \lambda \in [0,1]}.
\end{split}
\end{equation}

A **hyperplane** is a common convex set defined as
\begin{equation}
H = \cbrace{ x \in \Real{n} \mid \inprod{c}{x} = d }
(\#eq:hyperplane)
\end{equation}
for some $c \in \Real{n}$ and scalar $d$. A **halfspace** is a convex set defined as
\begin{equation}
H^{+} = \cbrace{ x \in \Real{n} \mid \inprod{c}{x} \geq d }.
(\#eq:halfspace)
\end{equation}

Given two nonempty convex sets $C_1$ and $C_2$, the **distance** between $C_1$ and $C_2$ is defined as 
\begin{equation}
\dist (C_1,C_2) = \inf \cbrace{\norm{c_1 - c_2} \mid c_1 \in C_1, c_2 \in C_2}.
\end{equation}

For a convex set $C$, the hyperplane $H$ in \@ref(eq:hyperplane) is called a **supporting hyperplane** for $C$ if $C$ is contained in the half space $H^{+}$ and the distance between $H$ and $C$ is zero. For example, the hyperplane $x_1 = 0$ is supporting for the hyperboloid $\cbrace{(x_1,x_2) \mid x_1 x_2 \geq 1, x_1 \geq 0, x_2 \geq 0}$ in $\Real{2}$.

An important property of a convex set is that we can _certify_ when a point is not in the set. This is usually done via a separation theorem.

::: {.theorembox}
::: {.theorem #SeparationTheorem name="Separation Theorem"}
Let $S_1,S_2$ be two convex sets in $\Real{n}$ and $S_1 \cap S_2 = \emptyset$, then there exists a hyperplane that separates $S_1$ and $S_2$, i.e., there exists $c$ and $d$ such that 
\begin{equation}
\begin{split}
\inprod{c}{x} \geq d, &  \forall x \in S_1,\\
\inprod{c}{x} \leq d, & \forall x \in S_2.
\end{split}
(\#eq:separation)
\end{equation}
Further, if $S_1$ is compact (i.e., closed and bounded) and $S_2$ is closed, then the separation is strict, i.e., the inequalities in \@ref(eq:separation) are strict.
:::
:::

The strict separation theorem is used typically when $S_1$ is a single point (hence compact).

We will see a generalization of the separation theorem for nonconvex sets later after we introduce the idea of sums of squares.

::: {.exercisebox}
::: {.exercise}
Provide examples of two disjoint convex sets such that the separation in \@ref(eq:separation) is not strict in one way and both ways.
:::
:::

::: {.exercisebox}
::: {.exercise}
Provide a constructive proof that the separation hyperplane exists in Theorem \@ref(thm:SeparationTheorem) when (1) both $S_1$ and $S_2$ are closed, and (2) at least one of them is bounded.
:::
:::

The intersection of convex sets is always convex (try to prove this).

## Convex Geometry {#background:convex:geometry}

### Basic Facts

Given a set $S$, its **affine hull** is the set 
$$
\aff (S) = \lcbrace{ \sum_{i=1}^k \lambda_i u_i \mid \lambda_1 + \dots + \lambda_k = 1, u_i \in S, k \in \bbN_{+} },
$$
where $\sum_{i=1}^{k} \lambda_i u_i$ is called an _affine combination_ of $u_1,\dots,u_k$ when $\sum_i \lambda_i = 1$. The affine hull of a set is the smallest affine subspace that contains $S$, and the **dimension** of $S$ is the dimension of its affine hull. The affine hull of the emptyset is the emptyset, of a singleton is the singleton itself. The affine hull of a set of two different points is the line going through them. The affine hull of a set of three points not on one line is the plane going through them. The affine hull of a set of four points not in a plane in $\Real{3}$ is the entire space $\Real{3}$.

For a convex set $C \subseteq \Real{n}$, the **interior** of $C$ is defined as
$$
\interior(C) := \cbrace{ u \in C \mid \exists \epsilon > 0, B(u,\epsilon) \subseteq C },
$$
where $B(u,\epsilon)$ denotes a ball centered at $u$ with radius $\epsilon$ (using the usual 2-norm). Each point in $\interior(C)$ is called an _interior point_ of $C$. If $\interior(C) = C$, then $C$ is said to be an **open set**. A convex set with nonempty interior is called a **convex domain**, while a compact (i.e., closed and bounded) convex domain is called a **convex body**.

The **boundary of $C$** is the subset of points that are in the **closure**^[The closure of a subset $C$ of points, denoted $\cl(C)$, consists of all points in $C$ together with all limit points of $C$. The closure of $C$ may equivalently be defined as the intersection of all closed sets containing $C$. Intuitively, the closure can be thought of as all the points that are either in $C$ or "very near" $C$. For example, the closure of the open line segment $C= (0,1)$ is the closed line segment $C=[0,1]$.] of $C$ but are not in the interior of $C$, and we denote it as $\partial C$. For example, the closed line segment $C = [0,1]$ has two points on the boundary: $0$ and $1$; the open line segment $C = (0,1)$ has the same two points as its boundary.

It is possible that a convex set has empty interior. For example, a hyperplane has no interior, and neither does a singleton. In such cases, the **relative interior** can be defined as
$$
\relint(C) := \cbrace{ u \in C \mid \exists \epsilon > 0, B(u,\epsilon) \cap \aff(C) \subseteq C }.
$$
For a nonempty convex set, the relative interior always exists. If $\relint(C) = C$, then $C$ is said to be **relatively open**. For example, the relative interior of a singleton is the singleton itself, and hence a singleton is relatively open.

For a convex set $C$, a point $u \in C$ is called an **extreme point** if 
$$
u \in (x,y), x \in C, y \in C \quad \Rightarrow u = x = y.
$$
For example, consider $C = \cbrace{(x,y)\mid x^2 + y^2 \leq 1}$, then all the points on the boundary $\partial C = \cbrace{(x,y) \mid x^2 + y^2 = 1}$ are extreme points.

A subset $F \subseteq C$ is called a **face** if $F$ itself is convex and 
$$
u \in (x,y), u \in F, x,y \in C \quad \Rightarrow x,y \in F. 
$$
Clearly, the empty set $\emptyset$ and the entire set $C$ are faces of $C$, which are called _trivial faces_. The face $F$ is said to be _proper_ if $F \neq C$. The set of any single extreme point is also a face. A face $F$ of $C$ is called **exposed** if there exists a supporting hyperplane $H$ for $C$ such that 
$$
F = H \cap C.
$$

### Cones, Duality, Polarity

::: {.definitionbox}
::: {.definition #polar name="Polar"}
For a nonempty set $T \subseteq \Real{n}$, its polar is the set 
\begin{equation}
T^\circ := \cbrace{ y \in \Real{n} \mid \inprod{x}{y} \leq 1, \forall x \in T }.
(\#eq:polar)
\end{equation}
:::
:::

The polar $T^\circ$ is a closed convex set and contains the origin. Note that $T$ is always contained in the polar of $T^\circ$, i.e., $T \subseteq (T^\circ)^\circ$. Indeed, they are equal under some assumptions.

::: {.theorembox}
::: {.theorem #bipolar name="Bipolar"}
If $T \subseteq \Real{n}$ is a closed convex set containing the origin, then $(T^\circ)^\circ = T$.
:::
:::

An important class of convex sets are those that are invariant under positive scalings.^[Some authors define a cone using nonnegative scalings.] A set $K \subseteq \Real{n}$ is a **cone** if $t x \in K$ for all $x \in K$ and for all $t > 0$. For example, the positive real line $\cbrace{x \in \Real{} \mid x > 0}$ is a cone. The cone $K$ is **pointed** if $K \cap -K = \{ 0 \}$. It is said to be **solid** if its interior $\interior(K) \neq \emptyset$. Any nonzero point of a cone cannot be extreme. If a cone is pointed, the only extreme point is the origin. 

The analogue of extreme point for convex cones is the **extreme ray**. For a convex cone $K$ and $0 \neq u \in K$, the line segment
$$
u \cdot [0,\infty) := \cbrace{tu \mid t\geq 0}
$$
is called an extreme ray of $K$ if 
$$
u \in (x,y), x,y \in K \quad \Rightarrow \quad u,x,y \text{ are parallel to each other}.
$$
If $u \cdot [0,\infty)$ is an extreme ray, then we say $u$ generates the extreme ray. 

::: {.definitionbox}
::: {.definition #ProperCone name="Proper Cone"}
A cone $K$ is proper if it is closed, convex, pointed, and solid.
:::
:::

A proper cone $K$ induces a **partial order** on the vector space, via $x \succeq y$ if $x - y \in K$. We also use $x \succ y$ if $x - y$ is in $\interior(K)$. Important examples of proper cones are the nonnegative orthant, the second-order cone, the set of symmetric positive semidefinite matrices, and the set of nonnegative polynomials, which we will describe later in the book.

::: {.definitionbox}
::: {.definition #Dual name="Dual"}
The dual of a nonempty set $S$ is 
$$
S^* := \cbrace{ y \in \Real{n} \mid \inprod{y}{x} \geq 0, \forall x \in S}.
$$
:::
:::

Given any set $S$, its dual $S^*$ is always a closed convex cone. Duality reverses inclusion, that is, 
$$
S_1 \subseteq S_2 \quad \Rightarrow \quad S_1^* \supseteq S_2^*.
$$
If $S$ is a closed convex cone, then $S^{* *}= S$. Otherwise, $S^{* *}$ is the closure of the smallest convex cone that contains $S$.

For a cone $K \subseteq \Real{n}$, one can show that 
$$
K^\circ = \cbrace{y \in \Real{n} \mid \inprod{x}{y} \leq 0, \forall x \in K}.
$$
The set $K^\circ$ is called the **polar cone** of $K$. The negative of $K^\circ$ is just the **dual cone**
$$
K^{*} = \cbrace{y \in \Real{n} \mid \inprod{x}{y} \geq 0, \forall x \in K}.
$$

::: {.definitionbox}
::: {.definition #selfdual name="Self-dual"}
A cone $K$ is self-dual if $K^{*} = K$.
:::
:::

As an easy example, the nonnegative orthant $\Real{n}_{+}$ is self-dual.

::: {.examplebox}
::: {.example #SecondOrderCone name="Second-order Cone"}
The second-order cone, or the Lorentz cone, or the ice cream cone
$$
\calQ_n := \cbrace{ (x_0,x_1,\dots,x_n) \in \Real{n+1} \mid \sqrt{x_1^2 + \dots + x_n^2} \leq x_0 }
$$
is a proper cone of $\Real{n+1}$. We will show that it is also self-dual.

**Proof**. Consider $(y_0,y_1,\dots,y_n) \in \calQ_n$, we want to show that 
\begin{equation}
x_0 y_0 + x_1 y_1 + \dots + x_n y_n \geq 0, \forall (x_0,x_1,\dots,x_n) \in \calQ_n.
(\#eq:dual-cone-condition)
\end{equation}
This is easy to verify because 
$$
x_1 y_1 + \dots + x_n y_n \geq - \sqrt{x_1^2 + \dots + x_n^2} \sqrt{y_1^2 + \dots + y_n^2} \geq - x_0 y_0.
$$
Hence we have $\calQ_n \subseteq \calQ_n^{*}$.

Conversely, if \@ref(eq:dual-cone-condition) holds, then take 
$$
x_1 = -y_1, \dots, x_n = - y_n, \quad x_0 = \sqrt{x_1^2 + \dots + x_n^2},
$$
we have 
$$
y_0 \geq \sqrt{y_1^2 + \dots + y_n^2},
$$
hence $\calQ_n^{*} \subseteq \calQ_n$. $\blacksquare$

:::
:::

Not every proper cone is self-dual.

::: {.exercisebox}
::: {.exercise}
Consider the following proper cone in $\Real{2}$
$$
K = \cbrace{(x_1,x_2) \mid 2x_1 - x_2 \geq 0, 2x_2 - x_1 \geq 0}.
$$
Show that it is not self-dual.
:::
:::

## Convex Optimization {#background:convex:optimization}

::: {.definitionbox}
::: {.definition #ConvexFun name="Convex Function"}
A function $f: \Real{n} \rightarrow \Real{}$ is a convex function if 
$$
f(\lambda x + (1-\lambda) y) \leq \lambda f(x) + (1-\lambda) f(y), \forall \lambda \in [0,1], \forall x,y \in \Real{n}.
$$
:::
:::

A function $f$ is convex if and only if its **epigraph** $\cbrace{(x,t) \in \Real{n+1} \mid f(x) \leq t}$ is a convex set.

When a function $f$ is differentiable, then there are several equivalent characterizations of convexity, in terms of the gradient $\nabla f(x)$ or the Hessian $\nabla^2 f(x)$.

::: {.theorembox}
::: {.theorem #CharacterizeConvexity name="Equivalent Characterizations of Convexity"}
Let $f: \Real{n} \rightarrow \Real{}$ be a twice differentiable function. The following propositions are equivalent.

i. $f$ is convex, i.e.,
$$
f(\lambda x + (1-\lambda) y) \leq \lambda f(x) + (1-\lambda) f(y), \forall \lambda \in [0,1], x,y \in \Real{n}.
$$

ii. The first-order convexity condition holds:
$$
f(y) \geq f(x) + \inprod{\nabla f(x)}{ y - x}, \forall x, y \in \Real{n},
$$
i.e., the hyperplane going through $(x,f(x))$ with slope $\nabla f(x)$ supports the epigraph of $f$.

iii. The second-order convexity condition holds:
$$
\nabla^2 f(x) \succeq 0, \forall x \in \Real{n},
$$
i.e., the Hessian is positive semidefinite everywhere.
:::
:::

Let's work on a little exercise.

::: {.exercisebox}
::: {.exercise}
Which one of the following functions $f: \Real{n} \rightarrow \Real{}$ is not convex?

a. $\exp(-c\tran x)$, with $c$ constant 

b. $\exp(c\tran x)$, with $c$ constant 

c. $\exp(x\tran x)$

d. $\exp(-x\tran x)$
:::
:::

### Minimax Theorem

Given a function $f: X \times Y \rightarrow \Real{}$, the following inequality always holds 
\begin{equation}
\max_{y \in Y} \min_{x \in X} f(x,y) \leq \min_{x \in X} \max_{y \in Y} f(x,y).
(\#eq:weak-minimax)
\end{equation}
If the maximum or minimum is not attained, then \@ref(eq:weak-minimax) holds with $\max$ / $\min$ replaced by $\sup$ and $\inf$, respectively.

::: {.exercisebox}
::: {.exercise}
Provide examples of $f$ such that the inequality in \@ref(eq:weak-minimax) is strict.
:::
:::

It is of interest to understand when equality holds in \@ref(eq:weak-minimax).

::: {.theorembox}
::: {.theorem #minimax name="Minimax Theorem"}
Let $X \subset \Real{n}$ and $Y \subset \Real{n}$ be compact convex sets, and $f: X \times Y \rightarrow \Real{}$ be a continuous function that is convex in its first argument and concave in the second. Then
$$
\max_{y \in Y} \min_{x \in X} f(x,y) = \min_{x \in X} \max_{y \in Y} f(x,y).
$$
:::
:::

A special case of this theorem, used in game theory to prove the existence of equilibria for zero-sum games, is when $X$ and $Y$ are standard unit simplicies and the function $f(x,y)$ is bilinear. In a research from our group [@tang23arxiv-uncertainty], we used the minimax theorem to convert a minimax problem into a single-level minimization problem.

### Lagrangian Duality 

Consider a nonlinear optimization problem 
\begin{equation}
\begin{split}
u^\star = \min_{x \in \Real{n}} & \quad f(x) \\
\subject & \quad g_i(x) \leq 0, i=1,\dots,m, \\
& \quad h_j(x) = 0, j = 1,\dots,p.
\end{split}
(\#eq:background-nlp)
\end{equation}
Define the **Lagrangian** associated with the optimization problem \@ref(eq:background-nlp) as 
\begin{equation}
\begin{split}
L: \Real{n} \times \Real{m}_{+} \times \Real{p} \quad & \rightarrow \quad \Real{}, \\
(x,\lambda,\mu) \quad & \mapsto \quad f(x) + \sum_{i=1}^m \lambda_i g_i(x) + \sum_{j=1}^p \mu_j h_j(x).
\end{split}
(\#eq:background-Lagrangian)
\end{equation}
The **Lagrangian dual function** is defined as 
\begin{equation}
\phi(\lambda,\mu) := \min_{x \in \Real{n}} L(x,\lambda,\mu).
(\#eq:background-Lagrangian-dual)
\end{equation}
Maximizing this function over the dual variables $(\lambda,\mu)$ yields 
\begin{equation}
v^\star := \max_{\lambda \geq 0, \mu \in \Real{p}} \phi(\lambda,\mu)
(\#eq:background-Lagrangian-dual-problem)
\end{equation}
Applying the minimax Theorem \@ref(thm:minimax), we can see that 
$$
v^\star = \max_{(\lambda,\mu)} \min_{x} L(x,\lambda,\mu) \leq \min_{x} \max_{(\lambda,\mu)} L(x,\lambda,\mu) = u^\star.
$$
That is to say solving the dual problem \@ref(eq:background-Lagrangian-dual-problem) always provides a lower bound to the primal problem \@ref(eq:background-nlp).

If the functions $f,g_i$ are convex and $h_i$ are affine, the Lagrangian is convex in $x$ and convex in $(\lambda,\mu)$. To ensure strong duality (i.e., $u^\star = v^\star$), compactness or other **constraint qualifications** are needed. An often used condition is the Slater constraint qualification. 

::: {.definitionbox}
::: {.definition #SlaterCQ name="Slater Constraint Qualification"}
There exists a strictly feasible point for \@ref(eq:background-nlp), i.e., a point $z \in \Real{n}$ such that $h_j(z) = 0,j=1,\dots,p$ and $g_i(z) < 0,i=1,\dots,m$.
:::
:::

Under these conditions, we have strong duality.

::: {.theorembox}
::: {.theorem #StrongDuality name="Strong Duality"}
Consider the optimization \@ref(eq:background-nlp) and assume $f,g_i$ are convex and $h_j$ are affine. If Slater's constraint qualification holds, then the optimal value of the primal problem \@ref(eq:background-nlp) is the same as the optimal value of the dual problem \@ref(eq:background-Lagrangian-dual-problem).
:::
:::

### KKT Optimality Conditions

Consider the nonlinear optimization problem \@ref(eq:background-nlp). A pair of primal and dual variables $(x^\star,\lambda^\star,\mu^\star)$ is said to satisfy the Karush-Kuhn-Tucker (KKT) optimality conditions if 

\begin{equation}
\begin{split}
\text{primal feasibility}:\ \  & g_i(x^\star) \leq 0,\forall i=1,\dots,m; h_j(x^\star) = 0, \forall j=1,\dots,p \\
\text{dual feasibility}:\ \  & \lambda_i^\star \geq 0, \forall i=1,\dots,m \\
\text{stationarity}:\ \  & \nabla_x L(x^\star,\lambda^\star,\mu^\star) = 0 \\
\text{complementarity}:\ \  & \lambda_i^\star \cdot g_i(x^\star) = 0, \forall i=1,\dots,m.
\end{split}
(\#eq:KKT-conditions)
\end{equation}

Under certain constraint qualifications, the KKT conditions are necessary for local optimality.

::: {.theorembox}
::: {.theorem #KKTNecessary name="Necessary Optimality Conditions"}
Assume any of the following constraint qualifications hold:

- The gradients of the constraints $\cbrace{\nabla g_i(x^\star)}_{i=1}^m$, $\cbrace{\nabla h_j(x^\star)}_{j=1}^p$ are linearly independent. 

- Slater's constraint qualification (cf. Definition \@ref(def:SlaterCQ)). 

- All constraints $g_i(x)$ and $h_j(x)$ are affine functions.

Then, at every local minimum $x^\star$ of \@ref(eq:background-nlp), the KKT conditions \@ref(eq:KKT-conditions) hold.
:::
:::

On the other hand, for convex optimization problems, the KKT conditions are sufficient for global optimality.

::: {.theorembox}
::: {.theorem #KKTSufficient name="Sufficient Optimality Conditions"}
Assume optimization \@ref(eq:background-nlp) is convex, i.e., $f,g_i$ are convex and $h_j$ are affine. Every point $x^\star$ that satisfies the KKT conditions \@ref(eq:KKT-conditions) is a global minimizer.
:::
:::


## Linear Optimization {#background:linear:optimization}

### Polyhedra 

In $\Real{n}$, a **polyhedron** is a set defined by finitely many linear inequalities, i.e.,
\begin{equation}
P = \cbrace{x \in \Real{n} \mid A x \geq b},
(\#eq:polyhedron)
\end{equation}
for some matrix $A \in \Real{m \times n}$ and $b \in \Real{m}$. In \@ref(eq:polyhedron), the inequality should be interpreted as $A x - b \in \Real{m}_{+}$, i.e., every entry of $Ax$ is no smaller than the corresponding entry of $b$. 

The convex hull of finitely many points in $\Real{n}$ is called a **polytope**, where the convex hull of a set $S$ is defined as 
\begin{equation}
\hspace{-10mm} \conv(S) = \lcbrace{\sum_{i=1}^k \lambda_i u_i \mid k \in \bbN_{+}, \sum_{i=1}^k \lambda_i = 1, \lambda_i \geq 0,i=1,\dots,k, u_i \in S, \forall i =1,\dots,k},
(\#eq:convex-hull)
\end{equation}
i.e., all possible convex combinations of points in $S$. Clearly, a polytope is bounded. 

The conic hull of finitely many points in $\Real{n}$ is called a **polyhedral cone**, where the conic hull of a set $S$ is defined as 
\begin{equation}
\hspace{-10mm} \cone(S) = \lcbrace{ \sum_{i=1}^k \lambda_i u_i \mid k \in \bbN_{+}, \lambda_i \geq 0,i=1,\dots,k, u_i \in S, \forall i =1,\dots,k  }.
(\#eq:conic-hull)
\end{equation}
The only difference between \@ref(eq:conic-hull) and \@ref(eq:convex-hull) is the removal of $\sum_{i} \lambda_i = 1$. Clearly, the origin belongs to the conic hull of any nonempty set, and the conic hull of any nonempty set is unbounded.

The next theorem characterizes a polyhedron.

::: {.theorembox}
::: {.theorem #DecomposePolyhedron name="Polyhedron Decomposition"}
Every polyhedron $P$ is finitely generated, i.e., it can be written as the Minkowski sum of a polytope and a polyhedral cone:
$$
P = \conv(u_1,\dots,u_r) + \cone(v_1,\dots,v_s),
$$
where the Minkowski sum of two sets is defined as $X + Y := \cbrace{x+y \mid x \in X, y \in Y}$.

Further, a bounded polyhedron is a polytope.
:::
:::

An extreme point of a polytope is called a **vertex**. A $1$-dimensional face of a polytope is called an **edge**. A $d-1$-dimensional face of a $d$-dimensional polytope is called a **facet**.

### Linear Program

We will now give a brief review of important results in linear programming (LP). In some sense, the theory of semidefinite programming (SDP) has been developed in order to generalize those of LP to the setup where the decision variable becomes a symmetric matrix and the inequality is interpreted as being positive semidefinite.

A standard form linear program (LP) reads
\begin{equation}
\begin{split}
\min_{x \in \Real{n}} & \quad \inprod{c}{x}  \\
\subject & \quad Ax = b \\
& \quad x \geq 0
\end{split},
(\#eq:primal-lp)
\end{equation}
for given $A \in \Real{m\times n}$, $b \in \Real{m}$, and $c \in \Real{n}$. Often the tuple $(A,b,c)$ is called the _problem data_ because the LP \@ref(eq:primal-lp) is fully defined once the tuple is given (indeed many LP numerical solvers take the tuple $(A,b,c)$ as input). Clearly, the feasible set of the LP \@ref(eq:primal-lp) is a polyhedron. The LP \@ref(eq:primal-lp) is often referred to as the **primal** LP. Associated with \@ref(eq:primal-lp) is the following **dual** LP 
\begin{equation}
\begin{split}
\max_{y \in \Real{m}} & \quad \inprod{b}{y} \\
\subject & \quad c - A\tran y \geq 0
\end{split}.
(\#eq:dual-lp)
\end{equation}
It is worth noting that the dimension of the dual variable $y$ is exactly the number of constraints in the primal LP. 

**Weak duality**. For the pair of primal-dual LPs, it is easy to verify that, for any $x$ that is feasible for the primal \@ref(eq:primal-lp) and $y$ that is feasible for the dual \@ref(eq:dual-lp), we have
$$
\inprod{c}{x} - \inprod{b}{y} = \inprod{c}{x} - \inprod{Ax}{y} = \inprod{c}{x} - \inprod{A\tran y}{x} = \inprod{c - A\tran y}{x} \geq 0.
$$
Therefore, denoting $p^\star$ as the optimum of \@ref(eq:primal-lp) and $d^\star$ as the optimum of \@ref(eq:dual-lp), we have the weak duality 
$$
p^\star \geq d^\star.
$$

If $p^\star = d^\star$, then we say **strong duality** holds. The LP \@ref(eq:primal-lp) is said to be **feasible** if its feasible set is nonempty. It is said to be **unbounded below** if there exists a sequence $\cbrace{u_i}_{i=1}^{\infty} \subseteq \Real{n}_{+}$ such that $\inprod{c}{u_i} \rightarrow -\infty$ and $A u_i = b$. If the primal \@ref(eq:primal-lp) is infeasible (resp. unbounded below), we set $p^\star = + \infty$ (resp. $p^\star = - \infty$). Similar characteristics are defined for the dual LP \@ref(eq:dual-lp). In particular, if the dual \@ref(eq:dual-lp) is unbounded, then we set $d^\star = + \infty$. If the dual is infeasible, then we set $d^\star = - \infty$.

We have the following strong duality result.

::: {.theorembox}
::: {.theorem #LPStrongDuality name="LP Strong Duality"}
For the LP primal-dual pair \@ref(eq:primal-lp) and \@ref(eq:dual-lp), we have 

:::
:::
