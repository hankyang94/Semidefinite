<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Mathematical Background | Semidefinite Optimization and Relaxation</title>
  <meta name="description" content="Lecture notes for Harvard ES 257 Semidefinite Optimization and Relaxation." />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Mathematical Background | Semidefinite Optimization and Relaxation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES 257 Semidefinite Optimization and Relaxation." />
  <meta name="github-repo" content="hankyang94/Semidefinite" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Mathematical Background | Semidefinite Optimization and Relaxation" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES 257 Semidefinite Optimization and Relaxation." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2024-02-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="notation.html"/>
<link rel="next" href="sdp.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Semidefinite Optimization and Relaxation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#offerings"><i class="fa fa-check"></i>Offerings</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="notation.html"><a href="notation.html"><i class="fa fa-check"></i>Notation</a></li>
<li class="chapter" data-level="1" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>1</b> Mathematical Background</a>
<ul>
<li class="chapter" data-level="1.1" data-path="background.html"><a href="background.html#background:convexity"><i class="fa fa-check"></i><b>1.1</b> Convexity</a></li>
<li class="chapter" data-level="1.2" data-path="background.html"><a href="background.html#background:convex:geometry"><i class="fa fa-check"></i><b>1.2</b> Convex Geometry</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="background.html"><a href="background.html#basic-facts"><i class="fa fa-check"></i><b>1.2.1</b> Basic Facts</a></li>
<li class="chapter" data-level="1.2.2" data-path="background.html"><a href="background.html#cones-duality-polarity"><i class="fa fa-check"></i><b>1.2.2</b> Cones, Duality, Polarity</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="background.html"><a href="background.html#background:convex:optimization"><i class="fa fa-check"></i><b>1.3</b> Convex Optimization</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="background.html"><a href="background.html#minimax-theorem"><i class="fa fa-check"></i><b>1.3.1</b> Minimax Theorem</a></li>
<li class="chapter" data-level="1.3.2" data-path="background.html"><a href="background.html#background:convex:optimization:Lagrangian"><i class="fa fa-check"></i><b>1.3.2</b> Lagrangian Duality</a></li>
<li class="chapter" data-level="1.3.3" data-path="background.html"><a href="background.html#kkt-optimality-conditions"><i class="fa fa-check"></i><b>1.3.3</b> KKT Optimality Conditions</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="background.html"><a href="background.html#background:linear:optimization"><i class="fa fa-check"></i><b>1.4</b> Linear Optimization</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="background.html"><a href="background.html#polyhedra"><i class="fa fa-check"></i><b>1.4.1</b> Polyhedra</a></li>
<li class="chapter" data-level="1.4.2" data-path="background.html"><a href="background.html#linear-program"><i class="fa fa-check"></i><b>1.4.2</b> Linear Program</a></li>
<li class="chapter" data-level="1.4.3" data-path="background.html"><a href="background.html#farkas-lemma"><i class="fa fa-check"></i><b>1.4.3</b> Farkas Lemma</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sdp.html"><a href="sdp.html"><i class="fa fa-check"></i><b>2</b> Semidefinite Optimization</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sdp.html"><a href="sdp.html#positive-semidefinite-matrices"><i class="fa fa-check"></i><b>2.1</b> Positive Semidefinite Matrices</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="sdp.html"><a href="sdp.html#geometric-properties"><i class="fa fa-check"></i><b>2.1.1</b> Geometric Properties</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="sdp.html"><a href="sdp.html#semidefinite-programming"><i class="fa fa-check"></i><b>2.2</b> Semidefinite Programming</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="sdp.html"><a href="sdp.html#spectrahedra"><i class="fa fa-check"></i><b>2.2.1</b> Spectrahedra</a></li>
<li class="chapter" data-level="2.2.2" data-path="sdp.html"><a href="sdp.html#formulation-and-duality"><i class="fa fa-check"></i><b>2.2.2</b> Formulation and Duality</a></li>
<li class="chapter" data-level="2.2.3" data-path="sdp.html"><a href="sdp.html#geometric-properties-1"><i class="fa fa-check"></i><b>2.2.3</b> Geometric Properties</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="sdp.html"><a href="sdp.html#software-for-conic-optimization"><i class="fa fa-check"></i><b>2.3</b> Software for Conic Optimization</a></li>
<li class="chapter" data-level="2.4" data-path="sdp.html"><a href="sdp.html#interior-point-algorithm"><i class="fa fa-check"></i><b>2.4</b> Interior Point Algorithm</a></li>
<li class="chapter" data-level="2.5" data-path="sdp.html"><a href="sdp.html#applications"><i class="fa fa-check"></i><b>2.5</b> Applications</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="sdp.html"><a href="sdp.html#lyapunov-stability"><i class="fa fa-check"></i><b>2.5.1</b> Lyapunov Stability</a></li>
<li class="chapter" data-level="2.5.2" data-path="sdp.html"><a href="sdp.html#linear-quadratic-regulator"><i class="fa fa-check"></i><b>2.5.2</b> Linear Quadratic Regulator</a></li>
<li class="chapter" data-level="2.5.3" data-path="sdp.html"><a href="sdp.html#domain-adaptation"><i class="fa fa-check"></i><b>2.5.3</b> Domain Adaptation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Shor.html"><a href="Shor.html"><i class="fa fa-check"></i><b>3</b> Shor’s Semidefinite Relaxation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="Shor.html"><a href="Shor.html#semidefinite-relaxation-of-qcqps"><i class="fa fa-check"></i><b>3.1</b> Semidefinite Relaxation of QCQPs</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="Shor.html"><a href="Shor.html#lagrangian-dual-problem"><i class="fa fa-check"></i><b>3.1.1</b> Lagrangian Dual Problem</a></li>
<li class="chapter" data-level="3.1.2" data-path="Shor.html"><a href="Shor.html#dual-of-the-dual-bidual"><i class="fa fa-check"></i><b>3.1.2</b> Dual of the Dual (Bidual)</a></li>
<li class="chapter" data-level="3.1.3" data-path="Shor.html"><a href="Shor.html#maxcut"><i class="fa fa-check"></i><b>3.1.3</b> MAXCUT</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Semidefinite Optimization and Relaxation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="background" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Mathematical Background<a href="background.html#background" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="background:convexity" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Convexity<a href="background.html#background:convexity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A very important notion in modern optimization is that of <em>convexity</em>. To a large extent, an optimization problem is “easy” if it is convex, and “difficult” when convexity is lost, i.e., <em>nonconvex</em>. We give a basic review of convexity here and refer the reader to <span class="citation">(<a href="#ref-rockafellar70-convexanalysis" role="doc-biblioref">Rockafellar 1970</a>)</span>, <span class="citation">(<a href="#ref-boyd04book-convex" role="doc-biblioref">S. P. Boyd and Vandenberghe 2004</a>)</span>, and <span class="citation">(<a href="#ref-bertsekas03book-convex" role="doc-biblioref">Bertsekas, Nedic, and Ozdaglar 2003</a>)</span> for comprehensive treatments.</p>
<p>We will work on a finite-dimensional real vector space, which we will identify with <span class="math inline">\(\mathbb{R}^{n}\)</span>.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:ConvexSet" class="definition"><strong>Definition 1.1  (Convex Set) </strong></span>A set <span class="math inline">\(S\)</span> is convex if <span class="math inline">\(x_1,x_2 \in S\)</span> implies <span class="math inline">\(\lambda x_1 + (1-\lambda) x_2 \in S\)</span> for any <span class="math inline">\(\lambda \in [0,1]\)</span>. In other words, if <span class="math inline">\(x_1,x_2 \in S\)</span>, then the line segment connecting <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> lies inside <span class="math inline">\(S\)</span>.</p>
</div>
</div>
<p>Conversely, a set <span class="math inline">\(S\)</span> is nonconvex if Definition <a href="background.html#def:ConvexSet">1.1</a> does not hold.</p>
<p>Given <span class="math inline">\(x_1, x_2 \in S\)</span>, <span class="math inline">\(\lambda x_1 + (1-\lambda) x_2\)</span> is called a <em>convex combination</em> when <span class="math inline">\(\lambda \in [0,1]\)</span>. For convenience, we will use the following notation
<span class="math display">\[\begin{equation}
\begin{split}
(x_1,x_2) = \{ \lambda x_1 + (1-\lambda) x_2 \mid \lambda \in (0,1) \}, \\ [x_1,x_2] = \{ \lambda x_1 + (1-\lambda) x_2 \mid \lambda \in [0,1] \}.
\end{split}
\end{equation}\]</span></p>
<p>A <strong>hyperplane</strong> is a common convex set defined as
<span class="math display" id="eq:hyperplane">\[\begin{equation}
H = \{  x \in \mathbb{R}^{n} \mid \langle c, x \rangle = d  \}
\tag{1.1}
\end{equation}\]</span>
for some <span class="math inline">\(c \in \mathbb{R}^{n}\)</span> and scalar <span class="math inline">\(d\)</span>. A <strong>halfspace</strong> is a convex set defined as
<span class="math display" id="eq:halfspace">\[\begin{equation}
H^{+} = \{  x \in \mathbb{R}^{n} \mid \langle c, x \rangle \geq d  \}.
\tag{1.2}
\end{equation}\]</span></p>
<p>Given two nonempty convex sets <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span>, the <strong>distance</strong> between <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> is defined as
<span class="math display">\[\begin{equation}
\mathrm{dist}(C_1,C_2) = \inf \{ \Vert c_1 - c_2 \Vert \mid c_1 \in C_1, c_2 \in C_2 \}.
\end{equation}\]</span></p>
<p>For a convex set <span class="math inline">\(C\)</span>, the hyperplane <span class="math inline">\(H\)</span> in <a href="background.html#eq:hyperplane">(1.1)</a> is called a <strong>supporting hyperplane</strong> for <span class="math inline">\(C\)</span> if <span class="math inline">\(C\)</span> is contained in the half space <span class="math inline">\(H^{+}\)</span> and the distance between <span class="math inline">\(H\)</span> and <span class="math inline">\(C\)</span> is zero. For example, the hyperplane <span class="math inline">\(x_1 = 0\)</span> is supporting for the hyperboloid <span class="math inline">\(\{ (x_1,x_2) \mid x_1 x_2 \geq 1, x_1 \geq 0, x_2 \geq 0 \}\)</span> in <span class="math inline">\(\mathbb{R}^{2}\)</span>.</p>
<p>An important property of a convex set is that we can <em>certify</em> when a point is not in the set. This is usually done via a separation theorem.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:SeparationTheorem" class="theorem"><strong>Theorem 1.1  (Separation Theorem) </strong></span>Let <span class="math inline">\(S_1,S_2\)</span> be two convex sets in <span class="math inline">\(\mathbb{R}^{n}\)</span> and <span class="math inline">\(S_1 \cap S_2 = \emptyset\)</span>, then there exists a hyperplane that separates <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span>, i.e., there exists <span class="math inline">\(c\)</span> and <span class="math inline">\(d\)</span> such that
<span class="math display" id="eq:separation">\[\begin{equation}
\begin{split}
\langle c, x \rangle \geq d, &amp;  \forall x \in S_1,\\
\langle c, x \rangle \leq d, &amp; \forall x \in S_2.
\end{split}
\tag{1.3}
\end{equation}\]</span>
Further, if <span class="math inline">\(S_1\)</span> is compact (i.e., closed and bounded) and <span class="math inline">\(S_2\)</span> is closed, then the separation is strict, i.e., the inequalities in <a href="background.html#eq:separation">(1.3)</a> are strict.</p>
</div>
</div>
<p>The strict separation theorem is used typically when <span class="math inline">\(S_1\)</span> is a single point (hence compact).</p>
<p>We will see a generalization of the separation theorem for nonconvex sets later after we introduce the idea of sums of squares.</p>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-1" class="exercise"><strong>Exercise 1.1  </strong></span>Provide examples of two disjoint convex sets such that the separation in <a href="background.html#eq:separation">(1.3)</a> is not strict in one way and both ways.</p>
</div>
</div>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-2" class="exercise"><strong>Exercise 1.2  </strong></span>Provide a constructive proof that the separation hyperplane exists in Theorem <a href="background.html#thm:SeparationTheorem">1.1</a> when (1) both <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> are closed, and (2) at least one of them is bounded.</p>
</div>
</div>
<p>The intersection of convex sets is always convex (try to prove this).</p>
</div>
<div id="background:convex:geometry" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Convex Geometry<a href="background.html#background:convex:geometry" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="basic-facts" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Basic Facts<a href="background.html#basic-facts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given a set <span class="math inline">\(S\)</span>, its <strong>affine hull</strong> is the set
<span class="math display">\[
\mathrm{aff}(S) =  \left\{  \sum_{i=1}^k \lambda_i u_i \mid \lambda_1 + \dots + \lambda_k = 1, u_i \in S, k \in \mathbb{N}_{+}  \right\} ,
\]</span>
where <span class="math inline">\(\sum_{i=1}^{k} \lambda_i u_i\)</span> is called an <em>affine combination</em> of <span class="math inline">\(u_1,\dots,u_k\)</span> when <span class="math inline">\(\sum_i \lambda_i = 1\)</span>. The affine hull of a set is the smallest affine subspace that contains <span class="math inline">\(S\)</span>, and the <strong>dimension</strong> of <span class="math inline">\(S\)</span> is the dimension of its affine hull. The affine hull of the emptyset is the emptyset, of a singleton is the singleton itself. The affine hull of a set of two different points is the line going through them. The affine hull of a set of three points not on one line is the plane going through them. The affine hull of a set of four points not in a plane in <span class="math inline">\(\mathbb{R}^{3}\)</span> is the entire space <span class="math inline">\(\mathbb{R}^{3}\)</span>.</p>
<p>For a convex set <span class="math inline">\(C \subseteq \mathbb{R}^{n}\)</span>, the <strong>interior</strong> of <span class="math inline">\(C\)</span> is defined as
<span class="math display">\[
\mathrm{int}(C) := \{  u \in C \mid \exists \epsilon &gt; 0, B(u,\epsilon) \subseteq C  \},
\]</span>
where <span class="math inline">\(B(u,\epsilon)\)</span> denotes a ball centered at <span class="math inline">\(u\)</span> with radius <span class="math inline">\(\epsilon\)</span> (using the usual 2-norm). Each point in <span class="math inline">\(\mathrm{int}(C)\)</span> is called an <em>interior point</em> of <span class="math inline">\(C\)</span>. If <span class="math inline">\(\mathrm{int}(C) = C\)</span>, then <span class="math inline">\(C\)</span> is said to be an <strong>open set</strong>. A convex set with nonempty interior is called a <strong>convex domain</strong>, while a compact (i.e., closed and bounded) convex domain is called a <strong>convex body</strong>.</p>
<p>The <strong>boundary of <span class="math inline">\(C\)</span></strong> is the subset of points that are in the <strong>closure</strong><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> of <span class="math inline">\(C\)</span> but are not in the interior of <span class="math inline">\(C\)</span>, and we denote it as <span class="math inline">\(\partial C\)</span>. For example, the closed line segment <span class="math inline">\(C = [0,1]\)</span> has two points on the boundary: <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>; the open line segment <span class="math inline">\(C = (0,1)\)</span> has the same two points as its boundary.</p>
<p>It is possible that a convex set has empty interior. For example, a hyperplane has no interior, and neither does a singleton. In such cases, the <strong>relative interior</strong> can be defined as
<span class="math display">\[
\mathrm{ri}(C) := \{  u \in C \mid \exists \epsilon &gt; 0, B(u,\epsilon) \cap \mathrm{aff}(C) \subseteq C  \}.
\]</span>
For a nonempty convex set, the relative interior always exists. If <span class="math inline">\(\mathrm{ri}(C) = C\)</span>, then <span class="math inline">\(C\)</span> is said to be <strong>relatively open</strong>. For example, the relative interior of a singleton is the singleton itself, and hence a singleton is relatively open.</p>
<p>For a convex set <span class="math inline">\(C\)</span>, a point <span class="math inline">\(u \in C\)</span> is called an <strong>extreme point</strong> if
<span class="math display">\[
u \in (x,y), x \in C, y \in C \quad \Rightarrow u = x = y.
\]</span>
For example, consider <span class="math inline">\(C = \{ (x,y)\mid x^2 + y^2 \leq 1 \}\)</span>, then all the points on the boundary <span class="math inline">\(\partial C = \{ (x,y) \mid x^2 + y^2 = 1 \}\)</span> are extreme points.</p>
<p>A subset <span class="math inline">\(F \subseteq C\)</span> is called a <strong>face</strong> if <span class="math inline">\(F\)</span> itself is convex and
<span class="math display">\[
u \in (x,y), u \in F, x,y \in C \quad \Rightarrow x,y \in F.
\]</span>
Clearly, the empty set <span class="math inline">\(\emptyset\)</span> and the entire set <span class="math inline">\(C\)</span> are faces of <span class="math inline">\(C\)</span>, which are called <em>trivial faces</em>. The face <span class="math inline">\(F\)</span> is said to be <em>proper</em> if <span class="math inline">\(F \neq C\)</span>. The set of any single extreme point is also a face. A face <span class="math inline">\(F\)</span> of <span class="math inline">\(C\)</span> is called <strong>exposed</strong> if there exists a supporting hyperplane <span class="math inline">\(H\)</span> for <span class="math inline">\(C\)</span> such that
<span class="math display">\[
F = H \cap C.
\]</span></p>
</div>
<div id="cones-duality-polarity" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Cones, Duality, Polarity<a href="background.html#cones-duality-polarity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definitionbox">
<div class="definition">
<p><span id="def:polar" class="definition"><strong>Definition 1.2  (Polar) </strong></span>For a nonempty set <span class="math inline">\(T \subseteq \mathbb{R}^{n}\)</span>, its polar is the set
<span class="math display" id="eq:polar">\[\begin{equation}
T^\circ := \{  y \in \mathbb{R}^{n} \mid \langle x, y \rangle \leq 1, \forall x \in T  \}.
\tag{1.4}
\end{equation}\]</span></p>
</div>
</div>
<p>The polar <span class="math inline">\(T^\circ\)</span> is a closed convex set and contains the origin. Note that <span class="math inline">\(T\)</span> is always contained in the polar of <span class="math inline">\(T^\circ\)</span>, i.e., <span class="math inline">\(T \subseteq (T^\circ)^\circ\)</span>. Indeed, they are equal under some assumptions.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:bipolar" class="theorem"><strong>Theorem 1.2  (Bipolar) </strong></span>If <span class="math inline">\(T \subseteq \mathbb{R}^{n}\)</span> is a closed convex set containing the origin, then <span class="math inline">\((T^\circ)^\circ = T\)</span>.</p>
</div>
</div>
<p>An important class of convex sets are those that are invariant under positive scalings.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> A set <span class="math inline">\(K \subseteq \mathbb{R}^{n}\)</span> is a <strong>cone</strong> if <span class="math inline">\(t x \in K\)</span> for all <span class="math inline">\(x \in K\)</span> and for all <span class="math inline">\(t &gt; 0\)</span>. For example, the positive real line <span class="math inline">\(\{ x \in \mathbb{R}^{} \mid x &gt; 0 \}\)</span> is a cone. The cone <span class="math inline">\(K\)</span> is <strong>pointed</strong> if <span class="math inline">\(K \cap -K = \{ 0 \}\)</span>. It is said to be <strong>solid</strong> if its interior <span class="math inline">\(\mathrm{int}(K) \neq \emptyset\)</span>. Any nonzero point of a cone cannot be extreme. If a cone is pointed, the only extreme point is the origin.</p>
<p>The analogue of extreme point for convex cones is the <strong>extreme ray</strong>. For a convex cone <span class="math inline">\(K\)</span> and <span class="math inline">\(0 \neq u \in K\)</span>, the line segment
<span class="math display">\[
u \cdot [0,\infty) := \{ tu \mid t\geq 0 \}
\]</span>
is called an extreme ray of <span class="math inline">\(K\)</span> if
<span class="math display">\[
u \in (x,y), x,y \in K \quad \Rightarrow \quad u,x,y \text{ are parallel to each other}.
\]</span>
If <span class="math inline">\(u \cdot [0,\infty)\)</span> is an extreme ray, then we say <span class="math inline">\(u\)</span> generates the extreme ray.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:ProperCone" class="definition"><strong>Definition 1.3  (Proper Cone) </strong></span>A cone <span class="math inline">\(K\)</span> is proper if it is closed, convex, pointed, and solid.</p>
</div>
</div>
<p>A proper cone <span class="math inline">\(K\)</span> induces a <strong>partial order</strong> on the vector space, via <span class="math inline">\(x \succeq y\)</span> if <span class="math inline">\(x - y \in K\)</span>. We also use <span class="math inline">\(x \succ y\)</span> if <span class="math inline">\(x - y\)</span> is in <span class="math inline">\(\mathrm{int}(K)\)</span>. Important examples of proper cones are the nonnegative orthant, the second-order cone, the set of symmetric positive semidefinite matrices, and the set of nonnegative polynomials, which we will describe later in the book.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:Dual" class="definition"><strong>Definition 1.4  (Dual) </strong></span>The dual of a nonempty set <span class="math inline">\(S\)</span> is
<span class="math display">\[
S^* := \{  y \in \mathbb{R}^{n} \mid \langle y, x \rangle \geq 0, \forall x \in S \}.
\]</span></p>
</div>
</div>
<p>Given any set <span class="math inline">\(S\)</span>, its dual <span class="math inline">\(S^*\)</span> is always a closed convex cone. Duality reverses inclusion, that is,
<span class="math display">\[
S_1 \subseteq S_2 \quad \Rightarrow \quad S_1^* \supseteq S_2^*.
\]</span>
If <span class="math inline">\(S\)</span> is a closed convex cone, then <span class="math inline">\(S^{* *}= S\)</span>. Otherwise, <span class="math inline">\(S^{* *}\)</span> is the closure of the smallest convex cone that contains <span class="math inline">\(S\)</span>.</p>
<p>For a cone <span class="math inline">\(K \subseteq \mathbb{R}^{n}\)</span>, one can show that
<span class="math display">\[
K^\circ = \{ y \in \mathbb{R}^{n} \mid \langle x, y \rangle \leq 0, \forall x \in K \}.
\]</span>
The set <span class="math inline">\(K^\circ\)</span> is called the <strong>polar cone</strong> of <span class="math inline">\(K\)</span>. The negative of <span class="math inline">\(K^\circ\)</span> is just the <strong>dual cone</strong>
<span class="math display">\[
K^{*} = \{ y \in \mathbb{R}^{n} \mid \langle x, y \rangle \geq 0, \forall x \in K \}.
\]</span></p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:selfdual" class="definition"><strong>Definition 1.5  (Self-dual) </strong></span>A cone <span class="math inline">\(K\)</span> is self-dual if <span class="math inline">\(K^{*} = K\)</span>.</p>
</div>
</div>
<p>As an easy example, the nonnegative orthant <span class="math inline">\(\mathbb{R}^{n}_{+}\)</span> is self-dual.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:SecondOrderCone" class="example"><strong>Example 1.1  (Second-order Cone) </strong></span>The second-order cone, or the Lorentz cone, or the ice cream cone
<span class="math display">\[
\mathcal{Q}_n := \{  (x_0,x_1,\dots,x_n) \in \mathbb{R}^{n+1} \mid \sqrt{x_1^2 + \dots + x_n^2} \leq x_0  \}
\]</span>
is a proper cone of <span class="math inline">\(\mathbb{R}^{n+1}\)</span>. We will show that it is also self-dual.</p>
<p><strong>Proof</strong>. Consider <span class="math inline">\((y_0,y_1,\dots,y_n) \in \mathcal{Q}_n\)</span>, we want to show that
<span class="math display" id="eq:dual-cone-condition">\[\begin{equation}
x_0 y_0 + x_1 y_1 + \dots + x_n y_n \geq 0, \forall (x_0,x_1,\dots,x_n) \in \mathcal{Q}_n.
\tag{1.5}
\end{equation}\]</span>
This is easy to verify because
<span class="math display">\[
x_1 y_1 + \dots + x_n y_n \geq - \sqrt{x_1^2 + \dots + x_n^2} \sqrt{y_1^2 + \dots + y_n^2} \geq - x_0 y_0.
\]</span>
Hence we have <span class="math inline">\(\mathcal{Q}_n \subseteq \mathcal{Q}_n^{*}\)</span>.</p>
<p>Conversely, if <a href="background.html#eq:dual-cone-condition">(1.5)</a> holds, then take
<span class="math display">\[
x_1 = -y_1, \dots, x_n = - y_n, \quad x_0 = \sqrt{x_1^2 + \dots + x_n^2},
\]</span>
we have
<span class="math display">\[
y_0 \geq \sqrt{y_1^2 + \dots + y_n^2},
\]</span>
hence <span class="math inline">\(\mathcal{Q}_n^{*} \subseteq \mathcal{Q}_n\)</span>. <span class="math inline">\(\blacksquare\)</span></p>
</div>
</div>
<p>Not every proper cone is self-dual.</p>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-3" class="exercise"><strong>Exercise 1.3  </strong></span>Consider the following proper cone in <span class="math inline">\(\mathbb{R}^{2}\)</span>
<span class="math display">\[
K = \{ (x_1,x_2) \mid 2x_1 - x_2 \geq 0, 2x_2 - x_1 \geq 0 \}.
\]</span>
Show that it is not self-dual.</p>
</div>
</div>
</div>
</div>
<div id="background:convex:optimization" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Convex Optimization<a href="background.html#background:convex:optimization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definitionbox">
<div class="definition">
<p><span id="def:ConvexFun" class="definition"><strong>Definition 1.6  (Convex Function) </strong></span>A function <span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{}\)</span> is a convex function if
<span class="math display">\[
f(\lambda x + (1-\lambda) y) \leq \lambda f(x) + (1-\lambda) f(y), \forall \lambda \in [0,1], \forall x,y \in \mathbb{R}^{n}.
\]</span></p>
</div>
</div>
<p>A function <span class="math inline">\(f\)</span> is convex if and only if its <strong>epigraph</strong> <span class="math inline">\(\{ (x,t) \in \mathbb{R}^{n+1} \mid f(x) \leq t \}\)</span> is a convex set.</p>
<p>When a function <span class="math inline">\(f\)</span> is differentiable, then there are several equivalent characterizations of convexity, in terms of the gradient <span class="math inline">\(\nabla f(x)\)</span> or the Hessian <span class="math inline">\(\nabla^2 f(x)\)</span>.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:CharacterizeConvexity" class="theorem"><strong>Theorem 1.3  (Equivalent Characterizations of Convexity) </strong></span>Let <span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{}\)</span> be a twice differentiable function. The following propositions are equivalent.</p>
<ol style="list-style-type: lower-roman">
<li><p><span class="math inline">\(f\)</span> is convex, i.e.,
<span class="math display">\[
f(\lambda x + (1-\lambda) y) \leq \lambda f(x) + (1-\lambda) f(y), \forall \lambda \in [0,1], x,y \in \mathbb{R}^{n}.
\]</span></p></li>
<li><p>The first-order convexity condition holds:
<span class="math display">\[
f(y) \geq f(x) + \langle \nabla f(x),  y - x \rangle, \forall x, y \in \mathbb{R}^{n},
\]</span>
i.e., the hyperplane going through <span class="math inline">\((x,f(x))\)</span> with slope <span class="math inline">\(\nabla f(x)\)</span> supports the epigraph of <span class="math inline">\(f\)</span>.</p></li>
<li><p>The second-order convexity condition holds:
<span class="math display">\[
\nabla^2 f(x) \succeq 0, \forall x \in \mathbb{R}^{n},
\]</span>
i.e., the Hessian is positive semidefinite everywhere.</p></li>
</ol>
</div>
</div>
<p>Let’s work on a little exercise.</p>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-4" class="exercise"><strong>Exercise 1.4  </strong></span>Which one of the following functions <span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{}\)</span> is not convex?</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\exp(-c^\top x)\)</span>, with <span class="math inline">\(c\)</span> constant</p></li>
<li><p><span class="math inline">\(\exp(c^\top x)\)</span>, with <span class="math inline">\(c\)</span> constant</p></li>
<li><p><span class="math inline">\(\exp(x^\top x)\)</span></p></li>
<li><p><span class="math inline">\(\exp(-x^\top x)\)</span></p></li>
</ol>
</div>
</div>
<div id="minimax-theorem" class="section level3 hasAnchor" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Minimax Theorem<a href="background.html#minimax-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given a function <span class="math inline">\(f: X \times Y \rightarrow \mathbb{R}^{}\)</span>, the following inequality always holds
<span class="math display" id="eq:weak-minimax">\[\begin{equation}
\max_{y \in Y} \min_{x \in X} f(x,y) \leq \min_{x \in X} \max_{y \in Y} f(x,y).
\tag{1.6}
\end{equation}\]</span>
If the maximum or minimum is not attained, then <a href="background.html#eq:weak-minimax">(1.6)</a> holds with <span class="math inline">\(\max\)</span> / <span class="math inline">\(\min\)</span> replaced by <span class="math inline">\(\sup\)</span> and <span class="math inline">\(\inf\)</span>, respectively.</p>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-5" class="exercise"><strong>Exercise 1.5  </strong></span>Provide examples of <span class="math inline">\(f\)</span> such that the inequality in <a href="background.html#eq:weak-minimax">(1.6)</a> is strict.</p>
</div>
</div>
<p>It is of interest to understand when equality holds in <a href="background.html#eq:weak-minimax">(1.6)</a>.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:minimax" class="theorem"><strong>Theorem 1.4  (Minimax Theorem) </strong></span>Let <span class="math inline">\(X \subset \mathbb{R}^{n}\)</span> and <span class="math inline">\(Y \subset \mathbb{R}^{n}\)</span> be compact convex sets, and <span class="math inline">\(f: X \times Y \rightarrow \mathbb{R}^{}\)</span> be a continuous function that is convex in its first argument and concave in the second. Then
<span class="math display">\[
\max_{y \in Y} \min_{x \in X} f(x,y) = \min_{x \in X} \max_{y \in Y} f(x,y).
\]</span></p>
</div>
</div>
<p>A special case of this theorem, used in game theory to prove the existence of equilibria for zero-sum games, is when <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are standard unit simplicies and the function <span class="math inline">\(f(x,y)\)</span> is bilinear. In a research from our group <span class="citation">(<a href="#ref-tang23arxiv-uncertainty" role="doc-biblioref">Tang, Lasserre, and Yang 2023</a>)</span>, we used the minimax theorem to convert a minimax problem into a single-level minimization problem.</p>
</div>
<div id="background:convex:optimization:Lagrangian" class="section level3 hasAnchor" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Lagrangian Duality<a href="background.html#background:convex:optimization:Lagrangian" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a nonlinear optimization problem
<span class="math display" id="eq:background-nlp">\[\begin{equation}
\begin{split}
u^\star = \min_{x \in \mathbb{R}^{n}} &amp; \quad f(x) \\
\mathrm{s.t.}&amp; \quad g_i(x) \leq 0, i=1,\dots,m, \\
&amp; \quad h_j(x) = 0, j = 1,\dots,p.
\end{split}
\tag{1.7}
\end{equation}\]</span>
Define the <strong>Lagrangian</strong> associated with the optimization problem <a href="background.html#eq:background-nlp">(1.7)</a> as
<span class="math display" id="eq:background-Lagrangian">\[\begin{equation}
\begin{split}
L: \mathbb{R}^{n} \times \mathbb{R}^{m}_{+} \times \mathbb{R}^{p} \quad &amp; \rightarrow \quad \mathbb{R}^{}, \\
(x,\lambda,\mu) \quad &amp; \mapsto \quad f(x) + \sum_{i=1}^m \lambda_i g_i(x) + \sum_{j=1}^p \mu_j h_j(x).
\end{split}
\tag{1.8}
\end{equation}\]</span>
The <strong>Lagrangian dual function</strong> is defined as
<span class="math display" id="eq:background-Lagrangian-dual">\[\begin{equation}
\phi(\lambda,\mu) := \min_{x \in \mathbb{R}^{n}} L(x,\lambda,\mu).
\tag{1.9}
\end{equation}\]</span>
Maximizing this function over the dual variables <span class="math inline">\((\lambda,\mu)\)</span> yields
<span class="math display" id="eq:background-Lagrangian-dual-problem">\[\begin{equation}
v^\star := \max_{\lambda \geq 0, \mu \in \mathbb{R}^{p}} \phi(\lambda,\mu)
\tag{1.10}
\end{equation}\]</span>
Applying the minimax Theorem <a href="background.html#thm:minimax">1.4</a>, we can see that
<span class="math display">\[
v^\star = \max_{(\lambda,\mu)} \min_{x} L(x,\lambda,\mu) \leq \min_{x} \max_{(\lambda,\mu)} L(x,\lambda,\mu) = u^\star.
\]</span>
That is to say solving the dual problem <a href="background.html#eq:background-Lagrangian-dual-problem">(1.10)</a> always provides a lower bound to the primal problem <a href="background.html#eq:background-nlp">(1.7)</a>.</p>
<p>If the functions <span class="math inline">\(f,g_i\)</span> are convex and <span class="math inline">\(h_i\)</span> are affine, the Lagrangian is convex in <span class="math inline">\(x\)</span> and convex in <span class="math inline">\((\lambda,\mu)\)</span>. To ensure strong duality (i.e., <span class="math inline">\(u^\star = v^\star\)</span>), compactness or other <strong>constraint qualifications</strong> are needed. An often used condition is the Slater constraint qualification.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:SlaterCQ" class="definition"><strong>Definition 1.7  (Slater Constraint Qualification) </strong></span>There exists a strictly feasible point for <a href="background.html#eq:background-nlp">(1.7)</a>, i.e., a point <span class="math inline">\(z \in \mathbb{R}^{n}\)</span> such that <span class="math inline">\(h_j(z) = 0,j=1,\dots,p\)</span> and <span class="math inline">\(g_i(z) &lt; 0,i=1,\dots,m\)</span>.</p>
</div>
</div>
<p>Under these conditions, we have strong duality.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:StrongDuality" class="theorem"><strong>Theorem 1.5  (Strong Duality) </strong></span>Consider the optimization <a href="background.html#eq:background-nlp">(1.7)</a> and assume <span class="math inline">\(f,g_i\)</span> are convex and <span class="math inline">\(h_j\)</span> are affine. If Slater’s constraint qualification holds, then the optimal value of the primal problem <a href="background.html#eq:background-nlp">(1.7)</a> is the same as the optimal value of the dual problem <a href="background.html#eq:background-Lagrangian-dual-problem">(1.10)</a>.</p>
</div>
</div>
</div>
<div id="kkt-optimality-conditions" class="section level3 hasAnchor" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> KKT Optimality Conditions<a href="background.html#kkt-optimality-conditions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the nonlinear optimization problem <a href="background.html#eq:background-nlp">(1.7)</a>. A pair of primal and dual variables <span class="math inline">\((x^\star,\lambda^\star,\mu^\star)\)</span> is said to satisfy the Karush-Kuhn-Tucker (KKT) optimality conditions if</p>
<p><span class="math display" id="eq:KKT-conditions">\[\begin{equation}
\begin{split}
\text{primal feasibility}:\ \  &amp; g_i(x^\star) \leq 0,\forall i=1,\dots,m; h_j(x^\star) = 0, \forall j=1,\dots,p \\
\text{dual feasibility}:\ \  &amp; \lambda_i^\star \geq 0, \forall i=1,\dots,m \\
\text{stationarity}:\ \  &amp; \nabla_x L(x^\star,\lambda^\star,\mu^\star) = 0 \\
\text{complementarity}:\ \  &amp; \lambda_i^\star \cdot g_i(x^\star) = 0, \forall i=1,\dots,m.
\end{split}
\tag{1.11}
\end{equation}\]</span></p>
<p>Under certain constraint qualifications, the KKT conditions are necessary for local optimality.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:KKTNecessary" class="theorem"><strong>Theorem 1.6  (Necessary Optimality Conditions) </strong></span>Assume any of the following constraint qualifications hold:</p>
<ul>
<li><p>The gradients of the constraints <span class="math inline">\(\{ \nabla g_i(x^\star) \}_{i=1}^m\)</span>, <span class="math inline">\(\{ \nabla h_j(x^\star) \}_{j=1}^p\)</span> are linearly independent.</p></li>
<li><p>Slater’s constraint qualification (cf. Definition <a href="background.html#def:SlaterCQ">1.7</a>).</p></li>
<li><p>All constraints <span class="math inline">\(g_i(x)\)</span> and <span class="math inline">\(h_j(x)\)</span> are affine functions.</p></li>
</ul>
<p>Then, at every local minimum <span class="math inline">\(x^\star\)</span> of <a href="background.html#eq:background-nlp">(1.7)</a>, the KKT conditions <a href="background.html#eq:KKT-conditions">(1.11)</a> hold.</p>
</div>
</div>
<p>On the other hand, for convex optimization problems, the KKT conditions are sufficient for global optimality.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:KKTSufficient" class="theorem"><strong>Theorem 1.7  (Sufficient Optimality Conditions) </strong></span>Assume optimization <a href="background.html#eq:background-nlp">(1.7)</a> is convex, i.e., <span class="math inline">\(f,g_i\)</span> are convex and <span class="math inline">\(h_j\)</span> are affine. Every point <span class="math inline">\(x^\star\)</span> that satisfies the KKT conditions <a href="background.html#eq:KKT-conditions">(1.11)</a> is a global minimizer.</p>
</div>
</div>
</div>
</div>
<div id="background:linear:optimization" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Linear Optimization<a href="background.html#background:linear:optimization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="polyhedra" class="section level3 hasAnchor" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Polyhedra<a href="background.html#polyhedra" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <span class="math inline">\(\mathbb{R}^{n}\)</span>, a <strong>polyhedron</strong> is a set defined by finitely many linear inequalities, i.e.,
<span class="math display" id="eq:polyhedron">\[\begin{equation}
P = \{ x \in \mathbb{R}^{n} \mid A x \geq b \},
\tag{1.12}
\end{equation}\]</span>
for some matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> and <span class="math inline">\(b \in \mathbb{R}^{m}\)</span>. In <a href="background.html#eq:polyhedron">(1.12)</a>, the inequality should be interpreted as <span class="math inline">\(A x - b \in \mathbb{R}^{m}_{+}\)</span>, i.e., every entry of <span class="math inline">\(Ax\)</span> is no smaller than the corresponding entry of <span class="math inline">\(b\)</span>.</p>
<p>The convex hull of finitely many points in <span class="math inline">\(\mathbb{R}^{n}\)</span> is called a <strong>polytope</strong>, where the convex hull of a set <span class="math inline">\(S\)</span> is defined as
<span class="math display" id="eq:convex-hull">\[\begin{equation}
\hspace{-10mm} \mathrm{conv}(S) =  \left\{ \sum_{i=1}^k \lambda_i u_i \mid k \in \mathbb{N}_{+}, \sum_{i=1}^k \lambda_i = 1, \lambda_i \geq 0,i=1,\dots,k, u_i \in S, \forall i =1,\dots,k \right\} ,
\tag{1.13}
\end{equation}\]</span>
i.e., all possible convex combinations of points in <span class="math inline">\(S\)</span>. Clearly, a polytope is bounded.</p>
<p>The conic hull of finitely many points in <span class="math inline">\(\mathbb{R}^{n}\)</span> is called a <strong>polyhedral cone</strong>, where the conic hull of a set <span class="math inline">\(S\)</span> is defined as
<span class="math display" id="eq:conic-hull">\[\begin{equation}
\hspace{-10mm} \mathrm{cone}(S) =  \left\{  \sum_{i=1}^k \lambda_i u_i \mid k \in \mathbb{N}_{+}, \lambda_i \geq 0,i=1,\dots,k, u_i \in S, \forall i =1,\dots,k   \right\} .
\tag{1.14}
\end{equation}\]</span>
The only difference between <a href="background.html#eq:conic-hull">(1.14)</a> and <a href="background.html#eq:convex-hull">(1.13)</a> is the removal of <span class="math inline">\(\sum_{i} \lambda_i = 1\)</span>. Clearly, the origin belongs to the conic hull of any nonempty set, and the conic hull of any nonempty set is unbounded.</p>
<p>The next theorem characterizes a polyhedron.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:DecomposePolyhedron" class="theorem"><strong>Theorem 1.8  (Polyhedron Decomposition) </strong></span>Every polyhedron <span class="math inline">\(P\)</span> is finitely generated, i.e., it can be written as the Minkowski sum of a polytope and a polyhedral cone:
<span class="math display">\[
P = \mathrm{conv}(u_1,\dots,u_r) + \mathrm{cone}(v_1,\dots,v_s),
\]</span>
where the Minkowski sum of two sets is defined as <span class="math inline">\(X + Y := \{ x+y \mid x \in X, y \in Y \}\)</span>.</p>
<p>Further, a bounded polyhedron is a polytope.</p>
</div>
</div>
<p>An extreme point of a polytope is called a <strong>vertex</strong>. A <span class="math inline">\(1\)</span>-dimensional face of a polytope is called an <strong>edge</strong>. A <span class="math inline">\(d-1\)</span>-dimensional face of a <span class="math inline">\(d\)</span>-dimensional polytope is called a <strong>facet</strong>.</p>
</div>
<div id="linear-program" class="section level3 hasAnchor" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Linear Program<a href="background.html#linear-program" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will now give a brief review of important results in linear programming (LP). The standard reference for linear programming is <span class="citation">(<a href="#ref-bertsimas97book-lp" role="doc-biblioref">Bertsimas and Tsitsiklis 1997</a>)</span>. In some sense, the theory of semidefinite programming (SDP) has been developed in order to generalize those of LP to the setup where the decision variable becomes a symmetric matrix and the inequality is interpreted as being positive semidefinite.</p>
<p>A standard form linear program (LP) reads
<span class="math display" id="eq:primal-lp">\[\begin{equation}
\begin{split}
\min_{x \in \mathbb{R}^{n}} &amp; \quad \langle c, x \rangle  \\
\mathrm{s.t.}&amp; \quad Ax = b \\
&amp; \quad x \geq 0
\end{split}
\tag{1.15}
\end{equation}\]</span>
for given <span class="math inline">\(A \in \mathbb{R}^{m\times n}\)</span>, <span class="math inline">\(b \in \mathbb{R}^{m}\)</span>, and <span class="math inline">\(c \in \mathbb{R}^{n}\)</span>. Often the tuple <span class="math inline">\((A,b,c)\)</span> is called the <em>problem data</em> because the LP <a href="background.html#eq:primal-lp">(1.15)</a> is fully defined once the tuple is given (indeed many LP numerical solvers take the tuple <span class="math inline">\((A,b,c)\)</span> as input). Clearly, the feasible set of the LP <a href="background.html#eq:primal-lp">(1.15)</a> is a polyhedron. The LP <a href="background.html#eq:primal-lp">(1.15)</a> is often referred to as the <strong>primal</strong> LP. Associated with <a href="background.html#eq:primal-lp">(1.15)</a> is the following <strong>dual</strong> LP
<span class="math display" id="eq:dual-lp">\[\begin{equation}
\begin{split}
\max_{y \in \mathbb{R}^{m}} &amp; \quad \langle b, y \rangle \\
\mathrm{s.t.}&amp; \quad c - A^\top y \geq 0
\end{split}
\tag{1.16}
\end{equation}\]</span>
It is worth noting that the dimension of the dual variable <span class="math inline">\(y\)</span> is exactly the number of constraints in the primal LP.</p>
<p><strong>Lagrangian duality</strong>. Let us use the idea of Lagrangian duality introduced in Section <a href="background.html#background:convex:optimization:Lagrangian">1.3.2</a> to verify that <a href="background.html#eq:dual-lp">(1.16)</a> is indeed the Lagrangian dual problem of <a href="background.html#eq:primal-lp">(1.15)</a>. The Lagrangian associated with <a href="background.html#eq:primal-lp">(1.15)</a> is
<span class="math display">\[\begin{equation}
\begin{split}
L(x,\lambda,\mu) &amp; = \langle c, x \rangle + \langle \mu, Ax - b \rangle + \langle \lambda, -x \rangle, \quad \mu \in \mathbb{R}^{m}, \lambda \in \mathbb{R}^{n}_{+}\\
&amp; = \langle c + A^\top\mu - \lambda, x \rangle - \langle \mu, b \rangle, \quad \mu \in \mathbb{R}^{m}, \lambda \in \mathbb{R}^{n}_{+}.
\end{split}
\end{equation}\]</span>
The Lagrangian dual function is therefore
<span class="math display">\[
\phi(\lambda,\mu) = \min_{x} L(x,\lambda,\mu) = \begin{cases}
- \langle \mu, b \rangle &amp; \text{if } c + A^\top\mu - \lambda = 0 \\
- \infty &amp; \text{Otherwise}
\end{cases}, \mu \in \mathbb{R}^{m}, \lambda \in \mathbb{R}^{n}_{+}.
\]</span>
The Lagrangian dual problem seeks to maximize the dual function <span class="math inline">\(\phi(\lambda,\mu)\)</span>, and hence it must set <span class="math inline">\(c + A^\top\mu - \lambda = 0\)</span> (otherwise it leads to <span class="math inline">\(-\infty\)</span>). As a result, the dual problem is
<span class="math display" id="eq:lp-Lagrangian-dual">\[\begin{equation}
\begin{split}
\max_{\mu \in \mathbb{R}^{m}} &amp; \quad \langle b, -\mu \rangle \\
\mathrm{s.t.}&amp; \quad c + A^\top\mu = \lambda \geq 0
\end{split}
\tag{1.17}
\end{equation}\]</span>
With a change of variable <span class="math inline">\(y := -\mu\)</span>, we observe that problem <a href="background.html#eq:lp-Lagrangian-dual">(1.17)</a> is precisely problem <a href="background.html#eq:dual-lp">(1.16)</a>.</p>
<p><strong>Weak duality</strong>. For the pair of primal-dual LPs, it is easy to verify that, for any <span class="math inline">\(x\)</span> that is feasible for the primal <a href="background.html#eq:primal-lp">(1.15)</a> and <span class="math inline">\(y\)</span> that is feasible for the dual <a href="background.html#eq:dual-lp">(1.16)</a>, we have
<span class="math display" id="eq:weak-duality-lp">\[\begin{equation}
\langle c, x \rangle - \langle b, y \rangle = \langle c, x \rangle - \langle Ax, y \rangle = \langle c, x \rangle - \langle A^\top y, x \rangle = \langle c - A^\top y, x \rangle \geq 0.
\tag{1.18}
\end{equation}\]</span>
Therefore, denoting <span class="math inline">\(p^\star\)</span> as the optimum of <a href="background.html#eq:primal-lp">(1.15)</a> and <span class="math inline">\(d^\star\)</span> as the optimum of <a href="background.html#eq:dual-lp">(1.16)</a>, we have the weak duality
<span class="math display">\[
p^\star \geq d^\star.
\]</span>
Note that such weak duality can also be directly obtained since <a href="background.html#eq:lp-Lagrangian-dual">(1.17)</a> is the Lagrangian dual of <a href="background.html#eq:primal-lp">(1.15)</a>.</p>
<p>If <span class="math inline">\(p^\star = d^\star\)</span>, then we say <strong>strong duality</strong> holds. The LP <a href="background.html#eq:primal-lp">(1.15)</a> is said to be <strong>feasible</strong> if its feasible set is nonempty. It is said to be <strong>unbounded below</strong> if there exists a sequence <span class="math inline">\(\{ u_i \}_{i=1}^{\infty} \subseteq \mathbb{R}^{n}_{+}\)</span> such that <span class="math inline">\(\langle c, u_i \rangle \rightarrow -\infty\)</span> and <span class="math inline">\(A u_i = b\)</span>. If the primal <a href="background.html#eq:primal-lp">(1.15)</a> is infeasible (resp. unbounded below), we set <span class="math inline">\(p^\star = + \infty\)</span> (resp. <span class="math inline">\(p^\star = - \infty\)</span>). Similar characteristics are defined for the dual LP <a href="background.html#eq:dual-lp">(1.16)</a>. In particular, if the dual <a href="background.html#eq:dual-lp">(1.16)</a> is unbounded, then we set <span class="math inline">\(d^\star = + \infty\)</span>. If the dual is infeasible, then we set <span class="math inline">\(d^\star = - \infty\)</span>.</p>
<p>Strong duality is well understood in linear programming.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:LPStrongDuality" class="theorem"><strong>Theorem 1.9  (LP Strong Duality) </strong></span>For the LP primal-dual pair <a href="background.html#eq:primal-lp">(1.15)</a> and <a href="background.html#eq:dual-lp">(1.16)</a>, we have</p>
<ul>
<li><p>If one of <a href="background.html#eq:primal-lp">(1.15)</a> and <a href="background.html#eq:dual-lp">(1.16)</a> is feasible, then <span class="math inline">\(p^\star = d^\star\)</span> (i.e., finite, <span class="math inline">\(+\infty\)</span>, or <span class="math inline">\(-\infty\)</span>).</p></li>
<li><p>If one of <span class="math inline">\(p^\star\)</span> or <span class="math inline">\(d^\star\)</span> is finite, then <span class="math inline">\(p^\star = d^\star\)</span> is finite, and both <a href="background.html#eq:primal-lp">(1.15)</a> and <a href="background.html#eq:dual-lp">(1.16)</a> achieve the same optimal value (i.e., they botb have optimizers).</p></li>
<li><p>A primal feasible point <span class="math inline">\(x^\star\)</span> of <a href="background.html#eq:primal-lp">(1.15)</a> is a minimizer if and only if there exists a dual feasible point <span class="math inline">\(y^\star\)</span> such that <span class="math inline">\(\langle c, x^\star \rangle = \langle b, y^\star \rangle\)</span>.</p></li>
</ul>
</div>
</div>
<p>For example, consider the following primal-dual LP pair
<span class="math display">\[\begin{equation}
\begin{cases}
\min_{x \in \mathbb{R}^{3}_{+}} &amp; x_1 + x_2 + 2 x_3 \\
\mathrm{s.t.}&amp; \begin{bmatrix} -1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 2 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}
\end{cases},
\begin{cases}
\max_{y \in \mathbb{R}^{2}} &amp; y_2 \\
\mathrm{s.t.}&amp; \begin{bmatrix} 1 \\ 1 \\ 2 \end{bmatrix} - \begin{bmatrix} -1 &amp; 1 \\ 1 &amp; 1 \\ 1 &amp; 2 \end{bmatrix} \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} \geq 0
\end{cases}.
\end{equation}\]</span>
<span class="math inline">\(x^\star = [1/2,1/2,0]^\top\)</span> is feasible for the primal and attains <span class="math inline">\(p^\star = 1\)</span>. <span class="math inline">\(y^\star = [0,1]^\top\)</span> is feasible for the dual and attains <span class="math inline">\(d^\star = 1\)</span>. Therefore, both <span class="math inline">\(x^\star\)</span> and <span class="math inline">\(y^\star\)</span> are optimizers for the primal and dual, respectively.</p>
<p><strong>Complementary slackness</strong>. Strong duality, when combined with <a href="background.html#eq:weak-duality-lp">(1.18)</a>, implies that
<span class="math display">\[
x_i^\star (c - A^\top y^\star)_i = 0, \forall i = 1,\dots,n,
\]</span>
where <span class="math inline">\((\cdot)_i\)</span> denotes the <span class="math inline">\(i\)</span>-th entry of a vector. This is known as complementary slackness, which states that whenever a primal optimal solution has a nonzero entry, the corresponding dual inequality must be tight.</p>
<p>An important property of LP is that if the primal problem is feasible and bounded below, then it must have an optimizer that is a <strong>basic feasible point</strong>, i.e., a feasible point has at most <span class="math inline">\(m\)</span> nonzero entries. The simplex method <span class="citation">(<a href="#ref-bertsimas97book-lp" role="doc-biblioref">Bertsimas and Tsitsiklis 1997</a>)</span> for solving LPs searches for optimizers among the basic feasible points.</p>
<p>We also introduce how to detect infeasibility and unboundedness of LPs.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:LPInfeasUnbound" class="theorem"><strong>Theorem 1.10  (LP Infeasibility and Unboundedness) </strong></span>Infeasibility and Unboundedness of LP can be certified by existence of an improving/decreasing ray for the primal and dual:</p>
<ul>
<li><p>When the primal <a href="background.html#eq:primal-lp">(1.15)</a> is feasible, it is unbounded below if and only if it has a decreasing ray, i.e., there exists <span class="math inline">\(u \in \mathbb{R}^{n}\)</span> such that
<span class="math display">\[
A u = 0, \quad u \geq 0, \quad \langle c, u \rangle &lt; 0.
\]</span></p></li>
<li><p>When the dual <a href="background.html#eq:dual-lp">(1.16)</a> is feasible, it is unbounded above if and only if it has an improving ray, i.e., there exists <span class="math inline">\(u \in \mathbb{R}^{m}\)</span> such that
<span class="math display">\[
A^\top u \leq 0, \quad \langle b, u \rangle &gt; 0.
\]</span></p></li>
<li><p>The primal problem <a href="background.html#eq:primal-lp">(1.15)</a> is infeasible if and only if the dual problem <a href="background.html#eq:dual-lp">(1.16)</a> has an improving ray, i.e., there exists <span class="math inline">\(u \in \mathbb{R}^{m}\)</span> such that
<span class="math display">\[
A^\top u \leq 0, \quad \langle b, u \rangle &gt; 0.
\]</span></p></li>
<li><p>The dual problem <a href="background.html#eq:dual-lp">(1.16)</a> is infeasible if and only if the primal problem <a href="background.html#eq:primal-lp">(1.15)</a> has a decreasing ray, i.e., there exists <span class="math inline">\(u \in \mathbb{R}^{n}\)</span> such that
<span class="math display">\[
A u = 0, \quad u \geq 0, \quad \langle c, u \rangle &lt; 0.
\]</span></p></li>
</ul>
</div>
</div>
<p>It is important to note that both the primal and dual can be infeasible, as in the following example.
<span class="math display">\[\begin{equation}
\begin{cases}
\min_{x \in \mathbb{R}^{2}_{+}} &amp; - x_1 - x_2 \\
\mathrm{s.t.}&amp; \begin{bmatrix} -1 &amp; 1 \\ -1 &amp; 1 \end{bmatrix} x = \begin{bmatrix} 2 \\ 3 \end{bmatrix}
\end{cases},
\begin{cases}
\max_{y \in \mathbb{R}^{2}} &amp; 2 y_1 + 3 y_2 \\
\mathrm{s.t.}&amp; \begin{bmatrix} -1 \\ -1 \end{bmatrix} - \begin{bmatrix} -1 &amp; -1 \\ 1 &amp; 1 \end{bmatrix} y \geq 0
\end{cases}.
\end{equation}\]</span></p>
</div>
<div id="farkas-lemma" class="section level3 hasAnchor" number="1.4.3">
<h3><span class="header-section-number">1.4.3</span> Farkas Lemma<a href="background.html#farkas-lemma" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A foundational result in linear programming is the Farkas Lemma.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:FarkasLemma" class="theorem"><strong>Theorem 1.11  (Farkas Lemma) </strong></span>For a given <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> and <span class="math inline">\(c \in \mathbb{R}^{n}\)</span>, if <span class="math inline">\(\langle c, x \rangle \geq 0\)</span> for all <span class="math inline">\(x\)</span> satisfying <span class="math inline">\(Ax \geq 0\)</span>, then there exists <span class="math inline">\(\lambda \in \mathbb{R}^{m}\)</span> such that
<span class="math display">\[
c = A^\top\lambda, \quad \lambda \geq 0.
\]</span></p>
</div>
</div>
<p>As a simple example, take <span class="math inline">\(A = \mathrm{I}_n\)</span> as the identity matrix, then Farkas Lemma says if <span class="math inline">\(\langle c, x \rangle \geq 0\)</span> for all <span class="math inline">\(x \geq 0\)</span>, then <span class="math inline">\(c\)</span> must be that <span class="math inline">\(c \geq 0\)</span> – this is exactly the fact that the nonnegative orthant <span class="math inline">\(\mathbb{R}^{n}_{+}\)</span> is self-dual.</p>
<p>In general, the Farkas Lemma states if the linear function <span class="math inline">\(\langle c, x \rangle\)</span> is nonnegative on the space <span class="math inline">\(\{ Ax \geq 0 \}\)</span>, then there exists <span class="math inline">\(\lambda \in \mathbb{R}^{m}_{+}\)</span> such that
<span class="math display" id="eq:farkas-lemma-imply">\[\begin{equation}
\langle c, x \rangle = \langle \lambda,  Ax \rangle = \sum_{i=1}^m \lambda_i (a_i^\top x),
\tag{1.19}
\end{equation}\]</span>
where <span class="math inline">\(a_i^\top\)</span> is the <span class="math inline">\(i\)</span>-th row of <span class="math inline">\(A\)</span>. Note that <a href="background.html#eq:farkas-lemma-imply">(1.19)</a> is a polynomial identity. As we will see later in the course, the idea of sums of squares (SOS), to some extent, is to generalize Farkas Lemma to the case where the function is a polynomial and the set is a basic semialgebraic set (i.e., defined by polynomial equalities and inequalities).</p>
<p>A generalization of Farkas Lemma to inhomogeneous affine functions is stated below.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:InhomogeneousFarkasLemma" class="theorem"><strong>Theorem 1.12  (Inhomogeneous Farkas Lemma) </strong></span>Suppose the set <span class="math inline">\(P = \{ x \in \mathbb{R}^{n} \mid A x \geq b \}\)</span> with <span class="math inline">\(A \in \mathbb{R}^{m \times n}, b \in \mathbb{R}^{m}\)</span> is nonempty. If a linear function <span class="math inline">\(\langle c, x \rangle - d\)</span> is nonnegative on <span class="math inline">\(P\)</span>, then there exists <span class="math inline">\(\lambda \in \mathbb{R}^{m}\)</span> and <span class="math inline">\(\nu \in \mathbb{R}^{}\)</span> such that
<span class="math display">\[
\langle c, x \rangle - d = \nu + \langle \lambda, A x - b \rangle, \quad \lambda \geq 0, \nu \geq 0.
\]</span></p>
</div>
</div>
<p>A more general result is called the Theorem of Alternatives, which states that a polyhedral set is empty if and only if another polyhedral set is nonempty.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:Alternative" class="theorem"><strong>Theorem 1.13  (Theorem of Alternatives) </strong></span>Given <span class="math inline">\(A_1 \in \mathbb{R}^{m_1 \times n}, A_2 \in \mathbb{R}^{m_2 \times n}\)</span>, <span class="math inline">\(b_1 \in \mathbb{R}^{m_1}\)</span>, and <span class="math inline">\(b_2 \in \mathbb{R}^{m_2}\)</span>, the set
<span class="math display">\[
\{ x \in \mathbb{R}^{n} \mid A_1 x &gt; b_1, A_2 x \geq b_2 \}
\]</span>
is empty if and only if the following set
<span class="math display">\[
\left\{ (\lambda_1,\lambda_2) \in \mathbb{R}^{m_1} \times \mathbb{R}^{m_2}\ \middle\vert\ \begin{array}{r} \lambda_1 \geq 0, \lambda_2 \geq 0, \\ b_1^\top\lambda_1 + b_2^\top\lambda_2 \geq 0, \\ A_1^\top\lambda_1 + A_2^\top\lambda_2 = 0, \\ (e + b_1)^\top\lambda_1 + b_2^\top\lambda_2 = 1 \end{array}  \right\}
\]</span>
is nonempty, with <span class="math inline">\(e\)</span> being the vector of all ones.</p>
</div>
</div>

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bertsekas03book-convex" class="csl-entry">
Bertsekas, Dimitri, Angelia Nedic, and Asuman Ozdaglar. 2003. <em>Convex Analysis and Optimization</em>. Vol. 1. Athena Scientific.
</div>
<div id="ref-bertsimas97book-lp" class="csl-entry">
Bertsimas, Dimitris, and John N Tsitsiklis. 1997. <em>Introduction to Linear Optimization</em>. Vol. 6. Athena scientific Belmont, MA.
</div>
<div id="ref-boyd04book-convex" class="csl-entry">
Boyd, Stephen P, and Lieven Vandenberghe. 2004. <em>Convex Optimization</em>. Cambridge university press.
</div>
<div id="ref-rockafellar70-convexanalysis" class="csl-entry">
Rockafellar, Ralph Tyrell. 1970. <em>Convex Analysis</em>. Princeton university press.
</div>
<div id="ref-tang23arxiv-uncertainty" class="csl-entry">
Tang, Yukai, Jean-Bernard Lasserre, and Heng Yang. 2023. <span>“Uncertainty Quantification of Set-Membership Estimation in Control and Perception: Revisiting the Minimum Enclosing Ellipsoid.”</span> <em>arXiv Preprint arXiv:2311.15962</em>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>The closure of a subset <span class="math inline">\(C\)</span> of points, denoted <span class="math inline">\(\mathrm{cl}(C)\)</span>, consists of all points in <span class="math inline">\(C\)</span> together with all limit points of <span class="math inline">\(C\)</span>. The closure of <span class="math inline">\(C\)</span> may equivalently be defined as the intersection of all closed sets containing <span class="math inline">\(C\)</span>. Intuitively, the closure can be thought of as all the points that are either in <span class="math inline">\(C\)</span> or “very near” <span class="math inline">\(C\)</span>. For example, the closure of the open line segment <span class="math inline">\(C= (0,1)\)</span> is the closed line segment <span class="math inline">\(C=[0,1]\)</span>.<a href="background.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Some authors define a cone using nonnegative scalings.<a href="background.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://semidefinite.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="notation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sdp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/hankyang94/Semidefinite/blob/main/01-background.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["semidefinite-optimization-relaxation.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
