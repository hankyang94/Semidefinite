<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Mathematical Background | Semidefinite Optimization and Relaxation</title>
  <meta name="description" content="Lecture notes for Harvard ES 257 Semidefinite Optimization and Relaxation." />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Mathematical Background | Semidefinite Optimization and Relaxation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES 257 Semidefinite Optimization and Relaxation." />
  <meta name="github-repo" content="hankyang94/Semidefinite" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Mathematical Background | Semidefinite Optimization and Relaxation" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES 257 Semidefinite Optimization and Relaxation." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2024-01-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="notation.html"/>
<link rel="next" href="sdp.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Semidefinite Optimization and Relaxation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#offerings"><i class="fa fa-check"></i>Offerings</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="notation.html"><a href="notation.html"><i class="fa fa-check"></i>Notation</a></li>
<li class="chapter" data-level="1" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>1</b> Mathematical Background</a>
<ul>
<li class="chapter" data-level="1.1" data-path="background.html"><a href="background.html#background:convexity"><i class="fa fa-check"></i><b>1.1</b> Convexity</a></li>
<li class="chapter" data-level="1.2" data-path="background.html"><a href="background.html#background:convex:geometry"><i class="fa fa-check"></i><b>1.2</b> Convex Geometry</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="background.html"><a href="background.html#basic-facts"><i class="fa fa-check"></i><b>1.2.1</b> Basic Facts</a></li>
<li class="chapter" data-level="1.2.2" data-path="background.html"><a href="background.html#cones-duality-polarity"><i class="fa fa-check"></i><b>1.2.2</b> Cones, Duality, Polarity</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="background.html"><a href="background.html#background:convex:optimization"><i class="fa fa-check"></i><b>1.3</b> Convex Optimization</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="background.html"><a href="background.html#minimax-theorem"><i class="fa fa-check"></i><b>1.3.1</b> Minimax Theorem</a></li>
<li class="chapter" data-level="1.3.2" data-path="background.html"><a href="background.html#lagrangian-duality"><i class="fa fa-check"></i><b>1.3.2</b> Lagrangian Duality</a></li>
<li class="chapter" data-level="1.3.3" data-path="background.html"><a href="background.html#kkt-optimality-conditions"><i class="fa fa-check"></i><b>1.3.3</b> KKT Optimality Conditions</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="background.html"><a href="background.html#background:linear:optimization"><i class="fa fa-check"></i><b>1.4</b> Linear Optimization</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sdp.html"><a href="sdp.html"><i class="fa fa-check"></i><b>2</b> Semidefinite Optimization</a></li>
<li class="chapter" data-level="3" data-path="psets.html"><a href="psets.html"><i class="fa fa-check"></i><b>3</b> Problem Sets</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Semidefinite Optimization and Relaxation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="background" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Mathematical Background<a href="background.html#background" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="background:convexity" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Convexity<a href="background.html#background:convexity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A very important notion in modern optimization is that of <em>convexity</em>. To a large extent, an optimization problem is “easy” if it is convex, and “difficult” when convexity is lost, i.e., <em>nonconvex</em>. We give a basic review of convexity here and refer the reader to <span class="citation">(<a href="#ref-rockafellar70-convexanalysis" role="doc-biblioref">Rockafellar 1970</a>)</span>, <span class="citation">(<a href="#ref-boyd04book-convex" role="doc-biblioref">Boyd and Vandenberghe 2004</a>)</span>, and <span class="citation">(<a href="#ref-bertsekas03book-convex" role="doc-biblioref">Bertsekas, Nedic, and Ozdaglar 2003</a>)</span> for comprehensive treatments.</p>
<p>We will work on a finite-dimensional real vector space, which we will identify with <span class="math inline">\(\mathbb{R}^{n}\)</span>.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:ConvexSet" class="definition"><strong>Definition 1.1  (Convex Set) </strong></span>A set <span class="math inline">\(S\)</span> is convex if <span class="math inline">\(x_1,x_2 \in S\)</span> implies <span class="math inline">\(\lambda x_1 + (1-\lambda) x_2 \in S\)</span> for any <span class="math inline">\(\lambda \in [0,1]\)</span>. In other words, if <span class="math inline">\(x_1,x_2 \in S\)</span>, then the line segment connecting <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> lies inside <span class="math inline">\(S\)</span>.</p>
</div>
</div>
<p>Conversely, a set <span class="math inline">\(S\)</span> is nonconvex if Definition <a href="background.html#def:ConvexSet">1.1</a> does not hold.</p>
<p>Given <span class="math inline">\(x_1, x_2 \in S\)</span>, <span class="math inline">\(\lambda x_1 + (1-\lambda) x_2\)</span> is called a <em>convex combination</em> when <span class="math inline">\(\lambda \in [0,1]\)</span>. For convenience, we will use the following notation
<span class="math display">\[\begin{equation}
\begin{split}
(x_1,x_2) = \{ \lambda x_1 + (1-\lambda) x_2 \mid \lambda \in (0,1) \}, \\ [x_1,x_2] = \{ \lambda x_1 + (1-\lambda) x_2 \mid \lambda \in [0,1] \}.
\end{split}
\end{equation}\]</span></p>
<p>A <strong>hyperplane</strong> is a common convex set defined as
<span class="math display" id="eq:hyperplane">\[\begin{equation}
H = \{  x \in \mathbb{R}^{n} \mid \langle c, x \rangle = d  \}
\tag{1.1}
\end{equation}\]</span>
for some <span class="math inline">\(c \in \mathbb{R}^{n}\)</span> and scalar <span class="math inline">\(d\)</span>. A <strong>halfspace</strong> is a convex set defined as
<span class="math display" id="eq:halfspace">\[\begin{equation}
H^{+} = \{  x \in \mathbb{R}^{n} \mid \langle c, x \rangle \geq d  \}.
\tag{1.2}
\end{equation}\]</span></p>
<p>Given two nonempty convex sets <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span>, the <strong>distance</strong> between <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> is defined as
<span class="math display">\[\begin{equation}
\mathrm{dist}(C_1,C_2) = \inf \{ \Vert c_1 - c_2 \Vert \mid c_1 \in C_1, c_2 \in C_2 \}.
\end{equation}\]</span></p>
<p>For a convex set <span class="math inline">\(C\)</span>, the hyperplane <span class="math inline">\(H\)</span> in <a href="background.html#eq:hyperplane">(1.1)</a> is called a <strong>supporting hyperplane</strong> for <span class="math inline">\(C\)</span> if <span class="math inline">\(C\)</span> is contained in the half space <span class="math inline">\(H^{+}\)</span> and the distance between <span class="math inline">\(H\)</span> and <span class="math inline">\(C\)</span> is zero. For example, the hyperplane <span class="math inline">\(x_1 = 0\)</span> is supporting for the hyperboloid <span class="math inline">\(\{ (x_1,x_2) \mid x_1 x_2 \geq 1, x_1 \geq 0, x_2 \geq 0 \}\)</span> in <span class="math inline">\(\mathbb{R}^{2}\)</span>.</p>
<p>An important property of a convex set is that we can <em>certify</em> when a point is not in the set. This is usually done via a separation theorem.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:SeparationTheorem" class="theorem"><strong>Theorem 1.1  (Separation Theorem) </strong></span>Let <span class="math inline">\(S_1,S_2\)</span> be two convex sets in <span class="math inline">\(\mathbb{R}^{n}\)</span> and <span class="math inline">\(S_1 \cap S_2 = \emptyset\)</span>, then there exists a hyperplane that separates <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span>, i.e., there exists <span class="math inline">\(c\)</span> and <span class="math inline">\(d\)</span> such that
<span class="math display" id="eq:separation">\[\begin{equation}
\begin{split}
\langle c, x \rangle \geq d, &amp;  \forall x \in S_1,\\
\langle c, x \rangle \leq d, &amp; \forall x \in S_2.
\end{split}
\tag{1.3}
\end{equation}\]</span>
Further, if <span class="math inline">\(S_1\)</span> is compact (i.e., closed and bounded) and <span class="math inline">\(S_2\)</span> is closed, then the separation is strict, i.e., the inequalities in <a href="background.html#eq:separation">(1.3)</a> are strict.</p>
</div>
</div>
<p>The strict separation theorem is used typically when <span class="math inline">\(S_1\)</span> is a single point (hence compact).</p>
<p>We will see a generalization of the separation theorem for nonconvex sets later after we introduce the idea of sums of squares.</p>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-1" class="exercise"><strong>Exercise 1.1  </strong></span>Provide examples of two disjoint convex sets such that the separation in <a href="background.html#eq:separation">(1.3)</a> is not strict in one way and both ways.</p>
</div>
</div>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-2" class="exercise"><strong>Exercise 1.2  </strong></span>Provide a constructive proof that the separation hyperplane exists in Theorem <a href="background.html#thm:SeparationTheorem">1.1</a> when (1) both <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> are closed, and (2) at least one of them is bounded.</p>
</div>
</div>
<p>The intersection of convex sets is always convex (try to prove this).</p>
</div>
<div id="background:convex:geometry" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Convex Geometry<a href="background.html#background:convex:geometry" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="basic-facts" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Basic Facts<a href="background.html#basic-facts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given a set <span class="math inline">\(S\)</span>, its <strong>affine hull</strong> is the set
<span class="math display">\[
\mathrm{aff}(S) =  \left\{  \sum_{i=1}^k \lambda_i u_i \mid \lambda_1 + \dots + \lambda_k = 1, u_i \in S, k \in \mathbb{N}_{+}  \right\} ,
\]</span>
where <span class="math inline">\(\sum_{i=1}^{k} \lambda_i u_i\)</span> is called an <em>affine combination</em> of <span class="math inline">\(u_1,\dots,u_k\)</span> when <span class="math inline">\(\sum_i \lambda_i = 1\)</span>. The affine hull of the emptyset is the emptyset, of a singleton is the singleton itself. The affine hull of a set of two different points is the line going through them. The affine hull of a set of three points not on one line is the plane going through them. The affine hull of a set of four points not in a plane in <span class="math inline">\(\mathbb{R}^{3}\)</span> is the entire space <span class="math inline">\(\mathbb{R}^{3}\)</span>.</p>
<p>For a convex set <span class="math inline">\(C \subseteq \mathbb{R}^{n}\)</span>, the <strong>interior</strong> of <span class="math inline">\(C\)</span> is defined as
<span class="math display">\[
\mathrm{int}(C) := \{  u \in C \mid \exists \epsilon &gt; 0, B(u,\epsilon) \subseteq C  \},
\]</span>
where <span class="math inline">\(B(u,\epsilon)\)</span> denotes a ball centered at <span class="math inline">\(u\)</span> with radius <span class="math inline">\(\epsilon\)</span> (using the usual 2-norm). Each point in <span class="math inline">\(\mathrm{int}(C)\)</span> is called an <em>interior point</em> of <span class="math inline">\(C\)</span>. If <span class="math inline">\(\mathrm{int}(C) = C\)</span>, then <span class="math inline">\(C\)</span> is said to be an <strong>open set</strong>. A convex set with nonempty interior is called a <strong>convex domain</strong>, while a compact (i.e., closed and bounded) convex domain is called a <strong>convex body</strong>.</p>
<p>The <strong>boundary of <span class="math inline">\(C\)</span></strong> is the subset of points that are not in the interior of <span class="math inline">\(C\)</span> and we denote it as <span class="math inline">\(\partial C\)</span>.</p>
<p>It is possible that a convex set has empty interior. For example, a hyperplane has no interior, and neither does a singleton. In such cases, the <strong>relative interior</strong> can be defined as
<span class="math display">\[
\mathrm{ri}(C) := \{  u \in C \mid \exists \epsilon &gt; 0, B(u,\epsilon) \cap \mathrm{aff}(C) \subseteq C  \}.
\]</span>
For a nonempty convex set, the relative interior always exists. If <span class="math inline">\(\mathrm{ri}(C) = C\)</span>, then <span class="math inline">\(C\)</span> is said to be <strong>relatively open</strong>. For example, the relative interior of a singleton is the singleton itself, and hence a singleton is relatively open.</p>
<p>For a convex set <span class="math inline">\(C\)</span>, a point <span class="math inline">\(u \in C\)</span> is called an <strong>extreme point</strong> if
<span class="math display">\[
u \in (x,y), x \in C, y \in C \quad \Rightarrow u = x = y.
\]</span>
For example, consider <span class="math inline">\(C = \{ (x,y)\mid x^2 + y^2 \leq 1 \}\)</span>, then all the points on the boundary <span class="math inline">\(\partial C = \{ (x,y) \mid x^2 + y^2 = 1 \}\)</span> are extreme points.</p>
<p>A subset <span class="math inline">\(F \subseteq C\)</span> is called a <strong>face</strong> if <span class="math inline">\(F\)</span> itself is convex and
<span class="math display">\[
u \in (x,y), u \in F, x,y \in C \quad \Rightarrow x,y \in F.
\]</span>
Clearly, the empty set <span class="math inline">\(\emptyset\)</span> and the entire set <span class="math inline">\(C\)</span> are faces of <span class="math inline">\(C\)</span>, which are called <em>trivial faces</em>. The face <span class="math inline">\(F\)</span> is said to be <em>proper</em> if <span class="math inline">\(F \neq C\)</span>. The set of any single extreme point is also a face. A face <span class="math inline">\(F\)</span> of <span class="math inline">\(C\)</span> is called <strong>exposed</strong> if there exists a supporting hyperplane <span class="math inline">\(H\)</span> for <span class="math inline">\(C\)</span> such that
<span class="math display">\[
F = H \cap C.
\]</span></p>
</div>
<div id="cones-duality-polarity" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Cones, Duality, Polarity<a href="background.html#cones-duality-polarity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definitionbox">
<div class="definition">
<p><span id="def:polar" class="definition"><strong>Definition 1.2  (Polar) </strong></span>For a nonempty set <span class="math inline">\(T \subseteq \mathbb{R}^{n}\)</span>, its polar is the set
<span class="math display" id="eq:polar">\[\begin{equation}
T^\circ := \{  y \in \mathbb{R}^{n} \mid \langle x, y \rangle \leq 1, \forall x \in T  \}.
\tag{1.4}
\end{equation}\]</span></p>
</div>
</div>
<p>The polar <span class="math inline">\(T^\circ\)</span> is a closed convex set and contains the origin. Note that <span class="math inline">\(T\)</span> is always contained in the polar of <span class="math inline">\(T^\circ\)</span>, i.e., <span class="math inline">\(T \subseteq (T^\circ)^\circ\)</span>. Indeed, they are equal under some assumptions.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:bipolar" class="theorem"><strong>Theorem 1.2  (Bipolar) </strong></span>If <span class="math inline">\(T \subseteq \mathbb{R}^{n}\)</span> is a closed convex set containing the origin, then <span class="math inline">\((T^\circ)^\circ = T\)</span>.</p>
</div>
</div>
<p>An important class of convex sets are those that are invariant under nonnegative scalings. A set <span class="math inline">\(K \subseteq \mathbb{R}^{n}\)</span> is a <strong>cone</strong> if <span class="math inline">\(t x \in K\)</span> for all <span class="math inline">\(x \in K\)</span> and for all <span class="math inline">\(t &gt; 0\)</span>. For example, the positive real line <span class="math inline">\(\{ x \in \mathbb{R}^{} \mid x &gt; 0 \}\)</span> is a cone. The cone <span class="math inline">\(K\)</span> is <strong>pointed</strong> if <span class="math inline">\(K \cap -K = \{ 0 \}\)</span>. It is said to be <strong>solid</strong> if its interior <span class="math inline">\(\mathrm{int}(K) \neq \emptyset\)</span>. Any nonzero point of a cone cannot be extreme. If a cone is pointed, the only extreme point is the origin.</p>
<p>The analogue of extreme point for convex cones is the <strong>extreme ray</strong>. For a convex cone <span class="math inline">\(K\)</span> and <span class="math inline">\(0 \neq u \in K\)</span>, the line segment
<span class="math display">\[
u \cdot [0,\infty) := \{ tu \mid t\geq 0 \}
\]</span>
is called an extreme ray of <span class="math inline">\(K\)</span> if
<span class="math display">\[
u \in (x,y), x,y \in K \quad \Rightarrow \quad u,x,y \text{ are parallel to each other}.
\]</span>
If <span class="math inline">\(u \cdot [0,\infty)\)</span> is an extreme ray, then we say <span class="math inline">\(u\)</span> generates the extreme ray.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:ProperCone" class="definition"><strong>Definition 1.3  (Proper Cone) </strong></span>A cone <span class="math inline">\(K\)</span> is proper if it is closed, convex, pointed, and solid.</p>
</div>
</div>
<p>A proper cone <span class="math inline">\(K\)</span> induces a <strong>partial order</strong> on the vector space, via <span class="math inline">\(x \succeq y\)</span> if <span class="math inline">\(x - y \in K\)</span>. We also use <span class="math inline">\(x \succ y\)</span> if <span class="math inline">\(x - y\)</span> is in <span class="math inline">\(\mathrm{int}(K)\)</span>. Important examples of proper cones are the nonnegative orthant, the second-order cone, the set of symmetric positive semidefinite matrices, and the set of nonnegative polynomials, which we will describe later in the book.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:Dual" class="definition"><strong>Definition 1.4  (Dual) </strong></span>The dual of a nonempty set <span class="math inline">\(S\)</span> is
<span class="math display">\[
S^* := \{  y \in \mathbb{R}^{n} \mid \langle y, x \rangle \geq 0, \forall x \in S \}.
\]</span></p>
</div>
</div>
<p>Given any set <span class="math inline">\(S\)</span>, its dual <span class="math inline">\(S^*\)</span> is always a closed convex cone. Duality reverses inclusion, that is,
<span class="math display">\[
S_1 \subseteq S_2 \quad \Rightarrow \quad S_1^* \supseteq S_2^*.
\]</span>
If <span class="math inline">\(S\)</span> is a closed convex cone, then <span class="math inline">\(S^{* *}= S\)</span>. Otherwise, <span class="math inline">\(S^{* *}\)</span> is the closure of the smallest convex cone that contains <span class="math inline">\(S\)</span>.</p>
<p>For a cone <span class="math inline">\(K \subseteq \mathbb{R}^{n}\)</span>, one can show that
<span class="math display">\[
K^\circ = \{ y \in \mathbb{R}^{n} \mid \langle x, y \rangle \leq 0, \forall x \in K \}.
\]</span>
The set <span class="math inline">\(K^\circ\)</span> is called the <strong>polar cone</strong> of <span class="math inline">\(K\)</span>. The negative of <span class="math inline">\(K^\circ\)</span> is just the <strong>dual cone</strong>
<span class="math display">\[
K^{*} = \{ y \in \mathbb{R}^{n} \mid \langle x, y \rangle \geq 0, \forall x \in K \}.
\]</span></p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:selfdual" class="definition"><strong>Definition 1.5  (Self-dual) </strong></span>A cone <span class="math inline">\(K\)</span> is self-dual if <span class="math inline">\(K^{*} = K\)</span>.</p>
</div>
</div>
<p>As an easy example, the nonnegative orthant <span class="math inline">\(\mathbb{R}^{n}_{+}\)</span> is self-dual.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:SecondOrderCone" class="example"><strong>Example 1.1  (Second-order Cone) </strong></span>The second-order cone, or the Lorentz cone, or the ice cream cone
<span class="math display">\[
\mathcal{Q}_n := \{  (x_0,x_1,\dots,x_n) \in \mathbb{R}^{n+1} \mid \sqrt{x_1^2 + \dots + x_n^2} \leq x_0  \}
\]</span>
is a proper cone of <span class="math inline">\(\mathbb{R}^{n+1}\)</span>. We will show that it is also self-dual.</p>
<p><strong>Proof</strong>. Consider <span class="math inline">\((y_0,y_1,\dots,y_n) \in \mathcal{Q}_n\)</span>, we want to show that
<span class="math display" id="eq:dual-cone-condition">\[\begin{equation}
x_0 y_0 + x_1 y_1 + \dots + x_n y_n \geq 0, \forall (x_0,x_1,\dots,x_n) \in \mathcal{Q}_n.
\tag{1.5}
\end{equation}\]</span>
This is easy to verify because
<span class="math display">\[
x_1 y_1 + \dots + x_n y_n \geq - \sqrt{x_1^2 + \dots + x_n^2} \sqrt{y_1^2 + \dots + y_n^2} \geq - x_0 y_0.
\]</span>
Hence we have <span class="math inline">\(\mathcal{Q}_n \subseteq \mathcal{Q}_n^{*}\)</span>.</p>
<p>Conversely, if <a href="background.html#eq:dual-cone-condition">(1.5)</a> holds, then take
<span class="math display">\[
x_1 = -y_1, \dots, x_n = - y_n, \quad x_0 = \sqrt{x_1^2 + \dots + x_n^2},
\]</span>
we have
<span class="math display">\[
y_0 \geq \sqrt{y_1^2 + \dots + y_n^2},
\]</span>
hence <span class="math inline">\(\mathcal{Q}_n^{*} \subseteq \mathcal{Q}_n\)</span>. <span class="math inline">\(\blacksquare\)</span></p>
</div>
</div>
<p>Not every proper cone is self-dual.</p>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-3" class="exercise"><strong>Exercise 1.3  </strong></span>Consider the following proper cone in <span class="math inline">\(\mathbb{R}^{2}\)</span>
<span class="math display">\[
K = \{ (x_1,x_2) \mid 2x_1 - x_2 \geq 0, 2x_2 - x_1 \geq 0 \}.
\]</span>
Show that it is not self-dual.</p>
</div>
</div>
</div>
</div>
<div id="background:convex:optimization" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Convex Optimization<a href="background.html#background:convex:optimization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definitionbox">
<div class="definition">
<p><span id="def:ConvexFun" class="definition"><strong>Definition 1.6  (Convex Function) </strong></span>A function <span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{}\)</span> is a convex function if
<span class="math display">\[
f(\lambda x + (1-\lambda) y) \leq \lambda f(x) + (1-\lambda) f(y), \forall \lambda \in [0,1], \forall x,y \in \mathbb{R}^{n}.
\]</span></p>
</div>
</div>
<p>A function <span class="math inline">\(f\)</span> is convex if and only if its <strong>epigraph</strong> <span class="math inline">\(\{ (x,t) \in \mathbb{R}^{n+1} \mid f(x) \leq t \}\)</span> is a convex set.</p>
<p>When a function <span class="math inline">\(f\)</span> is differentiable, then there are several equivalent characterizations of convexity, in terms of the gradient <span class="math inline">\(\nabla f(x)\)</span> or the Hessian <span class="math inline">\(\nabla^2 f(x)\)</span>.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:CharacterizeConvexity" class="theorem"><strong>Theorem 1.3  (Equivalent Characterizations of Convexity) </strong></span>Let <span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{}\)</span> be a twice differentiable function. The following propositions are equivalent.</p>
<ol style="list-style-type: lower-roman">
<li><p><span class="math inline">\(f\)</span> is convex, i.e.,
<span class="math display">\[
f(\lambda x + (1-\lambda) y) \leq \lambda f(x) + (1-\lambda) f(y), \forall \lambda \in [0,1], x,y \in \mathbb{R}^{n}.
\]</span></p></li>
<li><p>The first-order convexity condition holds:
<span class="math display">\[
f(y) \geq f(x) + \langle \nabla f(x),  y - x \rangle, \forall x, y \in \mathbb{R}^{n},
\]</span>
i.e., the hyperplane going through <span class="math inline">\((x,f(x))\)</span> with slope <span class="math inline">\(\nabla f(x)\)</span> supports the epigraph of <span class="math inline">\(f\)</span>.</p></li>
<li><p>The second-order convexity condition holds:
<span class="math display">\[
\nabla^2 f(x) \succeq 0, \forall x \in \mathbb{R}^{n},
\]</span>
i.e., the Hessian is positive semidefinite everywhere.</p></li>
</ol>
</div>
</div>
<p>Let’s work on a little exercise.</p>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-4" class="exercise"><strong>Exercise 1.4  </strong></span>Which one of the following functions <span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{}\)</span> is not convex?</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\exp(-c^\top x)\)</span>, with <span class="math inline">\(c\)</span> constant</p></li>
<li><p><span class="math inline">\(\exp(c^\top x)\)</span>, with <span class="math inline">\(c\)</span> constant</p></li>
<li><p><span class="math inline">\(\exp(x^\top x)\)</span></p></li>
<li><p><span class="math inline">\(\exp(-x^\top x)\)</span></p></li>
</ol>
</div>
</div>
<div id="minimax-theorem" class="section level3 hasAnchor" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Minimax Theorem<a href="background.html#minimax-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given a function <span class="math inline">\(f: X \times Y \rightarrow \mathbb{R}^{}\)</span>, the following inequality always holds
<span class="math display" id="eq:weak-minimax">\[\begin{equation}
\max_{y \in Y} \min_{x \in X} f(x,y) \leq \min_{x \in X} \max_{y \in Y} f(x,y).
\tag{1.6}
\end{equation}\]</span>
If the maximum or minimum is not attained, then <a href="background.html#eq:weak-minimax">(1.6)</a> holds with <span class="math inline">\(\max\)</span> / <span class="math inline">\(\min\)</span> replaced by <span class="math inline">\(\sup\)</span> and <span class="math inline">\(\inf\)</span>, respectively.</p>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-5" class="exercise"><strong>Exercise 1.5  </strong></span>Provide examples of <span class="math inline">\(f\)</span> such that the inequality in <a href="background.html#eq:weak-minimax">(1.6)</a> is strict.</p>
</div>
</div>
<p>It is of interest to understand when equality holds in <a href="background.html#eq:weak-minimax">(1.6)</a>.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:minimax" class="theorem"><strong>Theorem 1.4  (Minimax Theorem) </strong></span>Let <span class="math inline">\(X \subset \mathbb{R}^{n}\)</span> and <span class="math inline">\(Y \subset \mathbb{R}^{n}\)</span> be compact convex sets, and <span class="math inline">\(f: X \times Y \rightarrow \mathbb{R}^{}\)</span> be a continuous function that is convex in its first argument and concave in the second. Then
<span class="math display">\[
\max_{y \in Y} \min_{x \in X} f(x,y) = \min_{x \in X} \max_{y \in Y} f(x,y).
\]</span></p>
</div>
</div>
<p>A special case of this theorem, used in game theory to prove the existence of equilibria for zero-sum games, is when <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are standard unit simplicies and the function <span class="math inline">\(f(x,y)\)</span> is bilinear. In a research from our group <span class="citation">(<a href="#ref-tang23arxiv-uncertainty" role="doc-biblioref">Tang, Lasserre, and Yang 2023</a>)</span>, we used the minimax theorem to convex a minimax problem into a single-level minimization problem.</p>
</div>
<div id="lagrangian-duality" class="section level3 hasAnchor" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Lagrangian Duality<a href="background.html#lagrangian-duality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a nonlinear optimization problem
<span class="math display" id="eq:background-nlp">\[\begin{equation}
\begin{split}
u^\star = \min_{x \in \mathbb{R}^{n}} &amp; \quad f(x) \\
\mathrm{s.t.}&amp; \quad g_i(x) \leq 0, i=1,\dots,m, \\
&amp; \quad h_j(x) = 0, j = 1,\dots,p.
\end{split}
\tag{1.7}
\end{equation}\]</span>
Define the <strong>Lagrangian</strong> associated with the optimization problem <a href="background.html#eq:background-nlp">(1.7)</a> as
<span class="math display" id="eq:background-Lagrangian">\[\begin{equation}
\begin{split}
L: \mathbb{R}^{n} \times \mathbb{R}^{m}_{+} \times \mathbb{R}^{p} \quad &amp; \rightarrow \quad \mathbb{R}^{}, \\
(x,\lambda,\mu) \quad &amp; \mapsto \quad f(x) + \sum_{i=1}^m \lambda_i g_i(x) + \sum_{j=1}^p \mu_j h_j(x).
\end{split}
\tag{1.8}
\end{equation}\]</span>
The <strong>Lagrangian dual function</strong> is defined as
<span class="math display" id="eq:background-Lagrangian-dual">\[\begin{equation}
\phi(\lambda,\mu) := \min_{x \in \mathbb{R}^{n}} L(x,\lambda,\mu).
\tag{1.9}
\end{equation}\]</span>
Maximizing this function over the dual variables <span class="math inline">\((\lambda,\mu)\)</span> yields
<span class="math display" id="eq:background-Lagrangian-dual-problem">\[\begin{equation}
v^\star := \max_{\lambda \geq 0, \mu \in \mathbb{R}^{p}} \phi(\lambda,\mu)
\tag{1.10}
\end{equation}\]</span>
Applying the minimax Theorem <a href="background.html#thm:minimax">1.4</a>, we can see that
<span class="math display">\[
v^\star = \max_{(\lambda,\mu)} \min_{x} L(x,\lambda,\mu) \leq \min_{x} \max_{(\lambda,\mu)} L(x,\lambda,\mu) = u^\star.
\]</span>
That is to say solving the dual problem <a href="background.html#eq:background-Lagrangian-dual-problem">(1.10)</a> always provides a lower bound to the primal problem <a href="background.html#eq:background-nlp">(1.7)</a>.</p>
<p>If the functions <span class="math inline">\(f,g_i\)</span> are convex and <span class="math inline">\(h_i\)</span> are affine, the Lagrangian is convex in <span class="math inline">\(x\)</span> and convex in <span class="math inline">\((\lambda,\mu)\)</span>. To ensure strong duality (i.e., <span class="math inline">\(u^\star = v^\star\)</span>), compactness or other <strong>constraint qualifications</strong> are needed. An often used condition is the Slater constraint qualification.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:SlaterCQ" class="definition"><strong>Definition 1.7  (Slater Constraint Qualification) </strong></span>There exists a strictly feasible point for <a href="background.html#eq:background-nlp">(1.7)</a>, i.e., a point <span class="math inline">\(z \in \mathbb{R}^{n}\)</span> such that <span class="math inline">\(h_j(z) = 0,j=1,\dots,p\)</span> and <span class="math inline">\(g_i(z) &lt; 0,i=1,\dots,m\)</span>.</p>
</div>
</div>
<p>Under these conditions, we have strong duality.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:StrongDuality" class="theorem"><strong>Theorem 1.5  (Strong Duality) </strong></span>Consider the optimization <a href="background.html#eq:background-nlp">(1.7)</a> and assume <span class="math inline">\(f,g_i\)</span> are convex and <span class="math inline">\(h_j\)</span> are affine. If Slater’s constraint qualification holds, then the optimal value of the primal problem <a href="background.html#eq:background-nlp">(1.7)</a> is the same as the optimal value of the dual problem <a href="background.html#eq:background-Lagrangian-dual-problem">(1.10)</a>.</p>
</div>
</div>
</div>
<div id="kkt-optimality-conditions" class="section level3 hasAnchor" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> KKT Optimality Conditions<a href="background.html#kkt-optimality-conditions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the nonlinear optimization problem <a href="background.html#eq:background-nlp">(1.7)</a>. A pair of primal and dual variables <span class="math inline">\((x^\star,\lambda^\star,\mu^\star)\)</span> is said to satisfy the Karush-Kuhn-Tucker (KKT) optimality conditions if</p>
<p><span class="math display" id="eq:KKT-conditions">\[\begin{equation}
\begin{split}
\text{primal feasibility}:\ \  &amp; g_i(x^\star) \leq 0,\forall i=1,\dots,m; h_j(x^\star) = 0, \forall j=1,\dots,p \\
\text{dual feasibility}:\ \  &amp; \lambda_i^\star \geq 0, \forall i=1,\dots,m \\
\text{stationarity}:\ \  &amp; \nabla_x L(x^\star,\lambda^\star,\mu^\star) = 0 \\
\text{complementarity}:\ \  &amp; \lambda_i^\star \cdot g_i(x^\star) = 0, \forall i=1,\dots,m.
\end{split}
\tag{1.11}
\end{equation}\]</span></p>
<p>Under certain constraint qualifications, the KKT conditions are necessary for local optimality.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:KKTNecessary" class="theorem"><strong>Theorem 1.6  (Necessary Optimality Conditions) </strong></span>Assume any of the following constraint qualifications hold:</p>
<ul>
<li><p>The gradients of the constraints <span class="math inline">\(\{ \nabla g_i(x^\star) \}_{i=1}^m\)</span>, <span class="math inline">\(\{ \nabla h_j(x^\star) \}_{j=1}^p\)</span> are linearly independent.</p></li>
<li><p>Slater’s constraint qualification (cf. Definition <a href="background.html#def:SlaterCQ">1.7</a>).</p></li>
<li><p>All constraints <span class="math inline">\(g_i(x)\)</span> and <span class="math inline">\(h_j(x)\)</span> are affine functions.</p></li>
</ul>
<p>Then, at every local minimum <span class="math inline">\(x^\star\)</span> of <a href="background.html#eq:background-nlp">(1.7)</a>, the KKT conditions <a href="background.html#eq:KKT-conditions">(1.11)</a> hold.</p>
</div>
</div>
<p>On the other hand, for convex optimization problems, the KKT conditions are sufficient for global optimality.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:KKTSufficient" class="theorem"><strong>Theorem 1.7  (Sufficient Optimality Conditions) </strong></span>Assume optimization <a href="background.html#eq:background-nlp">(1.7)</a> is convex, i.e., <span class="math inline">\(f,g_i\)</span> are convex and <span class="math inline">\(h_j\)</span> are affine. Every point <span class="math inline">\(x^\star\)</span> that satisfies the KKT conditions <a href="background.html#eq:KKT-conditions">(1.11)</a> is a global minimizer.</p>
</div>
</div>
</div>
</div>
<div id="background:linear:optimization" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Linear Optimization<a href="background.html#background:linear:optimization" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bertsekas03book-convex" class="csl-entry">
Bertsekas, Dimitri, Angelia Nedic, and Asuman Ozdaglar. 2003. <em>Convex Analysis and Optimization</em>. Vol. 1. Athena Scientific.
</div>
<div id="ref-boyd04book-convex" class="csl-entry">
Boyd, Stephen P, and Lieven Vandenberghe. 2004. <em>Convex Optimization</em>. Cambridge university press.
</div>
<div id="ref-rockafellar70-convexanalysis" class="csl-entry">
Rockafellar, Ralph Tyrell. 1970. <span>“Convex Analysis.”</span>
</div>
<div id="ref-tang23arxiv-uncertainty" class="csl-entry">
Tang, Yukai, Jean-Bernard Lasserre, and Heng Yang. 2023. <span>“Uncertainty Quantification of Set-Membership Estimation in Control and Perception: Revisiting the Minimum Enclosing Ellipsoid.”</span> <em>arXiv Preprint arXiv:2311.15962</em>.
</div>
</div>
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://semidefinite.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="notation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sdp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/hankyang94/Semidefinite/blob/main/01-background.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["semidefinite-optimization-relaxation.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
