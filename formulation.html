<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 The Optimal Control Formulation | Semidefinite Optimization and Relaxation</title>
  <meta name="description" content="Lecture notes for Harvard ES/AM 2XX Semidefinite Optimization and Relaxation." />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 The Optimal Control Formulation | Semidefinite Optimization and Relaxation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES/AM 2XX Semidefinite Optimization and Relaxation." />
  <meta name="github-repo" content="hankyang94/Semidefinite" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 The Optimal Control Formulation | Semidefinite Optimization and Relaxation" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES/AM 2XX Semidefinite Optimization and Relaxation." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2023-08-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Semidefinite Optimization and Relaxation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="formulation.html"><a href="formulation.html"><i class="fa fa-check"></i><b>1</b> The Optimal Control Formulation</a>
<ul>
<li class="chapter" data-level="1.1" data-path="formulation.html"><a href="formulation.html#the-basic-problem"><i class="fa fa-check"></i><b>1.1</b> The Basic Problem</a></li>
<li class="chapter" data-level="1.2" data-path="formulation.html"><a href="formulation.html#dynamic-programming-and-principle-of-optimality"><i class="fa fa-check"></i><b>1.2</b> Dynamic Programming and Principle of Optimality</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Semidefinite Optimization and Relaxation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="formulation" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> The Optimal Control Formulation<a href="formulation.html#formulation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="the-basic-problem" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> The Basic Problem<a href="formulation.html#the-basic-problem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider a discrete-time dynamical system
<span class="math display" id="eq:discrete-time-dynamics">\[\begin{equation}
x_{k+1} = f_k (x_k, u_k, w_k), \quad k =0,1,\dots,N-1
\tag{1.1}
\end{equation}\]</span>
where</p>
<ul>
<li><p><span class="math inline">\(x_k \in \mathbb{X} \subseteq \mathbb{R}^n\)</span> is the <em>state</em> of the system,</p></li>
<li><p><span class="math inline">\(u_k \in \mathbb{U} \subseteq \mathbb{R}^m\)</span> is the <em>control</em> we wish to design,</p></li>
<li><p><span class="math inline">\(w_k \in \mathbb{W} \subseteq \mathbb{R}^p\)</span> a random <em>disturbance</em> or noise (e.g., due to unmodelled dynamics) which is described by a probability distribution <span class="math inline">\(P_k(\cdot \mid x_k, u_k)\)</span> that may depend on <span class="math inline">\(x_k\)</span> and <span class="math inline">\(u_k\)</span> but not on prior disturbances <span class="math inline">\(w_0,\dots,w_{k-1}\)</span>,</p></li>
<li><p><span class="math inline">\(k\)</span> indexes the discrete time,</p></li>
<li><p><span class="math inline">\(N\)</span> denotes the horizon,</p></li>
<li><p><span class="math inline">\(f_k\)</span> models the transition function of the system (typically <span class="math inline">\(f_k \equiv f\)</span> is time-invariant, especially for robotics systems; we use <span class="math inline">\(f_k\)</span> here to keep full generality).</p></li>
</ul>
<div class="remark">
<p><span id="unlabeled-div-1" class="remark"><em>Remark</em> (Deterministic v.s. Stochastic). </span>When <span class="math inline">\(w_k \equiv 0\)</span> for all <span class="math inline">\(k\)</span>, we say the system <a href="formulation.html#eq:discrete-time-dynamics">(1.1)</a> is <em>deterministic</em>; otherwise we say the system is <em>stochastic</em>. In the following we will deal with the stochastic case, but most of the methodology should carry over to the deterministic setup.</p>
</div>
<p>We consider the class of <em>controllers</em> (also called <em>policies</em>) that consist of a sequence of functions
<span class="math display">\[
\pi = \{ \mu_0,\dots,\mu_{N-1} \},
\]</span>
where <span class="math inline">\(\mu_k (x_k) \in \mathbb{U}\)</span> for all <span class="math inline">\(x_k\)</span>, i.e., <span class="math inline">\(\mu_k\)</span> is a <em>feedback</em> controller that maps the state to an admissible control. Given an initial state <span class="math inline">\(x_0\)</span> and an admissible policy <span class="math inline">\(\pi\)</span>, the state <em>trajectory</em> of the system is a sequence of random variables that evolve according to
<span class="math display" id="eq:closed-loop-state-trajectory">\[\begin{equation}
x_{k+1} = f_k(x_k,\mu_k(x_k),w_k), \quad k=0,\dots,N-1
\tag{1.2}
\end{equation}\]</span>
where the randomness comes from the disturbance <span class="math inline">\(w_k\)</span>.</p>
<p>We assume the state-control trajectory <span class="math inline">\(\{u_k\}_{k=0}^{N-1}\)</span> and <span class="math inline">\(\{x_k \}_{k=0}^{N}\)</span> induce an <em>additive cost</em>
<span class="math display" id="eq:additive-cost">\[\begin{equation}
g_N(x_N) + \sum_{k=0}^{N-1} g_k(x_k,u_k)
\tag{1.3}
\end{equation}\]</span>
where <span class="math inline">\(g_k,k=0,\dots,N\)</span> are some user-designed functions.</p>
<p>With <a href="formulation.html#eq:closed-loop-state-trajectory">(1.2)</a> and <a href="formulation.html#eq:additive-cost">(1.3)</a>, for any admissible policy <span class="math inline">\(\pi\)</span>, we denote its induced <em>expected cost</em> with initial state <span class="math inline">\(x_0\)</span> as
<span class="math display" id="eq:expected-cost">\[\begin{equation}
J_\pi (x_0) = \mathbb{E} \left\{ g_N(x_N) + \sum_{k=0}^{N-1} g_k (x_k, \mu_k(x_k))  \right\},
\tag{1.4}
\end{equation}\]</span>
where the expectation is taken over the randomness of <span class="math inline">\(w_k\)</span>.</p>
<div class="definition">
<p><span id="def:basicproblem" class="definition"><strong>Definition 1.1  (Discrete-time, Finite-horizon Optimal Control) </strong></span>Find the best admissible controller that minimizes the expected cost in <a href="formulation.html#eq:expected-cost">(1.4)</a>
<span class="math display">\[\begin{equation}
\pi^\star \in \arg\min_{\pi \in \Pi} J_\pi(x_0),
\end{equation}\]</span>
where <span class="math inline">\(\Pi\)</span> is the set of all admissible controllers.
The cost attained by the optimal controller, i.e., <span class="math inline">\(J^\star = J_{\pi^\star}(x_0)\)</span> is called the optimal <em>cost-to-go</em>, or the optimal <em>value function</em>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-2" class="remark"><em>Remark</em> (Open-loop v.s. Closed-loop). </span>An important feature of the basic problem in Definition <a href="formulation.html#def:basicproblem">1.1</a> is that the problem seeks <em>feedback policies</em>, instead of numerical values of the controls, i.e., <span class="math inline">\(u_k = \mu_k(x_k)\)</span> is in general a function of the state <span class="math inline">\(x_k\)</span>. In other words, the controls are executed sequentially, one at a time after observing the state at each time. This is called closed-loop control, and is in general better than open-loop control
<span class="math display">\[
\min_{u_0,\dots,u_{N-1}} \mathbb{E} \left\{ g_N(x_N) + \sum_{k=0}^{N-1} g_k (x_k, u_k)  \right\}
\]</span>
where all the controls are planned at <span class="math inline">\(k=0\)</span>. Intuitively, a closed-loop policy is able to utilize the extra information received at each timestep (i.e., it observes <span class="math inline">\(x_{k+1}\)</span> and hence also observes the disturbance <span class="math inline">\(w_k\)</span>) to obtain a lower cost than an open-loop controller. Example 1.2.1 in <span class="citation">(<a href="#ref-bertsekas12book-dpocI">Bertsekas 2012</a>)</span> gives a concrete application where a closed-loop policy attains a lower cost than an open-loop policy.</p>
<p>In deterministic control (i.e., when <span class="math inline">\(w_k \equiv 0,\forall k\)</span>), however, a closed-loop policy has no advantage over an open-loop controller. This is obvious because at <span class="math inline">\(k=0\)</span>, even the open-loop controller predicts perfectly the consequences of all its actions and there is no extra information to be observed at later time steps. In fact, even in stochastic problems, a closed-loop policy may not be advantageous, see Exercise 1.27 in <span class="citation">(<a href="#ref-bertsekas12book-dpocI">Bertsekas 2012</a>)</span>.</p>
</div>
</div>
<div id="dynamic-programming-and-principle-of-optimality" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Dynamic Programming and Principle of Optimality<a href="formulation.html#dynamic-programming-and-principle-of-optimality" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now introduce a general and powerful algorithm, namely <em>dynamic programming</em> (DP), for solving the optimal control problem <a href="formulation.html#def:basicproblem">1.1</a>. The DP algorithm builds upon a quite simple intuition called the <em>Bellman principle of optimality</em>.</p>
<div class="{.}">
<div class="theorem">
<p><span id="thm:bellmanoptimality" class="theorem"><strong>Theorem 1.1  (Bellman Principle of Optimality) </strong></span>Let <span class="math inline">\(\pi^\star = \{ \mu_0^\star,\mu_1^\star,\dots,\mu_{N-1}^\star \}\)</span> be an optimal policy for the optimal control problem <a href="formulation.html#def:basicproblem">1.1</a>. Assume that when using <span class="math inline">\(\pi^\star\)</span>, a given state <span class="math inline">\(x_i\)</span> occurs at timestep <span class="math inline">\(i\)</span> with positive probability (i.e., <span class="math inline">\(x_i\)</span> is reachable at time <span class="math inline">\(i\)</span>).</p>
<p>Now consider the following subproblem where we are at <span class="math inline">\(x_i\)</span> at time <span class="math inline">\(i\)</span> and wish to minimize the cost-to-go from time <span class="math inline">\(i\)</span> to time <span class="math inline">\(N\)</span>
<span class="math display">\[
\min_{\mu_i,\dots,\mu_{N-1}} \mathbb{E} \left\{ g_N(x_N) + \sum_{k=i}^{N-1} g_k (x_k, \mu_k(x_k)) \right\}.
\]</span>
Then the truncated policy <span class="math inline">\(\{\mu^\star_i,\mu^\star_{i+1},\dots, \mu^\star_{N-1}\}\)</span> must be optimal for the subproblem.</p>
</div>
</div>
<p>Theorem <a href="formulation.html#thm:bellmanoptimality">1.1</a> can be proved intuitively by contradiction: if the truncated policy <span class="math inline">\(\{\mu^\star_i,\mu^\star_{i+1},\dots, \mu^\star_{N-1}\}\)</span> is not optimal for the subproblem, say there exists a different policy <span class="math inline">\(\{\mu_i&#39;,\mu_{i+1}&#39;,\dots, \mu_{N-1}&#39;\}\)</span> that attains a lower cost for the subproblem starting at <span class="math inline">\(x_i\)</span> at time <span class="math inline">\(i\)</span>. Then the combined policy <span class="math inline">\(\{\mu_0^\star,\dots,\mu^\star_{i-1},\mu_i&#39;,\dots,\mu_{N-1}&#39;\}\)</span> must attain a lower cost for the original optimal control problem <a href="formulation.html#def:basicproblem">1.1</a> due to the additive cost structure, contradicting the optimality of <span class="math inline">\(\pi^\star\)</span>.</p>
<p>The Bellman principle of optimality is more than just a principle, it is also an algorithm. It suggests that, to build an optimal policy, one can start by solving the last-stage subproblem to obtain <span class="math inline">\(\{\mu^\star_{N-1} \}\)</span>, and then proceed to solve the subproblem containing the last two stages to obtain <span class="math inline">\(\{ \mu^\star_{N-2},\mu^\star_{N-1} \}\)</span>. The recursion continues until optimal policies at all stages are computed. The following theorem formalizes this concept.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:dynamicprogramming" class="theorem"><strong>Theorem 1.2  (Dynamic Programming) </strong></span>The optimal value function <span class="math inline">\(J^\star(x_0)\)</span> of the optimal control problem <a href="formulation.html#def:basicproblem">1.1</a> (starting from any given initial condition <span class="math inline">\(x_0\)</span>) is equal to <span class="math inline">\(J_0(x_0)\)</span>, which can be computed backwards and recursively as
<span class="math display" id="eq:dpbackwardrecursion">\[\begin{align}
J_N(X_N) &amp;= g_N(x_N) \\
J_k(x_k) &amp;= \min_{u_k \in \mathbb{U}} \displaystyle \mathbb{E}_{w_k \sim P_k(\cdot \mid x_k, u_k)} \displaystyle \left\{ g_k(x_k,u_k) + J_{k+1}(f_k(x_k,u_k,w_k) ) \right\}, \ k=N-1,\dots,1,0.
\tag{1.5}
\end{align}\]</span>
Moreover, if <span class="math inline">\(u_k^\star = \mu_k^\star(x_k)\)</span> is a minimizer of <a href="formulation.html#eq:dpbackwardrecursion">(1.5)</a> for every <span class="math inline">\(x_k\)</span>, then the policy <span class="math inline">\(\pi^\star = \{\mu_0^\star,\dots,\mu_{N-1}^\star \}\)</span> is optimal.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-3" class="proof"><em>Proof</em>. </span>For any admissible policy <span class="math inline">\(\pi = \{ \mu_0,\dots,\mu_{N-1} \}\)</span>, denote <span class="math inline">\(\pi^k = \{ \mu_k,\dots,\mu_{N-1} \}\)</span> the last-<span class="math inline">\((N-k)\)</span>-stage truncated policy. Consider the subproblem consisting of the last <span class="math inline">\(N-k\)</span> stages starting from <span class="math inline">\(x_k\)</span>, and let <span class="math inline">\(J^\star_k(x_k)\)</span> be its optimal cost-to-go. Mathematically, this is
<span class="math display" id="eq:dptheoremdefineJkstar">\[\begin{equation}
J^\star_{k}(x_k) = \min_{\pi^k} \mathbb{E}_{w_k,\dots,w_{N-1}} \left\{ g_N(x_N) + \sum_{i=k}^{N-1} g_i (x_i,\mu_i(x_i)) \right\}, \quad k=0,1,\dots,N-1.
\tag{1.6}
\end{equation}\]</span>
We define <span class="math inline">\(J^\star_N(x_N) = g(x_N)\)</span> for <span class="math inline">\(k=N\)</span>.</p>
<p>Our goal is to prove the <span class="math inline">\(J_k(x_k)\)</span> computed by dynamic programming from <a href="formulation.html#eq:dpbackwardrecursion">(1.5)</a> is equal to <span class="math inline">\(J^\star_k (x_k)\)</span> for all <span class="math inline">\(k=0,\dots,N\)</span>. We will prove this by induction.</p>
<p>Firstly, we already have <span class="math inline">\(J^\star_N(x_N) = J_N(x_N) = g(x_N)\)</span>, so <span class="math inline">\(k=N\)</span> holds automatically.</p>
<p>Now we assume <span class="math inline">\(J^\star_{k+1}(x_{k+1}) = J_{k+1}(x_{k+1})\)</span> for all <span class="math inline">\(x_{k+1}\)</span>, and we wish to induce <span class="math inline">\(J^\star_{k}(x_{k}) = J_{k}(x_{k})\)</span>. To show this, we write
<span class="math display" id="eq:dpproof-8" id="eq:dpproof-7" id="eq:dpproof-6" id="eq:dpproof-5" id="eq:dpproof-4" id="eq:dpproof-3" id="eq:dpproof-2" id="eq:dpproof-1">\[\begin{align}
\hspace{-16mm} J^\star_{k}(x_k) &amp;= \min_{\pi^k} \mathbb{E}_{w_k,\dots,w_{N-1}} \left\{ g_N(x_N) + \sum_{i=k}^{N-1} g_i (x_i,\mu_i(x_i)) \right\} \tag{1.7}\\
&amp;= \min_{\mu_k,\pi^{k+1}} \mathbb{E}_{w_k,\dots,w_{N-1}} \left\{ g_k(x_k,\mu_k(x_k)) + g_N(x_N) + \sum_{i=k+1}^{N-1} g_i(x_i,\mu_i(x_i))  \right\}
\tag{1.8}\\
&amp;= \min_{\mu_k} \left[ \min_{\pi^{k+1}} \mathbb{E}_{w_k,\dots,w_{N-1}} \left\{ g_k(x_k,\mu_k(x_k)) + g_N(x_N) + \sum_{i=k+1}^{N-1} g_i(x_i,\mu_i(x_i))  \right\}\right] \tag{1.9}\\
&amp;= \min_{\mu_k} \mathbb{E}_{w_k} \left\{ g_k(x_k,\mu_k(x_k)) + \min_{\pi^{k+1}} \left[ \mathbb{E}_{w_{k+1},\dots,w_{N-1}} \left\{ g_N(x_N) + \sum_{i=k+1}^{N-1} g_i(x_i,\mu_i(x_i))  \right\}  \right]    \right\} \tag{1.10}\\
&amp;= \min_{\mu_k} \mathbb{E}_{w_k} \left\{ g_k(x_k,\mu_k(x_k)) + J^\star_{k+1}(f_k(x_k,\mu_k(x_k),w_k)) \right\} \tag{1.11}\\
&amp;= \min_{\mu_k} \mathbb{E}_{w_k} \left\{ g_k(x_k,\mu_k(x_k)) + J_{k+1}(f_k(x_k,\mu_k(x_k),w_k)) \right\} \tag{1.12}\\
&amp;= \min_{u_k \in \mathbb{U}} \mathbb{E}_{w_k} \left\{ g_k(x_k,\mu_k(x_k)) + J_{k+1}(f_k(x_k,\mu_k(x_k),w_k)) \right\} \tag{1.13}\\
&amp;= J_k(x_k), \tag{1.14}
\end{align}\]</span>
where <a href="formulation.html#eq:dpproof-1">(1.7)</a> follows from definition <a href="formulation.html#eq:dptheoremdefineJkstar">(1.6)</a>; <a href="formulation.html#eq:dpproof-2">(1.8)</a> expands <span class="math inline">\(\pi^k = \{ \mu_k, \pi^{k+1}\}\)</span> and <span class="math inline">\(\sum_{i=k}^{N-1} g_i = g_k + \sum_{i=k+1}^{N-1}\)</span>; <a href="formulation.html#eq:dpproof-3">(1.9)</a> writes the joint minimization over <span class="math inline">\(\mu_k\)</span> and <span class="math inline">\(\pi^{k+1}\)</span> as equivalently first minimizing over <span class="math inline">\(\pi^{k+1}\)</span> and then minimizing over <span class="math inline">\(\mu_k\)</span>; <a href="formulation.html#eq:dpproof-4">(1.10)</a> is the key step and holds because <span class="math inline">\(g_k\)</span> and <span class="math inline">\(w_k\)</span> depend only on <span class="math inline">\(\mu_k\)</span> but not on <span class="math inline">\(\pi^{k+1}\)</span>; <a href="formulation.html#eq:dpproof-5">(1.11)</a> follows again from definition <a href="formulation.html#eq:dptheoremdefineJkstar">(1.6)</a> with <span class="math inline">\(k\)</span> replaced by <span class="math inline">\(k+1\)</span>; <a href="formulation.html#eq:dpproof-6">(1.12)</a> results from the induction assumption; <a href="formulation.html#eq:dpproof-7">(1.13)</a> clearly holds because any <span class="math inline">\(\mu_k(x_k)\)</span> belongs to <span class="math inline">\(\mathbb{U}\)</span> and any element in <span class="math inline">\(\mathbb{U}\)</span> can be chosen by a feedback controller <span class="math inline">\(\mu_k\)</span>; and lastly <a href="formulation.html#eq:dpproof-8">(1.14)</a> follows from the dynamic programming algorithm <a href="formulation.html#eq:dpbackwardrecursion">(1.5)</a>.</p>
<p>By induction, this shows that <span class="math inline">\(J^\star_k(x_k) = J_k(x_k)\)</span> for all <span class="math inline">\(k=0,\dots,N\)</span>.</p>
</div>
</div>
<p>The careful reader, especially from a robotics background, may soon become disappointed when seeing the DP algorithm <a href="formulation.html#eq:dpbackwardrecursion">(1.5)</a> because it is rather conceptual than practical. To see this, we only need to run DP for <span class="math inline">\(k=N-1\)</span>:
<span class="math display" id="eq:dptryN-1">\[\begin{equation}
J_{N-1}(x_{N-1}) = \min_{u_{N-1} \in \mathbb{U}} \mathbb{E}_{w_{N-1}} \left\{ g_{N-1}(x_{N-1},u_{N-1}) + J_N(f_{N-1}(x_{N-1},u_{N-1},w_{N-1})) \right\}.
\tag{1.15}
\end{equation}\]</span></p>
<p>Two challenges immediately show up:</p>
<ul>
<li><p>How to perform the minimization over <span class="math inline">\(u_{N-1}\)</span> when <span class="math inline">\(\mathbb{U}\)</span> is a continuous constraint set? Even if we assume <span class="math inline">\(g_{N-1}\)</span> is convex<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> in <span class="math inline">\(u_{N-1}\)</span>, <span class="math inline">\(J_N\)</span> is convex in <span class="math inline">\(x_{N}\)</span>, and the dynamics <span class="math inline">\(f_{N-1}\)</span> is also convex in <span class="math inline">\(u_{N-1}\)</span> (so that the optimization <a href="formulation.html#eq:dptryN-1">(1.15)</a> is convex), we may be able to solve the minimization <em>numerically</em> for each <span class="math inline">\(x_{N-1}\)</span> using a convex optimization solver, but rarely will we be able to find an analytical policy <span class="math inline">\(\mu_{N-1}^\star\)</span> such that <span class="math inline">\(u_{N-1}^\star = \mu_{N-1}^\star (x_{N-1})\)</span> for every <span class="math inline">\(x_{N-1}\)</span> (i.e., the optimal policy <span class="math inline">\(\mu_{N-1}^\star\)</span> is implict but not explict).</p></li>
<li><p>Suppose we can find an anlytical optimal policy <span class="math inline">\(\mu_{N-1}^\star\)</span>, say <span class="math inline">\(\mu_{N-1}^\star = K x_{N-1}\)</span> a linear policy, how will plugging <span class="math inline">\(\mu_{N-1}^\star\)</span> into <a href="formulation.html#eq:dptryN-1">(1.15)</a> affect the complexity of <span class="math inline">\(J_{N-1}(x_{N-1})\)</span>? One can see that even if <span class="math inline">\(\mu_{N-1}^\star\)</span> is linear in <span class="math inline">\(x_{N-1}\)</span>, <span class="math inline">\(J_{N-1}\)</span> may be highly nonlinear in <span class="math inline">\(x_{N-1}\)</span> due to the composition with <span class="math inline">\(g_{N-1}\)</span>, <span class="math inline">\(f_{N-1}\)</span> and <span class="math inline">\(J_N\)</span>. If <span class="math inline">\(J_{N-1}(x_{N-1})\)</span> becomes too complex, then clearly it becomes more challenging to perform <a href="formulation.html#eq:dptryN-1">(1.15)</a> for the next step <span class="math inline">\(k=N-2\)</span>.</p></li>
</ul>
<p>Due to these challenges, only in a very limited amount of cases will we be able to perform <em>exact dynamic programming</em>. For example, when the state space <span class="math inline">\(\mathbb{X}\)</span> and control space <span class="math inline">\(\mathbb{U}\)</span> are discrete, we can design efficient algorithms for exact DP. For another example, when the dynamics <span class="math inline">\(f_k\)</span> is linear and the cost <span class="math inline">\(g_k\)</span> is quadratic, we will also be able to compute <span class="math inline">\(J_k(x_k)\)</span> in closed form (though this sounds a bit surprising!). We will study these problems in more details in Chapter <a href="#exactdp"><strong>??</strong></a>.</p>
<p>For general optimal control problems with continuous state space and control space (and most problems we care about in robotics), unfortunately, we will have to resort to <em>approximate dynamic programming</em>, basically variations of the DP algorithm <a href="formulation.html#eq:dpbackwardrecursion">(1.5)</a> where approximate value functions <span class="math inline">\(J_k(x_k)\)</span> and/or control policies <span class="math inline">\(\mu_k(x_k)\)</span> are used (e.g., with neural networks and machine learning).<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> We will introduce several popular approximation schemes in Chapter <a href="#approximatedp"><strong>??</strong></a>. We will see that, although exact DP is not possible anymore, the Bellman principle of optimality still remains one of the most important guidelines for designing approximation algorithms. Efficient algorithms for approximate dynamic programming, preferrably with performance guarantees, still remain an active area of research.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bertsekas12book-dpocI" class="csl-entry">
Bertsekas, Dimitri. 2012. <em>Dynamic Programming and Optimal Control: Volume i</em>. Vol. 1. Athena scientific.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>You may want to read Appendix <a href="#appconvex"><strong>??</strong></a> if this is your first time seeing “convex” things.<a href="formulation.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Another possible solution is to discretize continuous states and controls. However, when the dimension of state and control is high, discretization becomes too expensive in terms of memory and computational complexity.<a href="formulation.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/hankyang94/Semidefinite/blob/main/01-formulation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["semidefinite-optimization-relaxation.pdf", "semidefinite-optimization-relaxation.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
